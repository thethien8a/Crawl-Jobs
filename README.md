# Job Scraping Project

Dá»± Ã¡n web scraping Ä‘á»ƒ láº¥y dá»¯ liá»‡u viá»‡c lÃ m tá»« cÃ¡c trang tuyá»ƒn dá»¥ng Viá»‡t Nam nhÆ° JobsGO, JobOKO, 123job, CareerViet vÃ  JobStreet.

## ğŸ¯ TÃ­nh nÄƒng

- **Input**: Tá»« khÃ³a viá»‡c lÃ m
- **Output**: Dá»¯ liá»‡u viá»‡c lÃ m Ä‘Æ°á»£c lÆ°u vÃ o SQL Server
- **Sites**: JobsGO, JobOKO, 123job, CareerViet, JobStreet
- **Data**: Job title, company, salary, location, requirements, job_deadline, etc.

## ğŸ“‹ CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### 2. Cáº¥u hÃ¬nh SQL Server

Chá»‰nh sá»­a file `CrawlJob/settings.py`:

```python
# SQL Server Database Configuration
SQL_SERVER = "localhost"  # Thay Ä‘á»•i thÃ nh SQL Server instance cá»§a báº¡n
SQL_DATABASE = "JobDatabase"  # Thay Ä‘á»•i thÃ nh tÃªn database
SQL_USERNAME = "sa"  # Thay Ä‘á»•i thÃ nh username
SQL_PASSWORD = "your_password"  # Thay Ä‘á»•i thÃ nh password
```

Hoáº·c táº¡o file `.env` á»Ÿ thÆ° má»¥c gá»‘c (khuyáº¿n nghá»‹ an toÃ n):

```env
SQL_SERVER=localhost
SQL_DATABASE=JobDatabase
SQL_USERNAME=sa
SQL_PASSWORD=your_password
```

Ghi chÃº: `settings.py` Ä‘Ã£ tá»± Ä‘á»™ng Ä‘á»c biáº¿n mÃ´i trÆ°á»ng (qua `python-dotenv`) náº¿u cÃ³ file `.env`.

### 3. Táº¡o database

Táº¡o database `JobDatabase` trong SQL Server. Pipeline sáº½ **táº¡o báº£ng `jobs` náº¿u chÆ°a tá»“n táº¡i** khi cháº¡y láº§n Ä‘áº§u.

## ğŸš€ Sá»­ dá»¥ng

### CÃ¡ch 1: Sá»­ dá»¥ng script run_spider.py

```bash
# Cháº¡y spider JobsGO
python run_spider.py --spider jobsgo --keyword "python developer"

# Cháº¡y spider JobOKO
python run_spider.py --spider joboko --keyword "java developer"

# Cháº¡y spider 123job
python run_spider.py --spider 123job --keyword "data analyst"

# Cháº¡y spider CareerViet
python run_spider.py --spider careerviet --keyword "data analyst"

# Cháº¡y spider JobStreet
python run_spider.py --spider jobstreet --keyword "data analyst"

# Cháº¡y táº¥t cáº£ spider
python run_spider.py --spider all --keyword "developer"

# LÆ°u káº¿t quáº£ vÃ o file JSON
python run_spider.py --spider jobsgo --keyword "marketing" --output "marketing_jobs.json"
```

### API Read-Only (FastAPI)

```bash
pip install -r requirements.txt
uvicorn api.main:app --reload

# Kiá»ƒm tra
curl http://127.0.0.1:8000/health
curl "http://127.0.0.1:8000/jobs?keyword=python&site=jobsgo&page=1&page_size=20"
```

### CÃ¡ch 2: Sá»­ dá»¥ng Scrapy command

```bash
# Cháº¡y spider JobsGO
scrapy crawl jobsgo -a keyword="python developer"

# Cháº¡y spider JobOKO
scrapy crawl joboko -a keyword="java developer"

# Cháº¡y spider 123job
scrapy crawl 123job -a keyword="data analyst"

# Cháº¡y spider CareerViet
scrapy crawl careerviet -a keyword="data analyst"

# Cháº¡y spider JobStreet
scrapy crawl jobstreet -a keyword="data analyst"
```

## ğŸ“Š Cáº¥u trÃºc dá»¯ liá»‡u

Báº£ng `jobs` trong SQL Server:

| Field | Type | Description |
|-------|------|-------------|
| id | INT | Primary key |
| job_title | NVARCHAR(500) | TÃªn cÃ´ng viá»‡c |
| company_name | NVARCHAR(500) | TÃªn cÃ´ng ty |
| salary | NVARCHAR(200) | Má»©c lÆ°Æ¡ng |
| location | NVARCHAR(200) | Äá»‹a Ä‘iá»ƒm |
| job_type | NVARCHAR(100) | Loáº¡i cÃ´ng viá»‡c (Full-time, Part-time) |
| experience_level | NVARCHAR(200) | YÃªu cáº§u kinh nghiá»‡m |
| education_level | NVARCHAR(200) | YÃªu cáº§u há»c váº¥n |
| job_industry | NVARCHAR(200) | NgÃ nh nghá» |
| job_position | NVARCHAR(200) | Chá»©c vá»¥/Vá»‹ trÃ­ |
| job_description | NVARCHAR(MAX) | MÃ´ táº£ cÃ´ng viá»‡c |
| requirements | NVARCHAR(MAX) | YÃªu cáº§u cÃ´ng viá»‡c |
| benefits | NVARCHAR(MAX) | PhÃºc lá»£i |
| job_deadline | NVARCHAR(200) | Háº¡n cuá»‘i ná»™p CV |
| source_site | NVARCHAR(100) | Nguá»“n dá»¯ liá»‡u |
| job_url | NVARCHAR(1000) | URL cÃ´ng viá»‡c |
| search_keyword | NVARCHAR(200) | Tá»« khÃ³a tÃ¬m kiáº¿m |
| scraped_at | NVARCHAR(50) | Thá»i gian scrape |
| created_at | DATETIME | Thá»i gian táº¡o record |

LÆ°u Ã½: Náº¿u báº£ng `jobs` Ä‘Ã£ tá»“n táº¡i trÆ°á»›c khi thÃªm cá»™t má»›i (vÃ­ dá»¥ `job_position`) thÃ¬ cáº§n ALTER thá»§ cÃ´ng:

```sql
IF COL_LENGTH('dbo.jobs','job_position') IS NULL
    ALTER TABLE dbo.jobs ADD job_position NVARCHAR(200) NULL;
```

## ğŸ› ï¸ Cáº¥u trÃºc project

```
CrawlJob/
â”œâ”€â”€ CrawlJob/
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”œâ”€â”€ jobsgo_spider.py     # Spider cho JobsGO
â”‚   â”‚   â”œâ”€â”€ joboko_spider.py     # Spider cho JobOKO
â”‚   â”‚   â”œâ”€â”€ job123_spider.py     # Spider cho 123job
â”‚   â”‚   â””â”€â”€ careerviet_spider.py # Spider cho CareerViet
â”‚   â”œâ”€â”€ items.py                 # Äá»‹nh nghÄ©a cáº¥u trÃºc dá»¯ liá»‡u
â”‚   â”œâ”€â”€ pipelines.py             # Pipeline xá»­ lÃ½ dá»¯ liá»‡u (SQL Server, dedup/upsert)
â”‚   â”œâ”€â”€ settings.py              # Cáº¥u hÃ¬nh project
â”‚   â”œâ”€â”€ selenium_middleware.py   # (TÃ¹y chá»n) Middleware Selenium - hiá»‡n Ä‘ang táº¯t
â”‚   â””â”€â”€ utils.py                 # Tiá»‡n Ã­ch há»— trá»£ (encode_input, encode_joboko_input)
â”œâ”€â”€ api/main.py                  # FastAPI read-only (/health, /jobs)
â”œâ”€â”€ run_spider.py                # Script cháº¡y spider
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ scrapy.cfg                   # Cáº¥u hÃ¬nh Scrapy
â”œâ”€â”€ crawl_daily.bat              # Script batch cháº¡y Ä‘á»‹nh ká»³ (logs/outputs cÃ³ timestamp)
â””â”€â”€ README.md                    # HÆ°á»›ng dáº«n sá»­ dá»¥ng
```

## âš™ï¸ Cáº¥u hÃ¬nh nÃ¢ng cao

### Thay Ä‘á»•i delay giá»¯a cÃ¡c request

Chá»‰nh sá»­a `DOWNLOAD_DELAY` trong `settings.py`:

```python
DOWNLOAD_DELAY = 2  # Delay 2 giÃ¢y giá»¯a cÃ¡c request
```

### Thay Ä‘á»•i sá»‘ lÆ°á»£ng request Ä‘á»“ng thá»i

```python
CONCURRENT_REQUESTS = 16  # Sá»‘ request Ä‘á»“ng thá»i
```

### ThÃªm User Agent

```python
USER_AGENT = "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Mobile Safari/537.36"
```

## ğŸ—“ï¸ Scheduling (Windows Task Scheduler)

1. Test thá»§ cÃ´ng file batch:
```bat
cd /d D:\Practice\Scrapy\CrawlJob
crawl_daily.bat
```
- Káº¿t quáº£: táº¡o `outputs\jobs_YYYY-MM-DD_HH-mm-ss.json` vÃ  `logs\crawl_YYYY-MM-DD_HH-mm-ss.log`.

2. Táº¡o task tá»± Ä‘á»™ng (GUI):
- Task Scheduler â†’ Create Taskâ€¦
- General: Run whether user is logged on or not; Run with highest privileges
- Triggers: Daily 02:00
- Actions:
  - Program/script: `cmd.exe`
  - Add arguments: `/c D:\Practice\Scrapy\CrawlJob\crawl_daily.bat`
  - Start in: `D:\Practice\Scrapy\CrawlJob`
- Nháº¥n Run Ä‘á»ƒ test

3. Táº¡o task báº±ng lá»‡nh (tÃ¹y chá»n):
```bat
SCHTASKS /Create /TN "CrawlJob Daily" /TR "cmd.exe /c Path_to\crawl_daily.bat" /SC DAILY /ST 02:00 /RL HIGHEST /F
```

4. LÆ°u Ã½:
- Náº¿u `python` khÃ´ng nháº­n diá»‡n, dÃ¹ng full path tá»›i `python.exe` trong `crawl_daily.bat`.
- Náº¿u dÃ¹ng venv, bá» comment dÃ²ng `call ...activate.bat` trong batch.
- Äáº£m báº£o SQL Server báº­t TCP/IP vÃ  cá»•ng Ä‘Ãºng (thÆ°á»ng 1433), `.env` trá» Ä‘Ãºng `SQL_SERVER`.

### Chi tiáº¿t cáº¥u hÃ¬nh Task Scheduler (GUI)

1) Má»Ÿ Task Scheduler â†’ Create Taskâ€¦ (khÃ´ng pháº£i Basic Task)
- Tab General:
  - Name: CrawlJob Daily (hoáº·c tÃªn báº¡n muá»‘n)
  - Description: Cháº¡y `crawl_daily.bat` Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u háº±ng ngÃ y
  - Chá»n "Run whether user is logged on or not"
  - Tick "Run with highest privileges"
  - Configure for: Windows 10/11
- Tab Triggers â†’ Newâ€¦
  - Begin the task: On a schedule
  - Daily, Start at: 02:00 (vÃ­ dá»¥)
  - (Tuá»³ chá»n) Advanced: Repeat task every: 4 hours; For a duration of: Indefinitely â†’ dÃ¹ng khi muá»‘n cháº¡y nhiá»u láº§n/ngÃ y
  - OK
- Tab Actions â†’ Newâ€¦
  - Action: Start a program
  - Program/script: `cmd.exe`
  - Add arguments: `/c "D:\Practice\Scrapy\CrawlJob\crawl_daily.bat"`
  - Start in (optional): `D:\Practice\Scrapy\CrawlJob`
  - LÆ°u Ã½: luÃ´n bá»c Ä‘Æ°á»ng dáº«n cÃ³ dáº¥u cÃ¡ch trong dáº¥u nhÃ¡y kÃ©p ""
  - OK
- Tab Conditions: tuá»³ nhu cáº§u (vÃ­ dá»¥ bá» chá»n "Start the task only if the computer is on AC power")
- Tab Settings:
  - Cho phÃ©p "Allow task to be run on demand"
  - Náº¿u task cÃ³ thá»ƒ cháº¡y lÃ¢u: Ä‘iá»u chá»‰nh "Stop the task if it runs longer than"
- Nháº¥n OK vÃ  nháº­p máº­t kháº©u user náº¿u Ä‘Æ°á»£c yÃªu cáº§u

2) Cháº¡y test ngay
- Trong Task Scheduler, chá»n task â†’ Run
- Kiá»ƒm tra:
  - File `outputs\jobs_*.json` Ä‘Æ°á»£c sinh
  - File `logs\crawl_*.log` cÃ³ ná»™i dung log

3) Táº¡o task báº±ng dÃ²ng lá»‡nh (tÃ¹y chá»n)
```bat
REM ÄÆ°á»ng dáº«n generic (sá»­a Path_to cho phÃ¹ há»£p)
SCHTASKS /Create /TN "CrawlJob Daily" /TR "cmd.exe /c \"Path_to\\crawl_daily.bat\"" /SC DAILY /ST 02:00 /RL HIGHEST /F

REM VÃ­ dá»¥ theo project nÃ y
SCHTASKS /Create /TN "CrawlJob Daily" /TR "cmd.exe /c \"D:\\Practice\\Scrapy\\CrawlJob\\crawl_daily.bat\"" /SC DAILY /ST 02:00 /RL HIGHEST /F

REM Cháº¡y má»—i 4 giá» (láº·p vÃ´ háº¡n) báº¯t Ä‘áº§u tá»« 00:00
SCHTASKS /Create /TN "CrawlJob Every4H" /TR "cmd.exe /c \"D:\\Practice\\Scrapy\\CrawlJob\\crawl_daily.bat\"" /SC HOURLY /MO 4 /ST 00:00 /RL HIGHEST /F

REM Cháº¡y dÆ°á»›i tÃ i khoáº£n SYSTEM (khÃ´ng cáº§n Ä‘Äƒng nháº­p)
SCHTASKS /Create /TN "CrawlJob SYSTEM" /TR "cmd.exe /c \"D:\\Practice\\Scrapy\\CrawlJob\\crawl_daily.bat\"" /SC DAILY /ST 02:00 /RU SYSTEM /RL HIGHEST /F
```

4) Gá»£i Ã½ cáº¥u hÃ¬nh trong `crawl_daily.bat`
- Náº¿u dÃ¹ng virtualenv, bá» comment dÃ²ng `call ...activate.bat` vÃ  sá»­a path cho Ä‘Ãºng
- Náº¿u `python` khÃ´ng cÃ³ trong PATH cá»§a dá»‹ch vá»¥, dÃ¹ng full path tá»›i `python.exe` (Ä‘Ã£ cÃ³ dÃ²ng máº«u trong file .bat)
- CÃ³ thá»ƒ Ä‘á»•i `--keyword` theo nhu cáº§u

5) Troubleshooting Task Scheduler
- "The system cannot find the file specified": kiá»ƒm tra quotes vÃ  Ä‘Æ°á»ng dáº«n trong Program/script, Arguments, Start in
- Exit code 1/2: xem file log trong `logs\crawl_*.log` Ä‘á»ƒ biáº¿t lá»—i chi tiáº¿t (selector, SQL, máº¡ngâ€¦)
- KhÃ´ng táº¡o ra output/log: kiá»ƒm tra quyá»n ghi thÆ° má»¥c hoáº·c dÃ¹ng Start in Ä‘á»ƒ Ä‘áº·t Working Directory Ä‘Ãºng
- KhÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c SQL Server: kiá»ƒm tra TCP/IP, port 1433, firewall; `.env` Ä‘Ãºng `SQL_SERVER`

## ğŸ”§ Troubleshooting

### Lá»—i káº¿t ná»‘i SQL Server
1. Kiá»ƒm tra SQL Server Ä‘ang cháº¡y
2. Kiá»ƒm tra `.env`: `SQL_SERVER=localhost,1433` hoáº·c `localhost\SQLEXPRESS`
3. Báº­t TCP/IP vÃ  má»Ÿ firewall port 1433

### Lá»—i scraping
1. Kiá»ƒm tra internet connection
2. Thá»­ tÄƒng `DOWNLOAD_DELAY`
3. Kiá»ƒm tra website cÃ³ thay Ä‘á»•i cáº¥u trÃºc HTML khÃ´ng

### Lá»—i CSS selector
- Cáº­p nháº­t selector trong spider náº¿u website Ä‘á»•i HTML.

## ğŸ“ Ghi chÃº

- Spider cÃ³ delay giá»¯a cÃ¡c request Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i server
- Dá»¯ liá»‡u lÆ°u vÃ o SQL Server (UTF-8); dedup theo `(source_site, job_url)` vÃ  upsert `updated_at`
- CÃ³ thá»ƒ má»Ÿ rá»™ng thÃªm cÃ¡c trang tuyá»ƒn dá»¥ng khÃ¡c

## ğŸ“„ License

MIT License
