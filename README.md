# Job Scraping Project

Dá»± Ã¡n web scraping Ä‘á»ƒ láº¥y dá»¯ liá»‡u viá»‡c lÃ m tá»« cÃ¡c trang tuyá»ƒn dá»¥ng Viá»‡t Nam nhÆ° JobsGO vÃ  TopCV.

## ğŸ¯ TÃ­nh nÄƒng

- **Input**: Tá»« khÃ³a viá»‡c lÃ m vÃ  Ä‘á»‹a Ä‘iá»ƒm
- **Output**: Dá»¯ liá»‡u viá»‡c lÃ m Ä‘Æ°á»£c lÆ°u vÃ o SQL Server
- **Sites**: JobsGO, TopCV
- **Data**: Job title, company, salary, location, requirements, etc.

## ğŸ“‹ CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### 2. Cáº¥u hÃ¬nh SQL Server

Chá»‰nh sá»­a file `CrawlJob/settings.py`:

```python
# SQL Server Database Configuration
SQL_SERVER = "localhost"  # Thay Ä‘á»•i thÃ nh SQL Server instance cá»§a báº¡n
SQL_DATABASE = "JobDatabase"  # Thay Ä‘á»•i thÃ nh tÃªn database
SQL_USERNAME = "sa"  # Thay Ä‘á»•i thÃ nh username
SQL_PASSWORD = "your_password"  # Thay Ä‘á»•i thÃ nh password
```

### 3. Táº¡o database

Táº¡o database `JobDatabase` trong SQL Server. Spider sáº½ tá»± Ä‘á»™ng táº¡o báº£ng `jobs` khi cháº¡y láº§n Ä‘áº§u.

## ğŸš€ Sá»­ dá»¥ng

### CÃ¡ch 1: Sá»­ dá»¥ng script run_spider.py

```bash
# Cháº¡y spider JobsGO
python run_spider.py --spider jobsgo --keyword "python developer" --location "Há»“ ChÃ­ Minh"

# Cháº¡y spider TopCV
python run_spider.py --spider topcv --keyword "java developer" --location "HÃ  Ná»™i"

# Cháº¡y cáº£ hai spider
python run_spider.py --spider both --keyword "data analyst" --location "ÄÃ  Náºµng"

# LÆ°u káº¿t quáº£ vÃ o file JSON
python run_spider.py --spider jobsgo --keyword "marketing" --output "marketing_jobs.json"
```

### CÃ¡ch 2: Sá»­ dá»¥ng Scrapy command

```bash
# Cháº¡y spider JobsGO
scrapy crawl jobsgo -a keyword="python developer" -a location="Há»“ ChÃ­ Minh"

# Cháº¡y spider TopCV
scrapy crawl topcv -a keyword="java developer" -a location="HÃ  Ná»™i"
```

## ğŸ“Š Cáº¥u trÃºc dá»¯ liá»‡u

Báº£ng `jobs` trong SQL Server:

| Field | Type | Description |
|-------|------|-------------|
| id | INT | Primary key |
| job_title | NVARCHAR(500) | TÃªn cÃ´ng viá»‡c |
| company_name | NVARCHAR(500) | TÃªn cÃ´ng ty |
| salary | NVARCHAR(200) | Má»©c lÆ°Æ¡ng |
| location | NVARCHAR(200) | Äá»‹a Ä‘iá»ƒm |
| job_type | NVARCHAR(100) | Loáº¡i cÃ´ng viá»‡c (Full-time, Part-time) |
| experience_level | NVARCHAR(200) | YÃªu cáº§u kinh nghiá»‡m |
| education_level | NVARCHAR(200) | YÃªu cáº§u há»c váº¥n |
| job_description | NVARCHAR(MAX) | MÃ´ táº£ cÃ´ng viá»‡c |
| requirements | NVARCHAR(MAX) | YÃªu cáº§u cÃ´ng viá»‡c |
| benefits | NVARCHAR(MAX) | PhÃºc lá»£i |
| posted_date | NVARCHAR(200) | NgÃ y Ä‘Äƒng |
| source_site | NVARCHAR(100) | Nguá»“n dá»¯ liá»‡u |
| job_url | NVARCHAR(1000) | URL cÃ´ng viá»‡c |
| search_keyword | NVARCHAR(200) | Tá»« khÃ³a tÃ¬m kiáº¿m |
| scraped_at | NVARCHAR(50) | Thá»i gian scrape |
| created_at | DATETIME | Thá»i gian táº¡o record |

## ğŸ› ï¸ Cáº¥u trÃºc project

```
CrawlJob/
â”œâ”€â”€ CrawlJob/
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”œâ”€â”€ jobsgo_spider.py    # Spider cho JobsGO
â”‚   â”‚   â””â”€â”€ topcv_spider.py     # Spider cho TopCV
â”‚   â”œâ”€â”€ items.py                # Äá»‹nh nghÄ©a cáº¥u trÃºc dá»¯ liá»‡u
â”‚   â”œâ”€â”€ pipelines.py            # Pipeline xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â””â”€â”€ settings.py             # Cáº¥u hÃ¬nh project
â”œâ”€â”€ run_spider.py               # Script cháº¡y spider
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md                  # HÆ°á»›ng dáº«n sá»­ dá»¥ng
```

## âš™ï¸ Cáº¥u hÃ¬nh nÃ¢ng cao

### Thay Ä‘á»•i delay giá»¯a cÃ¡c request

Chá»‰nh sá»­a `DOWNLOAD_DELAY` trong `settings.py`:

```python
DOWNLOAD_DELAY = 2  # Delay 2 giÃ¢y giá»¯a cÃ¡c request
```

### Thay Ä‘á»•i sá»‘ lÆ°á»£ng request Ä‘á»“ng thá»i

```python
CONCURRENT_REQUESTS = 8  # Sá»‘ request Ä‘á»“ng thá»i
```

### ThÃªm User Agent

```python
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
```

## ğŸ”§ Troubleshooting

### Lá»—i káº¿t ná»‘i SQL Server

1. Kiá»ƒm tra SQL Server Ä‘ang cháº¡y
2. Kiá»ƒm tra thÃ´ng tin Ä‘Äƒng nháº­p trong `settings.py`
3. Äáº£m báº£o database Ä‘Ã£ Ä‘Æ°á»£c táº¡o

### Lá»—i scraping

1. Kiá»ƒm tra internet connection
2. Thá»­ tÄƒng `DOWNLOAD_DELAY`
3. Kiá»ƒm tra website cÃ³ thay Ä‘á»•i cáº¥u trÃºc HTML khÃ´ng

### Lá»—i CSS selector

CÃ¡c spider sá»­ dá»¥ng CSS selector linh hoáº¡t Ä‘á»ƒ tÃ¬m dá»¯ liá»‡u. Náº¿u website thay Ä‘á»•i cáº¥u trÃºc, cáº§n cáº­p nháº­t selector trong spider.

## ğŸ“ Ghi chÃº

- Spider tuÃ¢n thá»§ robots.txt vÃ  cÃ³ delay giá»¯a cÃ¡c request
- Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u vÃ o SQL Server vá»›i encoding UTF-8
- CÃ³ thá»ƒ má»Ÿ rá»™ng thÃªm cÃ¡c trang tuyá»ƒn dá»¥ng khÃ¡c
- Spider tá»± Ä‘á»™ng táº¡o báº£ng náº¿u chÆ°a tá»“n táº¡i

## ğŸ¤ ÄÃ³ng gÃ³p

Äá»ƒ thÃªm spider cho trang tuyá»ƒn dá»¥ng má»›i:

1. Táº¡o file spider má»›i trong `spiders/`
2. Káº¿ thá»«a tá»« `scrapy.Spider`
3. Implement cÃ¡c method `start_requests()`, `parse_search_results()`, `parse_job_detail()`
4. ThÃªm spider vÃ o `run_spider.py`

## ğŸ“„ License

MIT License
