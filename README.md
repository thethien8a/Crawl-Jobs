# CrawlJob - Vietnam Job Market Analytics Platform

Dá»± Ã¡n thu tháº­p, lÆ°u trá»¯ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u viá»‡c lÃ m nghÃ nh data tá»« cÃ¡c trang tuyá»ƒn dá»¥ng lá»›n táº¡i Viá»‡t Nam (TopCV, Linkedin, ITViec, JobStreet, v.v.). Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ theo kiáº¿n trÃºc Hybrid, tÃ¡ch biá»‡t giá»¯a nhu cáº§u truy xuáº¥t nhanh cho á»©ng dá»¥ng (OLTP) vÃ  nhu cáº§u phÃ¢n tÃ­ch dá»¯ liá»‡u lá»›n (OLAP).

## ğŸ— Kiáº¿n TrÃºc Há»‡ Thá»‘ng (Architecture)

MÃ´ hÃ¬nh tá»•ng quan luá»“ng dá»¯ liá»‡u (Data Flow):

```mermaid
graph LR
    subgraph "Collection Layer"
        A[Scrapy Spiders] 
    end

    subgraph "OLTP / Staging (Supabase)"
        B[(Table: staging_jobs)]
        B1[Web App Backend]
    end

    subgraph "Orchestration (Airflow)"
        C[ETL DAGs]
    end

    subgraph "OLAP / Data Warehouse (BigQuery)"
        D[(Dataset: job_market)]
        D1[Table: raw_jobs]
        D2[Table: dim_skills]
        D3[Table: fact_market_trends]
    end

    subgraph "User Interface"
        E[Job Search Website]
        F[BI Reports / Dashboard]
    end

    %% Flows
    A -->|Upsert Raw Data| B
    B <-->|Read/Write Hot Data| B1
    B1 --> E
    
    C -->|Extract New Data (Daily)| B
    C -->|Batch Load| D1
    
    D1 -->|SQL Transform| D2
    D1 -->|SQL Transform| D3
    
    D3 --> F
```

### Chi tiáº¿t cÃ¡c thÃ nh pháº§n:

1.  **Collection Layer (Scrapy):**
    *   Nhiá»‡m vá»¥: Thu tháº­p dá»¯ liá»‡u tá»« cÃ¡c nguá»“n job board.
    *   Äáº§u ra: Dá»¯ liá»‡u thÃ´ Ä‘Æ°á»£c lÃ m sáº¡ch cÆ¡ báº£n.
    *   Destination: Ghi trá»±c tiáº¿p vÃ o báº£ng `staging_jobs` trÃªn Supabase.

2.  **OLTP Layer (Supabase - PostgreSQL):**
    *   Vai trÃ²: Operational Database & Staging Area.
    *   Chá»©c nÄƒng: 
        *   LÆ°u trá»¯ dá»¯ liá»‡u "nÃ³ng" (viá»‡c lÃ m Ä‘ang tuyá»ƒn, dá»¯ liá»‡u má»›i nháº¥t).
        *   Cung cáº¥p API cho **Website tra cá»©u viá»‡c lÃ m**.
        *   Táº­n dá»¥ng tÃ­nh nÄƒng Realtime/Auth cá»§a Supabase Ä‘á»ƒ xÃ¢y dá»±ng App nhanh chÃ³ng.

3.  **Orchestration Layer (Airflow):**
    *   Vai trÃ²: Äiá»u phá»‘i luá»“ng dá»¯ liá»‡u (ETL Pipeline).
    *   Nhiá»‡m vá»¥:
        *   LÃªn lá»‹ch cháº¡y Spider Ä‘á»‹nh ká»³.
        *   **Sync Job:** Query dá»¯ liá»‡u má»›i tá»« Supabase -> Load vÃ o Google BigQuery (Batch processing).

4.  **OLAP Layer (Google BigQuery):**
    *   Vai trÃ²: Data Warehouse (Kho dá»¯ liá»‡u phÃ¢n tÃ­ch).
    *   Chá»©c nÄƒng:
        *   LÆ°u trá»¯ lá»‹ch sá»­ dÃ i háº¡n (Historical Data).
        *   Xá»­ lÃ½ cÃ¡c truy váº¥n náº·ng: PhÃ¢n tÃ­ch xu hÆ°á»›ng lÆ°Æ¡ng, ká»¹ nÄƒng hot, biáº¿n Ä‘á»™ng thá»‹ trÆ°á»ng.
        *   Nguá»“n dá»¯ liá»‡u cho cÃ¡c bÃ¡o cÃ¡o Insight (Looker Studio, Metabase).

## ğŸ“‚ Cáº¥u TrÃºc ThÆ° Má»¥c (Project Structure)

```text
CrawlJob/
â”œâ”€â”€ airflow/                # Airflow DAGs & Configuration
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ sync_supabase_bigquery.py  # ETL: Supabase -> BigQuery
â”‚       â””â”€â”€ trigger_spiders.py         # Schedule Scrapy Jobs
â”œâ”€â”€ api/                    # Backend API (náº¿u cáº§n custom logic ngoÃ i Supabase)
â”œâ”€â”€ CrawlJob/               # Scrapy Project Core
â”‚   â”œâ”€â”€ spiders/            # CÃ¡c Spider thu tháº­p dá»¯ liá»‡u
â”‚   â”œâ”€â”€ items.py            # Äá»‹nh nghÄ©a cáº¥u trÃºc dá»¯ liá»‡u (Data Models)
â”‚   â”œâ”€â”€ pipelines.py        # Xá»­ lÃ½ dá»¯ liá»‡u trÆ°á»›c khi lÆ°u vÃ o Supabase
â”‚   â””â”€â”€ settings.py         # Cáº¥u hÃ¬nh Scrapy (Delay, User-Agent, DB Connect)
â”œâ”€â”€ notebooks/              # Jupyter Notebooks (EDA, Data Analysis, Test DuckDB/BQ)
â”œâ”€â”€ scripts/                # Utility Scripts (Cháº¡y spider thá»§ cÃ´ng, helper tools)
â”œâ”€â”€ web/                    # Frontend (Website tra cá»©u viá»‡c lÃ m Ä‘Æ¡n giáº£n)
â”œâ”€â”€ docker-compose.yml      # Setup mÃ´i trÆ°á»ng (Airflow, Local DB...)
â”œâ”€â”€ requirements.txt        # Python Dependencies
â””â”€â”€ README.md               # Project Documentation
```

## ğŸš€ Getting Started

### 1. Prerequisites
*   Python 3.10+
*   Docker & Docker Compose (cho Airflow)
*   TÃ i khoáº£n Supabase & Google Cloud Platform (BigQuery API enabled)

### 2. Setup Environment
```bash
# Clone project
git clone <repo-url>
cd CrawlJob

# Táº¡o mÃ´i trÆ°á»ng áº£o
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# CÃ i Ä‘áº·t thÆ° viá»‡n
pip install -r requirements.txt
```

### 3. Configuration (.env)
Táº¡o file `.env` tá»« `env.example` vÃ  Ä‘iá»n cÃ¡c thÃ´ng tin credentials:
```ini
# Supabase
SUPABASE_URL=...
SUPABASE_KEY=...
DB_CONNECTION_STRING=postgresql://user:pass@host:port/dbname

# Google Cloud (BigQuery)
GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account.json
BQ_PROJECT_ID=...
BQ_DATASET_ID=...
```

## ğŸ“Š Roadmap
- [x] XÃ¢y dá»±ng Scrapy Spider cÆ¡ báº£n.
- [x] Thiáº¿t láº­p lÆ°u trá»¯ Staging trÃªn Supabase.
- [ ] XÃ¢y dá»±ng luá»“ng Airflow Sync dá»¯ liá»‡u sang BigQuery.
- [ ] PhÃ¡t triá»ƒn Web UI tra cá»©u Ä‘Æ¡n giáº£n.
- [ ] XÃ¢y dá»±ng Dashboard phÃ¢n tÃ­ch Insight trÃªn BigQuery.

