# CrawlJob - Vietnam Job Market Analytics Platform

Dá»± Ã¡n thu tháº­p, lÆ°u trá»¯ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u viá»‡c lÃ m nghÃ nh data tá»« cÃ¡c trang tuyá»ƒn dá»¥ng lá»›n táº¡i Viá»‡t Nam (TopCV, Linkedin, ITViec, JobStreet, v.v.). Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ theo kiáº¿n trÃºc Hybrid, tÃ¡ch biá»‡t giá»¯a nhu cáº§u truy xuáº¥t nhanh cho á»©ng dá»¥ng (OLTP) vÃ  nhu cáº§u phÃ¢n tÃ­ch dá»¯ liá»‡u lá»›n (OLAP).

## ğŸ— Kiáº¿n TrÃºc Há»‡ Thá»‘ng (Architecture)

MÃ´ hÃ¬nh tá»•ng quan luá»“ng dá»¯ liá»‡u (Data Flow):

```mermaid
graph TB
    subgraph Scheduling_Layer["ğŸ—“ï¸ Scheduling Layer"]
        subgraph GitHub_Actions["â˜ï¸ GitHub Actions (Cloud Server)"]
            GA_CRON["â° Cron Schedule<br/>*/6 * * * *"]
            GA_TOPCV["ğŸ•·ï¸ 123Job Spider"]
            GA_VNW["ğŸ•·ï¸ VietnamWorks Spider"]
            GA_SCRIPT["ğŸ“œ Python Script<br/>scrape_easy.py"]
        end
        
        subgraph Airflow_Local["ğŸ  Apache Airflow (Local Server)"]
            AF_DAG["ğŸ“‹ DAG: scrape_hard_sites<br/>0 2 * * *"]
            AF_LINKEDIN["ğŸ•·ï¸ LinkedIn Spider"]
            AF_GLASSDOOR["ğŸ•·ï¸ TopCV Spider"]
            AF_ANTIBOT["ğŸ›¡ï¸ Anti-bot Handler<br/>Proxy + Rotating UA"]
            AF_SCRIPT["ğŸ“œ Python Script<br/>scrape_hard.py"]
        end
    end

    subgraph Collection_Layer["ğŸ” Collection Layer"]
        A["ğŸ•¸ï¸ Scrapy Spiders"]
    end

    subgraph OLTP_Staging["ğŸ’¾ OLTP / Staging (Supabase)"]
        B[("ğŸ“¥ staging_jobs")]
        B3[("âš ï¸ quarantine_jobs")]
        B2[("âœ… jobs")]
        B1["ğŸ–¥ï¸ Web App Backend"]
    end

    subgraph Orchestration["âš™ï¸ Orchestration (Airflow)"]
        C1["ğŸ“¤ Task: Extract"]
        C2["âœ”ï¸ Task: Validate DQ"]
        C3["ğŸ”„ Task: Upsert"]
        C4["ğŸ“¦ Task: Load to DW"]
    end

    subgraph OLAP_DW["ğŸ“Š OLAP / Data Warehouse (BigQuery)"]
        D1[("ğŸ—ƒï¸ raw_jobs")]
        D2[("ğŸ·ï¸ dim_skills")]
        D3[("ğŸ“ˆ fact_market_trends")]
    end

    subgraph User_Interface["ğŸ‘¤ User Interface"]
        E["ğŸŒ Job Search Website"]
        F["ğŸ“Š BI Dashboard"]
    end

    %% GitHub Actions Flow
    GA_CRON --> GA_TOPCV
    GA_CRON --> GA_VNW
    GA_TOPCV --> GA_SCRIPT
    GA_VNW --> GA_SCRIPT
    GA_SCRIPT --> A

    %% Airflow Local Flow
    AF_DAG --> AF_LINKEDIN
    AF_DAG --> AF_GLASSDOOR
    AF_LINKEDIN --> AF_ANTIBOT
    AF_GLASSDOOR --> AF_ANTIBOT
    AF_ANTIBOT --> AF_SCRIPT
    AF_SCRIPT --> A

    %% Collection to Staging
    A -->|"Insert Raw"| B
    
    %% ETL Pipeline
    C1 -->|"Read"| B
    C1 --> C2
    C2 -->|"PASS âœ…"| C3
    C2 -->|"FAIL âŒ"| B3
    C3 -->|"Upsert"| B2
    C3 --> C4
    C4 -->|"Batch Load"| D1
    
    %% Backend & UI
    B2 <-->|"Read/Write"| B1
    B1 --> E
    
    %% Data Warehouse Transform
    D1 --> D2
    D1 --> D3
    D3 --> F

    %% Styling
    classDef github fill:#24292e,stroke:#ffffff,color:#ffffff
    classDef airflow fill:#017cee,stroke:#ffffff,color:#ffffff
    classDef scrapy fill:#60a839,stroke:#ffffff,color:#ffffff
    classDef supabase fill:#3ecf8e,stroke:#ffffff,color:#ffffff
    classDef bigquery fill:#4285f4,stroke:#ffffff,color:#ffffff
    classDef ui fill:#ff6b6b,stroke:#ffffff,color:#ffffff

    class GA_CRON,GA_TOPCV,GA_VNW,GA_SCRIPT github
    class AF_DAG,AF_LINKEDIN,AF_GLASSDOOR,AF_ANTIBOT,AF_SCRIPT airflow
    class A scrapy
    class B,B2,B3,B1 supabase
    class D1,D2,D3 bigquery
    class E,F ui
```

### Chi tiáº¿t cÃ¡c thÃ nh pháº§n:

1.  **Collection Layer (Scrapy):**
    *   Nhiá»‡m vá»¥: Thu tháº­p dá»¯ liá»‡u tá»« cÃ¡c nguá»“n job board.
    *   Äáº§u ra: Dá»¯ liá»‡u thÃ´ Ä‘Æ°á»£c lÃ m sáº¡ch cÆ¡ báº£n.
    *   Destination: Ghi trá»±c tiáº¿p vÃ o báº£ng `staging_jobs` trÃªn Supabase.

2.  **OLTP Layer (Supabase - PostgreSQL):**
    *   Vai trÃ²: Operational Database & Staging Area.
    *   Chá»©c nÄƒng: 
        *   LÆ°u trá»¯ dá»¯ liá»‡u "nÃ³ng" (viá»‡c lÃ m Ä‘ang tuyá»ƒn, dá»¯ liá»‡u má»›i nháº¥t).
        *   Cung cáº¥p API cho **Website tra cá»©u viá»‡c lÃ m**.
        *   Táº­n dá»¥ng tÃ­nh nÄƒng Realtime/Auth cá»§a Supabase Ä‘á»ƒ xÃ¢y dá»±ng App nhanh chÃ³ng.

3.  **Orchestration Layer (Airflow):**
    *   Vai trÃ²: Äiá»u phá»‘i luá»“ng dá»¯ liá»‡u (ETL Pipeline).
    *   Nhiá»‡m vá»¥:
        *   LÃªn lá»‹ch cháº¡y Spider Ä‘á»‹nh ká»³.
        *   **Sync Job:** Query dá»¯ liá»‡u má»›i tá»« Supabase -> Load vÃ o Google BigQuery (Batch processing).

4.  **OLAP Layer (Google BigQuery):**
    *   Vai trÃ²: Data Warehouse (Kho dá»¯ liá»‡u phÃ¢n tÃ­ch).
    *   Chá»©c nÄƒng:
        *   LÆ°u trá»¯ lá»‹ch sá»­ dÃ i háº¡n (Historical Data).
        *   Xá»­ lÃ½ cÃ¡c truy váº¥n náº·ng: PhÃ¢n tÃ­ch xu hÆ°á»›ng lÆ°Æ¡ng, ká»¹ nÄƒng hot, biáº¿n Ä‘á»™ng thá»‹ trÆ°á»ng.
        *   Nguá»“n dá»¯ liá»‡u cho cÃ¡c bÃ¡o cÃ¡o Insight (Looker Studio, Metabase).

## ğŸ“‚ Cáº¥u TrÃºc ThÆ° Má»¥c (Project Structure)

```text
CrawlJob/
â”œâ”€â”€ airflow/                # Airflow DAGs & Configuration
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ sync_supabase_bigquery.py  # ETL: Supabase -> BigQuery
â”‚       â””â”€â”€ trigger_spiders.py         # Schedule Scrapy Jobs
â”œâ”€â”€ api/                    # Backend API (náº¿u cáº§n custom logic ngoÃ i Supabase)
â”œâ”€â”€ CrawlJob/               # Scrapy Project Core
â”‚   â”œâ”€â”€ spiders/            # CÃ¡c Spider thu tháº­p dá»¯ liá»‡u
â”‚   â”œâ”€â”€ items.py            # Äá»‹nh nghÄ©a cáº¥u trÃºc dá»¯ liá»‡u (Data Models)
â”‚   â”œâ”€â”€ pipelines.py        # Xá»­ lÃ½ dá»¯ liá»‡u trÆ°á»›c khi lÆ°u vÃ o Supabase
â”‚   â””â”€â”€ settings.py         # Cáº¥u hÃ¬nh Scrapy (Delay, User-Agent, DB Connect)
â”œâ”€â”€ notebooks/              # Jupyter Notebooks (EDA, Data Analysis, Test DuckDB/BQ)
â”œâ”€â”€ scripts/                # Utility Scripts (Cháº¡y spider thá»§ cÃ´ng, helper tools)
â”œâ”€â”€ web/                    # Frontend (Website tra cá»©u viá»‡c lÃ m Ä‘Æ¡n giáº£n)
â”œâ”€â”€ docker-compose.yml      # Setup mÃ´i trÆ°á»ng (Airflow, Local DB...)
â”œâ”€â”€ requirements.txt        # Python Dependencies
â””â”€â”€ README.md               # Project Documentation
```

## ğŸš€ Getting Started

### 1. Prerequisites
*   Python 3.10+
*   Docker & Docker Compose (cho Airflow)
*   TÃ i khoáº£n Supabase & Google Cloud Platform (BigQuery API enabled)

### 2. Setup Environment
```bash
# Clone project
git clone <repo-url>
cd CrawlJob

# Táº¡o mÃ´i trÆ°á»ng áº£o
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# CÃ i Ä‘áº·t thÆ° viá»‡n
pip install -r requirements.txt
```

### 3. Configuration (.env)
Táº¡o file `.env` tá»« `env.example` vÃ  Ä‘iá»n cÃ¡c thÃ´ng tin credentials:
```ini
# Supabase
SUPABASE_URL=...
SUPABASE_KEY=...
DB_CONNECTION_STRING=postgresql://user:pass@host:port/dbname

# Google Cloud (BigQuery)
GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account.json
BQ_PROJECT_ID=...
BQ_DATASET_ID=...
```


