# CrawlJob - Vietnam Job Market Analytics Platform

D·ª± √°n thu th·∫≠p, l∆∞u tr·ªØ v√† ph√¢n t√≠ch d·ªØ li·ªáu vi·ªác l√†m ngh√†nh data t·ª´ c√°c trang tuy·ªÉn d·ª•ng l·ªõn t·∫°i Vi·ªát Nam (TopCV, Linkedin, ITViec, JobStreet, v.v.). H·ªá th·ªëng ƒë∆∞·ª£c thi·∫øt k·∫ø theo ki·∫øn tr√∫c Hybrid, t√°ch bi·ªát gi·ªØa nhu c·∫ßu truy xu·∫•t nhanh cho ·ª©ng d·ª•ng (OLTP) v√† nhu c·∫ßu ph√¢n t√≠ch d·ªØ li·ªáu l·ªõn (OLAP).

## üèó Ki·∫øn Tr√∫c H·ªá Th·ªëng (Architecture)

M√¥ h√¨nh t·ªïng quan lu·ªìng d·ªØ li·ªáu (Data Flow):

```mermaid
graph TB
    subgraph Scheduling_Layer["üóìÔ∏è Scheduling Layer"]
        subgraph GitHub_Actions["‚òÅÔ∏è GitHub Actions (Cloud Server)"]
            GA_CRON["‚è∞ Cron Schedule<br/>*/6 * * * *"]
            GA_TOPCV["üï∑Ô∏è 123Job Spider"]
            GA_VNW["üï∑Ô∏è VietnamWorks Spider"]
            GA_SCRIPT["üìú Python Script<br/>run_spider.py"]
        end
        
        subgraph Airflow_Local["üè† Apache Airflow (Local Server)"]
            AF_DAG["üìã DAG: scrape_hard_sites<br/>0 2 * * *"]
            AF_LINKEDIN["üï∑Ô∏è LinkedIn Spider"]
            AF_GLASSDOOR["üï∑Ô∏è TopCV Spider"]
            AF_ANTIBOT["üõ°Ô∏è Anti-bot Handler<br/>Proxy + Rotating UA"]
            AF_SCRIPT["üìú Python Script<br/>run_spider.py"]
        end
    end

    subgraph Collection_Layer["üîç Collection Layer"]
        A["üï∏Ô∏è Scrapy Spiders"]
    end

    subgraph OLTP_Staging["üíæ OLTP / Staging (Supabase)"]
        B[("üì• staging_jobs")]
        B3[("‚ö†Ô∏è quarantine_jobs")]
        B2[("‚úÖ jobs")]
        B1["üñ•Ô∏è Web App Backend"]
    end

    subgraph Orchestration["‚öôÔ∏è Orchestration (Airflow)"]
        C1["üì§ Task: Extract"]
        C2["‚úîÔ∏è Task: Validate DQ"]
        C3["üîÑ Task: Upsert"]
        C4["üì¶ Task: Load to DW"]
    end

    subgraph OLAP_DW["üìä OLAP / Data Warehouse (BigQuery)"]
        D1[("üóÉÔ∏è raw_jobs")]
        D2[("üè∑Ô∏è dim_skills")]
        D3[("üìà fact_market_trends")]
    end

    subgraph User_Interface["üë§ User Interface"]
        E["üåê Job Search Website"]
        F["üìä BI Dashboard"]
    end

    %% GitHub Actions Flow
    GA_CRON --> GA_TOPCV
    GA_CRON --> GA_VNW
    GA_TOPCV --> GA_SCRIPT
    GA_VNW --> GA_SCRIPT
    GA_SCRIPT --> A

    %% Airflow Local Flow
    AF_DAG --> AF_LINKEDIN
    AF_DAG --> AF_GLASSDOOR
    AF_LINKEDIN --> AF_ANTIBOT
    AF_GLASSDOOR --> AF_ANTIBOT
    AF_ANTIBOT --> AF_SCRIPT
    AF_SCRIPT --> A

    %% Collection to Staging
    A -->|"Insert Raw"| B
    
    %% ETL Pipeline
    C1 -->|"Read"| B
    C1 --> C2
    C2 -->|"PASS ‚úÖ"| C3
    C2 -->|"FAIL ‚ùå"| B3
    C3 -->|"Upsert"| B2
    C3 --> C4
    C4 -->|"Batch Load"| D1
    
    %% Backend & UI
    B2 <-->|"Read/Write"| B1
    B1 --> E
    
    %% Data Warehouse Transform
    D1 --> D2
    D1 --> D3
    D3 --> F

    %% Styling
    classDef github fill:#24292e,stroke:#ffffff,color:#ffffff
    classDef airflow fill:#017cee,stroke:#ffffff,color:#ffffff
    classDef scrapy fill:#60a839,stroke:#ffffff,color:#ffffff
    classDef supabase fill:#3ecf8e,stroke:#ffffff,color:#ffffff
    classDef bigquery fill:#4285f4,stroke:#ffffff,color:#ffffff
    classDef ui fill:#ff6b6b,stroke:#ffffff,color:#ffffff

    class GA_CRON,GA_TOPCV,GA_VNW,GA_SCRIPT github
    class AF_DAG,AF_LINKEDIN,AF_GLASSDOOR,AF_ANTIBOT,AF_SCRIPT airflow
    class A scrapy
    class B,B2,B3,B1 supabase
    class D1,D2,D3 bigquery
    class E,F ui
```


## üöÄ Getting Started

### 1. Prerequisites
*   Python 3.10+
*   Docker & Docker Compose (cho Airflow)
*   T√†i kho·∫£n Supabase & Google Cloud Platform (BigQuery API enabled)

### 2. Setup Environment
```bash
# Clone project
git clone <repo-url>
cd CrawlJob

# T·∫°o m√¥i tr∆∞·ªùng ·∫£o
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# C√†i ƒë·∫∑t th∆∞ vi·ªán
pip install -r requirements.txt
```

### 3. Configuration (.env)
T·∫°o file `.env` t·ª´ `env.example` v√† ƒëi·ªÅn c√°c th√¥ng tin credentials:
```ini
# Supabase
SUPABASE_URL=...
SUPABASE_KEY=...
DB_CONNECTION_STRING=postgresql://user:pass@host:port/dbname

# Google Cloud (BigQuery)
GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account.json
BQ_PROJECT_ID=...
BQ_DATASET_ID=...
```


