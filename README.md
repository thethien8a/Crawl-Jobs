# Job Scraping Project

Dá»± Ã¡n web scraping Ä‘á»ƒ láº¥y dá»¯ liá»‡u viá»‡c lÃ m tá»« **10 trang tuyá»ƒn dá»¥ng Viá»‡t Nam** vá»›i kiáº¿n trÃºc **modular** vÃ  **production-ready**.

## ğŸ¯ TÃ­nh nÄƒng

- **Input**: Tá»« khÃ³a viá»‡c lÃ m (VD: "Python Developer", "Data Analyst")
- **Output**: Dá»¯ liá»‡u viá»‡c lÃ m Ä‘Æ°á»£c lÆ°u vÃ o SQL Server vá»›i deduplication
- **Sites**: JobsGO, JobOKO, 123job, CareerViet, JobStreet, LinkedIn (public), TopCV, ITviec, CareerLink, VietnamWorks
- **Data**: Job title, company, salary, location, requirements, job_deadline, benefits, etc.
- **API**: FastAPI REST endpoints vá»›i pagination vÃ  search
- **Web Dashboard**: Bootstrap UI vá»›i search vÃ  pagination
- **Automation**: Windows Task Scheduler cho daily crawling

## ğŸ“‹ CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### 2. Cáº¥u hÃ¬nh SQL Server

Chá»‰nh sá»­a file `CrawlJob/settings.py`:

```python
# SQL Server Database Configuration
SQL_SERVER = "localhost"  # Thay Ä‘á»•i thÃ nh SQL Server instance cá»§a báº¡n
SQL_DATABASE = "JobDatabase"  # Thay Ä‘á»•i thÃ nh tÃªn database
SQL_USERNAME = "sa"  # Thay Ä‘á»•i thÃ nh username
SQL_PASSWORD = "your_password"  # Thay Ä‘á»•i thÃ nh password
```

Hoáº·c táº¡o file `.env` á»Ÿ thÆ° má»¥c gá»‘c (khuyáº¿n nghá»‹ an toÃ n):

```env
SQL_SERVER=localhost
SQL_DATABASE=JobDatabase
SQL_USERNAME=sa
SQL_PASSWORD=your_password
```

Ghi chÃº: `settings.py` Ä‘Ã£ tá»± Ä‘á»™ng Ä‘á»c biáº¿n mÃ´i trÆ°á»ng (qua `python-dotenv`) náº¿u cÃ³ file `.env`.

### 3. Táº¡o database

Táº¡o database `JobDatabase` trong SQL Server. Pipeline sáº½ **táº¡o báº£ng `jobs` náº¿u chÆ°a tá»“n táº¡i** khi cháº¡y láº§n Ä‘áº§u.

## ğŸš€ Sá»­ dá»¥ng

### Sá»­ dá»¥ng script run_spider.py

```bash
# Cháº¡y spider JobsGO
python run_spider.py --spider jobsgo --keyword "Data Analyst" --output jobsgo.json

# Cháº¡y spider JobOKO
python run_spider.py --spider joboko --keyword "Data Analyst" --output joboko.json

# Cháº¡y spider 123Job
python run_spider.py --spider job123 --keyword "Data Analyst" --output job123.json

# Cháº¡y spider CareerViet
python run_spider.py --spider careerviet --keyword "Data Analyst" --output careerviet.json

# Cháº¡y spider JobStreet
python run_spider.py --spider jobstreet --keyword "Data Analyst" --output jobstreet.json

# Cháº¡y spider LinkedIn (public): click list â†’ Ä‘á»c panel pháº£i
python run_spider.py --spider linkedin --keyword "Data Analyst" --output linkedin.json

# Cháº¡y spider TopCV
python run_spider.py --spider topcv --keyword "Data Analyst" --output topcv.json

# Cháº¡y spider ITviec
python run_spider.py --spider itviec --keyword "Data Analyst" --output itviec.json

# Cháº¡y spider CareerLink
python run_spider.py --spider careerlink --keyword "Data Analyst" --output careerlink.json

# Cháº¡y spider VietnamWorks
python run_spider.py --spider vietnamworks --keyword "Data Analyst" --output vietnamworks.json

# Cháº¡y táº¥t cáº£ spider
python run_spider.py --spider all --keyword "Data Analyst" --output all_jobs.json
```

Ghi chÃº: LinkedIn lÃ  site Ä‘á»™ng; spider dÃ¹ng Selenium click tá»«ng job á»Ÿ danh sÃ¡ch Ä‘á»ƒ hiá»ƒn thá»‹ panel pháº£i vÃ  trÃ­ch xuáº¥t mÃ´ táº£/chi tiáº¿t cÆ¡ báº£n (khÃ´ng Ä‘Äƒng nháº­p). UI cÃ³ thá»ƒ thay Ä‘á»•i theo thá»i gian, cáº§n Ä‘iá»u chá»‰nh selector khi cáº§n.

### API Read-Only (FastAPI)

```bash
pip install -r requirements.txt
uvicorn api.main:app --reload

# Kiá»ƒm tra
curl http://127.0.0.1:8000/health
curl "http://127.0.0.1:8000/jobs?keyword=python&site=jobsgo&page=1&page_size=20"
```

## ğŸ“Š Cáº¥u trÃºc dá»¯ liá»‡u

Báº£ng `jobs` trong SQL Server:

| Field | Type | Description |
|-------|------|-------------|
| id | INT | Primary key |
| job_title | NVARCHAR(500) | TÃªn cÃ´ng viá»‡c |
| company_name | NVARCHAR(500) | TÃªn cÃ´ng ty |
| salary | NVARCHAR(200) | Má»©c lÆ°Æ¡ng |
| location | NVARCHAR(200) | Äá»‹a Ä‘iá»ƒm |
| job_type | NVARCHAR(100) | Loáº¡i cÃ´ng viá»‡c (Full-time, Part-time) |
| experience_level | NVARCHAR(200) | YÃªu cáº§u kinh nghiá»‡m |
| education_level | NVARCHAR(200) | YÃªu cáº§u há»c váº¥n |
| job_industry | NVARCHAR(200) | NgÃ nh nghá» |
| job_position | NVARCHAR(200) | Chá»©c vá»¥/Vá»‹ trÃ­ |
| job_description | NVARCHAR(MAX) | MÃ´ táº£ cÃ´ng viá»‡c |
| requirements | NVARCHAR(MAX) | YÃªu cáº§u cÃ´ng viá»‡c |
| benefits | NVARCHAR(MAX) | PhÃºc lá»£i |
| job_deadline | NVARCHAR(200) | Háº¡n cuá»‘i ná»™p CV |
| source_site | NVARCHAR(100) | Nguá»“n dá»¯ liá»‡u |
| job_url | NVARCHAR(1000) | URL cÃ´ng viá»‡c |
| search_keyword | NVARCHAR(200) | Tá»« khÃ³a tÃ¬m kiáº¿m |
| scraped_at | NVARCHAR(50) | Thá»i gian scrape |
| created_at | DATETIME | Thá»i gian táº¡o record |

LÆ°u Ã½: Náº¿u báº£ng `jobs` Ä‘Ã£ tá»“n táº¡i trÆ°á»›c khi thÃªm cá»™t má»›i (vÃ­ dá»¥ `job_position`) thÃ¬ cáº§n ALTER thá»§ cÃ´ng:

```sql
IF COL_LENGTH('dbo.jobs','job_position') IS NULL
    ALTER TABLE dbo.jobs ADD job_position NVARCHAR(200) NULL;
```

## ğŸ› ï¸ Cáº¥u trÃºc project

```
CrawlJob/
â”œâ”€â”€ ğŸ“ CrawlJob/                 # Main Scrapy project
â”‚   â”œâ”€â”€ ğŸ“ spiders/              # 10 Job site spiders
â”‚   â”‚   â”œâ”€â”€ jobsgo_spider.py     # JobsGO.vn (Simple Scrapy)
â”‚   â”‚   â”œâ”€â”€ joboko_spider.py     # JobOKO.vn (Simple Scrapy)
â”‚   â”‚   â”œâ”€â”€ job123_spider.py     # 123job.vn (Simple Scrapy)
â”‚   â”‚   â”œâ”€â”€ careerviet_spider.py # CareerViet.vn (Simple Scrapy)
â”‚   â”‚   â”œâ”€â”€ jobstreet_spider.py  # JobStreet.vn (Simple Scrapy)
â”‚   â”‚   â”œâ”€â”€ careerlink_spider.py # CareerLink.vn (Simple Scrapy)
â”‚   â”‚   â”œâ”€â”€ topcv_spider.py      # TopCV.vn (Enhanced Scrapy + JS extraction)
â”‚   â”‚   â”œâ”€â”€ vietnamworks_spider.py # VietnamWorks.com (Hybrid Selenium + Scrapy)
â”‚   â”‚   â”œâ”€â”€ linkedin_spider.py   # LinkedIn.com (Selenium + authentication ready)
â”‚   â”‚   â””â”€â”€ itviec_spider.py     # ITviec.com (Selenium + click navigation)
â”‚   â”œâ”€â”€ items.py                 # JobItem data model (18+ fields)
â”‚   â”œâ”€â”€ pipelines.py             # SQL Server pipeline vá»›i deduplication
â”‚   â”œâ”€â”€ settings.py              # Scrapy configuration & database settings
â”‚   â”œâ”€â”€ selenium_middleware.py   # Selenium integration middleware
â”‚   â””â”€â”€ utils.py                 # Helper functions (encode_input, clean_location)
â”œâ”€â”€ ğŸ“ api/                      # FastAPI backend
â”‚   â””â”€â”€ main.py                  # REST API endpoints (/health, /jobs)
â”œâ”€â”€ ğŸ“ debug/                    # Debug utilities (NEW)
â”‚   â””â”€â”€ HTML_export_debug.py     # HTML export tool cho selector testing
â”œâ”€â”€ ğŸ“ web/                      # Web dashboard (Modular Architecture)
â”‚   â”œâ”€â”€ index.html               # Trang chÃ­nh cá»§a dashboard
â”‚   â”œâ”€â”€ css/                     # Stylesheets
â”‚   â”‚   â”œâ”€â”€ styles.css          # CSS chÃ­nh
â”‚   â”‚   â””â”€â”€ responsive.css      # Responsive design
â”‚   â”œâ”€â”€ js/                     # JavaScript modules
â”‚   â”‚   â”œâ”€â”€ main.js            # Logic chÃ­nh cá»§a á»©ng dá»¥ng
â”‚   â”‚   â”œâ”€â”€ api.js             # API communication layer
â”‚   â”‚   â””â”€â”€ ui.js              # UI helper functions
â”‚   â””â”€â”€ README.md               # Comprehensive documentation
â”œâ”€â”€ ğŸ“ logs/                     # Crawling logs (timestamped)
â”œâ”€â”€ ğŸ“ outputs/                  # JSON output files (timestamped)
â”œâ”€â”€ ğŸ“„ run_spider.py             # CLI runner cho táº¥t cáº£ spiders
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies (11 packages)
â”œâ”€â”€ ğŸ“„ scrapy.cfg                # Scrapy project configuration
â”œâ”€â”€ ğŸ“„ crawl_daily.bat           # Windows Task Scheduler automation
â”œâ”€â”€ ğŸ“„ env.example               # Environment variables template
â”œâ”€â”€ ğŸ“„ test.ipynb                # Jupyter notebook cho testing
â”œâ”€â”€ ğŸ“„ vietnamworks.json         # VietnamWorks output sample
â””â”€â”€ ğŸ“„ README.md                 # HÆ°á»›ng dáº«n sá»­ dá»¥ng
```

### ğŸ†• **Spider Categories**
- **Simple Scrapy** (6 sites): JobsGO, JobOKO, 123job, CareerViet, JobStreet, CareerLink
- **Enhanced Scrapy** (2 sites): TopCV (JavaScript extraction), ITviec (Advanced selectors)
- **Hybrid Selenium + Scrapy** (1 site): VietnamWorks (Selenium URL collection + Scrapy parsing)
- **Selenium-Based** (1 site): LinkedIn (Browser automation)

## âš™ï¸ Cáº¥u hÃ¬nh nÃ¢ng cao

### Thay Ä‘á»•i delay giá»¯a cÃ¡c request

Chá»‰nh sá»­a `DOWNLOAD_DELAY` trong `settings.py`:

```python
DOWNLOAD_DELAY = 2  # Delay 2 giÃ¢y giá»¯a cÃ¡c request
```

### Thay Ä‘á»•i sá»‘ lÆ°á»£ng request Ä‘á»“ng thá»i

```python
CONCURRENT_REQUESTS = 16  # Sá»‘ request Ä‘á»“ng thá»i
```

### ThÃªm User Agent

```python
USER_AGENT = "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Mobile Safari/537.36"
```

## ğŸ› Debug & Testing Tools

### HTML Export Tool
Script `debug/HTML_export_debug.py` Ä‘á»ƒ export HTML tá»« job sites cho viá»‡c testing selectors:

```bash
cd debug
python HTML_export_debug.py
```

### Jupyter Notebook Testing
File `test.ipynb` cho testing vÃ  development:

```bash
jupyter notebook test.ipynb
```

### Sample Output Files
- `vietnamworks.json` - Sample output tá»« VietnamWorks spider
- `outputs/jobs_*.json` - Timestamped output files
- `logs/crawl_*.log` - Timestamped log files

## ğŸ—“ï¸ Scheduling (Windows Task Scheduler)

1. Test thá»§ cÃ´ng file batch:
```bat
cd /d D:\Practice\Scrapy\CrawlJob
crawl_daily.bat
```
- Káº¿t quáº£: táº¡o `outputs\jobs_YYYY-MM-DD_HH-mm-ss.json` vÃ  `logs\crawl_YYYY-MM-DD_HH-mm-ss.log`.

2. Táº¡o task tá»± Ä‘á»™ng (GUI):
- Task Scheduler â†’ Create Taskâ€¦
- General: Run whether user is logged on or not; Run with highest privileges
- Triggers: Daily 02:00
- Actions:
  - Program/script: `cmd.exe`
  - Add arguments: `/c D:\Practice\Scrapy\CrawlJob\crawl_daily.bat`
  - Start in: `D:\Practice\Scrapy\CrawlJob`
- Nháº¥n Run Ä‘á»ƒ test

3. Táº¡o task báº±ng lá»‡nh (tÃ¹y chá»n):
```bat
SCHTASKS /Create /TN "CrawlJob Daily" /TR "cmd.exe /c Path_to\crawl_daily.bat" /SC DAILY /ST 02:00 /RL HIGHEST /F
```

4. LÆ°u Ã½:
- Náº¿u `python` khÃ´ng nháº­n diá»‡n, dÃ¹ng full path tá»›i `python.exe` trong `crawl_daily.bat`.
- Náº¿u dÃ¹ng venv, bá» comment dÃ²ng `call ...activate.bat` trong batch.
- Äáº£m báº£o SQL Server báº­t TCP/IP vÃ  cá»•ng Ä‘Ãºng (thÆ°á»ng 1433), `.env` trá» Ä‘Ãºng `SQL_SERVER`.

### Chi tiáº¿t cáº¥u hÃ¬nh Task Scheduler (GUI)

1) Má»Ÿ Task Scheduler â†’ Create Taskâ€¦ (khÃ´ng pháº£i Basic Task)
- Tab General:
  - Name: CrawlJob Daily (hoáº·c tÃªn báº¡n muá»‘n)
  - Description: Cháº¡y `crawl_daily.bat` Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u háº±ng ngÃ y
  - Chá»n "Run whether user is logged on or not"
  - Tick "Run with highest privileges"
  - Configure for: Windows 10/11
- Tab Triggers â†’ Newâ€¦
  - Begin the task: On a schedule
  - Daily, Start at: 02:00 (vÃ­ dá»¥)
  - (Tuá»³ chá»n) Advanced: Repeat task every: 4 hours; For a duration of: Indefinitely â†’ dÃ¹ng khi muá»‘n cháº¡y nhiá»u láº§n/ngÃ y
  - OK
- Tab Actions â†’ Newâ€¦
  - Action: Start a program
  - Program/script: `cmd.exe`
  - Add arguments: `/c "D:\Practice\Scrapy\CrawlJob\crawl_daily.bat"`
  - Start in (optional): `D:\Practice\Scrapy\CrawlJob`
  - LÆ°u Ã½: luÃ´n bá»c Ä‘Æ°á»ng dáº«n cÃ³ dáº¥u cÃ¡ch trong dáº¥u nhÃ¡y kÃ©p ""
  - OK
- Tab Conditions: tuá»³ nhu cáº§u (vÃ­ dá»¥ bá» chá»n "Start the task only if the computer is on AC power")
- Tab Settings:
  - Cho phÃ©p "Allow task to be run on demand"
  - Náº¿u task cÃ³ thá»ƒ cháº¡y lÃ¢u: Ä‘iá»u chá»‰nh "Stop the task if it runs longer than"
- Nháº¥n OK vÃ  nháº­p máº­t kháº©u user náº¿u Ä‘Æ°á»£c yÃªu cáº§u

2) Cháº¡y test ngay
- Trong Task Scheduler, chá»n task â†’ Run
- Kiá»ƒm tra:
  - File `outputs\jobs_*.json` Ä‘Æ°á»£c sinh
  - File `logs\crawl_*.log` cÃ³ ná»™i dung log

3) Táº¡o task báº±ng dÃ²ng lá»‡nh (tÃ¹y chá»n)
```bat
REM ÄÆ°á»ng dáº«n generic (sá»­a Path_to cho phÃ¹ há»£p)
SCHTASKS /Create /TN "CrawlJob Daily" /TR "cmd.exe /c \"Path_to\\crawl_daily.bat\"" /SC DAILY /ST 02:00 /RL HIGHEST /F

REM VÃ­ dá»¥ theo project nÃ y
SCHTASKS /Create /TN "CrawlJob Daily" /TR "cmd.exe /c \"D:\\Practice\\Scrapy\\CrawlJob\\crawl_daily.bat\"" /SC DAILY /ST 02:00 /RL HIGHEST /F

REM Cháº¡y má»—i 4 giá» (láº·p vÃ´ háº¡n) báº¯t Ä‘áº§u tá»« 00:00
SCHTASKS /Create /TN "CrawlJob Every4H" /TR "cmd.exe /c \"D:\\Practice\\Scrapy\\CrawlJob\\crawl_daily.bat\"" /SC HOURLY /MO 4 /ST 00:00 /RL HIGHEST /F

REM Cháº¡y dÆ°á»›i tÃ i khoáº£n SYSTEM (khÃ´ng cáº§n Ä‘Äƒng nháº­p)
SCHTASKS /Create /TN "CrawlJob SYSTEM" /TR "cmd.exe /c \"D:\\Practice\\Scrapy\\CrawlJob\\crawl_daily.bat\"" /SC DAILY /ST 02:00 /RU SYSTEM /RL HIGHEST /F
```

4) Gá»£i Ã½ cáº¥u hÃ¬nh trong `crawl_daily.bat`
- Náº¿u dÃ¹ng virtualenv, bá» comment dÃ²ng `call ...activate.bat` vÃ  sá»­a path cho Ä‘Ãºng
- Náº¿u `python` khÃ´ng cÃ³ trong PATH cá»§a dá»‹ch vá»¥, dÃ¹ng full path tá»›i `python.exe` (Ä‘Ã£ cÃ³ dÃ²ng máº«u trong file .bat)
- CÃ³ thá»ƒ Ä‘á»•i `--keyword` theo nhu cáº§u

5) Troubleshooting Task Scheduler
- "The system cannot find the file specified": kiá»ƒm tra quotes vÃ  Ä‘Æ°á»ng dáº«n trong Program/script, Arguments, Start in
- Exit code 1/2: xem file log trong `logs\crawl_*.log` Ä‘á»ƒ biáº¿t lá»—i chi tiáº¿t (selector, SQL, máº¡ngâ€¦)
- KhÃ´ng táº¡o ra output/log: kiá»ƒm tra quyá»n ghi thÆ° má»¥c hoáº·c dÃ¹ng Start in Ä‘á»ƒ Ä‘áº·t Working Directory Ä‘Ãºng
- KhÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c SQL Server: kiá»ƒm tra TCP/IP, port 1433, firewall; `.env` Ä‘Ãºng `SQL_SERVER`

## ğŸ”§ Troubleshooting

### Lá»—i káº¿t ná»‘i SQL Server
1. Kiá»ƒm tra SQL Server Ä‘ang cháº¡y
2. Kiá»ƒm tra `.env`: `SQL_SERVER=localhost,1433` hoáº·c `localhost\SQLEXPRESS`
3. Báº­t TCP/IP vÃ  má»Ÿ firewall port 1433
4. Kiá»ƒm tra database permissions cho user

### Lá»—i scraping
1. Kiá»ƒm tra internet connection
2. Thá»­ tÄƒng `DOWNLOAD_DELAY` trong `settings.py`
3. Kiá»ƒm tra website cÃ³ thay Ä‘á»•i cáº¥u trÃºc HTML khÃ´ng
4. Sá»­ dá»¥ng debug tools Ä‘á»ƒ export HTML: `python debug/HTML_export_debug.py`

### Lá»—i CSS selector
- Cáº­p nháº­t selector trong spider náº¿u website Ä‘á»•i HTML
- Sá»­ dá»¥ng `debug/HTML_export_debug.py` Ä‘á»ƒ test selectors
- Check `logs/crawl_*.log` cho error messages

### Spider-Specific Issues

#### Selenium Spiders (LinkedIn, ITviec)
- **ChromeDriver issues**: CÃ i Ä‘áº·t `webdriver-manager` hoáº·c update Chrome
- **Anti-detection**: Spiders cÃ³ anti-detection measures built-in
- **Login required**: Má»™t sá»‘ sites yÃªu cáº§u authentication (ITviec)
- **Slow performance**: Selenium spiders cháº­m hÆ¡n Scrapy spiders

#### JavaScript-Heavy Sites (TopCV)
- **Dynamic content**: Sá»­ dá»¥ng enhanced parsing vá»›i JavaScript extraction
- **Missing data**: Má»™t sá»‘ fields cÃ³ thá»ƒ missing do dynamic loading
- **Rate limiting**: TopCV cÃ³ strict rate limiting

#### Advanced Scrapy (VietnamWorks)
- **Complex selectors**: Sá»­ dá»¥ng multiple fallback selectors
- **Pagination**: Limited to 5 pages Ä‘á»ƒ trÃ¡nh blocking
- **Data quality**: High quality data vá»›i comprehensive fields

### Debug Tools Usage
```bash
# Export HTML Ä‘á»ƒ debug selectors
cd debug
python HTML_export_debug.py

# Check logs cho errors
type logs\crawl_*.log

# Test individual spider
python run_spider.py --spider topcv --keyword "python" --output debug.json
```

### Performance Optimization
1. **Rate Limiting**: Adjust `DOWNLOAD_DELAY` based on site restrictions
2. **Concurrent Requests**: Reduce `CONCURRENT_REQUESTS` náº¿u bá»‹ block
3. **Memory Usage**: Monitor RAM usage vá»›i large datasets
4. **Database Performance**: Ensure SQL Server cÃ³ Ä‘á»§ resources

## ğŸ“ Ghi chÃº

- **10 Job Sites**: Coverage toÃ n diá»‡n cÃ¡c trang tuyá»ƒn dá»¥ng lá»›n táº¡i Viá»‡t Nam
- **Smart Deduplication**: Loáº¡i bá» duplicate dá»±a trÃªn `(job_title, company_name, source_site)`
- **Rate Limiting**: Respectful crawling vá»›i 2s delay giá»¯a requests
- **Error Resilience**: Graceful handling cho individual spider failures
- **Production Ready**: Windows Task Scheduler integration
- **Modular Architecture**: Dá»… dÃ ng thÃªm job sites má»›i
- **Debug Tools**: Built-in tools cho testing vÃ  troubleshooting
- **Data Quality**: Comprehensive 18+ field data model

### ğŸ†• **Recent Updates**
- **New Spiders**: ITviec (Selenium), VietnamWorks (Advanced Scrapy)
- **Debug Tools**: HTML export utility cho selector testing
- **Enhanced Documentation**: Detailed troubleshooting guides
- **Performance Optimization**: Better memory management vÃ  error handling

### ğŸš€ **Future Enhancements**
- **ML Integration**: Job matching algorithms
- **Real-time Notifications**: Push notifications cho new jobs
- **Advanced Analytics**: Salary analysis vÃ  trend detection
- **API Rate Limiting**: Production-ready API management

## ğŸ“„ License

MIT License
