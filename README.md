# CrawlJob - Professional Data Engineering Project ğŸ‰

Há»‡ thá»‘ng ká»¹ thuáº­t dá»¯ liá»‡u chuyÃªn nghiá»‡p Ä‘á»ƒ thu tháº­p, kiá»ƒm tra cháº¥t lÆ°á»£ng, biáº¿n Ä‘á»•i vÃ  trá»±c quan hÃ³a dá»¯ liá»‡u viá»‡c lÃ m tá»« **10 trang tuyá»ƒn dá»¥ng hÃ ng Ä‘áº§u Viá»‡t Nam**. Dá»± Ã¡n nÃ y khÃ´ng chá»‰ lÃ  má»™t cÃ´ng cá»¥ scraping mÃ  cÃ²n lÃ  má»™t pipeline dá»¯ liá»‡u hoÃ n chá»‰nh, sáºµn sÃ ng cho cÃ¡c tÃ¡c vá»¥ phÃ¢n tÃ­ch vÃ  há»c mÃ¡y.

## ğŸ—ï¸ Kiáº¿n trÃºc Há»‡ thá»‘ng

Dá»± Ã¡n Ä‘Æ°á»£c xÃ¢y dá»±ng theo kiáº¿n trÃºc hiá»‡n Ä‘áº¡i, tÃ¡ch biá»‡t rÃµ rÃ ng cÃ¡c thÃ nh pháº§n, bao gá»“m:
- **Thu tháº­p dá»¯ liá»‡u (Ingestion)**: `Scrapy` & `Selenium` & `BeautifulSoup`
- **Äiá»u phá»‘i (Orchestration)**: `Apache Airflow`
- **LÆ°u trá»¯ (Storage)**: `PostgreSQL` (OLTP) & `DuckDB` (OLAP)
- **Kiá»ƒm tra cháº¥t lÆ°á»£ng (Data Quality)**: `Soda Core` (Raw gating) + `dbt tests` (Business rules)
- **Biáº¿n Ä‘á»•i dá»¯ liá»‡u (Transformation)**: `dbt-duckdb` (biáº¿n Ä‘á»•i trong DuckDB)
- **API & Giao diá»‡n (Presentation)**: `FastAPI` & `Vanilla JS`
- **Trá»±c quan hÃ³a (BI)**: `Apache Superset`

```mermaid
flowchart TD
    subgraph ingestion["ğŸ”„ Data Ingestion"]
        spiders["ğŸ•·ï¸ CrawlJob Spiders"]
        airflow["âš¡ Apache Airflow"]
    end
    subgraph storage["ğŸ’¾ Data Storage"]
        postgres["ğŸ˜ PostgreSQL (OLTP)"]
        duckdb["ğŸ¦† DuckDB (OLAP)"]
    end
    subgraph processing["âš™ï¸ Data Processing"]
        soda["ğŸ§ª Soda Core (Gate Raw)"]
        airbyte["ğŸ§² Airbyte (Sync Postgres â†’ DuckDB)"]
        dbt["ğŸ”¨ dbt-duckdb (Transform + Tests)"]
    end
    subgraph presentation["ğŸ“Š Presentation & Access"]
        superset["Apache Superset (BI)"]
        fastapi["ğŸš€ FastAPI (API)"]
        webapp["ğŸŒ Web App"]
    end
    airflow --> spiders --> postgres
    airflow --> soda --> postgres
    airflow --> airbyte --> duckdb
    airflow --> dbt
    dbt --> duckdb
    fastapi --> postgres
    webapp --> fastapi
    superset --> duckdb
```

## ğŸš€ Báº¯t Ä‘áº§u nhanh (Getting Started)

### YÃªu cáº§u
- Python 3.10+
- Docker & Docker Compose
- Git

### CÃ i Ä‘áº·t & Cáº¥u hÃ¬nh

Thá»±c hiá»‡n cÃ¡c bÆ°á»›c sau theo Ä‘Ãºng thá»© tá»± Ä‘á»ƒ cÃ i Ä‘áº·t mÃ´i trÆ°á»ng development.

**1. Clone Repository**
```bash
git clone <your-repository-url>
cd CrawlJob
```

**2. Táº¡o vÃ  kÃ­ch hoáº¡t MÃ´i trÆ°á»ng áº£o**
```bash
# Táº¡o mÃ´i trÆ°á»ng áº£o
python -m venv .venv

# KÃ­ch hoáº¡t (Windows)
.\.venv\Scripts\activate
```

**3. CÃ i Ä‘áº·t cÃ¡c gÃ³i phá»¥ thuá»™c**
```bash
pip install -r requirements.txt
```

**4. Cáº¥u hÃ¬nh Biáº¿n mÃ´i trÆ°á»ng**
Copy file `.env.example` thÃ nh file `.env` vÃ  Ä‘iá»n cÃ¡c thÃ´ng tin cáº§n thiáº¿t.
```bash
# Windows
copy .env.example .env
```
Sau Ä‘Ã³, má»Ÿ file `.env` vÃ  Ä‘iá»n thÃ´ng tin Ä‘Äƒng nháº­p PostgreSQL, tÃ i khoáº£n ITviec, LinkedIn, v.v.

**5. Khá»Ÿi Ä‘á»™ng Database**
Dá»± Ã¡n sá»­ dá»¥ng PostgreSQL cháº¡y trong Docker. HÃ£y khá»Ÿi Ä‘á»™ng container:
```bash
docker-compose up -d
```
Lá»‡nh nÃ y sáº½ khá»Ÿi Ä‘á»™ng má»™t service PostgreSQL cÃ³ thá»ƒ truy cáº­p táº¡i `localhost:5432`.

```
Lá»‡nh nÃ y sáº½ há»i báº¡n má»™t vÃ i cÃ¢u, hÃ£y nháº¥n `Enter` Ä‘á»ƒ cháº¥p nháº­n cÃ¡c giÃ¡ trá»‹ máº·c Ä‘á»‹nh.

**6. Cháº¡y thá»­ nghiá»‡m**
BÃ¢y giá» báº¡n Ä‘Ã£ sáºµn sÃ ng! HÃ£y thá»­ cháº¡y má»™t spider Ä‘á»ƒ kiá»ƒm tra:
```bash
python run_spider.py --spider itviec --keyword "Data Engineer"
```
Dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c thu tháº­p vÃ  lÆ°u vÃ o database PostgreSQL cá»§a báº¡n.

### Cáº¥u trÃºc thÆ° má»¥c chi tiáº¿t

```
CrawlJob/
â”œâ”€â”€ .env.example              # Template cho biáº¿n mÃ´i trÆ°á»ng
â”œâ”€â”€ .gitignore                # CÃ¡c file vÃ  thÆ° má»¥c Ä‘Æ°á»£c Git bá» qua
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py               # API Server (FastAPI)
â”œâ”€â”€ CrawlJob/                 # Source code chÃ­nh cá»§a Scrapy
â”‚   â”œâ”€â”€ spiders/              # Chá»©a 10 spiders cho cÃ¡c trang web
â”‚   â”œâ”€â”€ items.py              # Äá»‹nh nghÄ©a cáº¥u trÃºc dá»¯ liá»‡u JobItem
â”‚   â”œâ”€â”€ pipelines.py          # Xá»­ lÃ½ vÃ  lÆ°u trá»¯ dá»¯ liá»‡u vÃ o PostgreSQL
â”‚   â””â”€â”€ settings.py           # Cáº¥u hÃ¬nh cá»§a Scrapy
â”œâ”€â”€ airflow/                  # Airflow DAGs (pipeline orchestration)
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ crawljob_pipeline.py
â”œâ”€â”€ soda/                     # Data Quality (Raw Gating with Soda Core)
â”‚   â”œâ”€â”€ configuration.yml     # Káº¿t ná»‘i Postgres (dÃ¹ng .env)
â”‚   â””â”€â”€ checks/
â”‚       â””â”€â”€ raw_jobs.yml      # Kiá»ƒm tra báº£ng raw
â”œâ”€â”€ debug/                    # CÃ¡c script há»— trá»£ debug
â”œâ”€â”€ docker-compose.yml        # Äá»‹nh nghÄ©a cÃ¡c service Docker (PostgreSQL)
â”œâ”€â”€ plan/                     # CÃ¡c tÃ i liá»‡u káº¿ hoáº¡ch
â”‚   â””â”€â”€ DATA_ENGINEERING_STACK_PLAN.md
â”œâ”€â”€ README.md                 # TÃ i liá»‡u hÆ°á»›ng dáº«n dá»± Ã¡n
â”œâ”€â”€ requirements.txt          # CÃ¡c gÃ³i Python cáº§n thiáº¿t
â”œâ”€â”€ run_spider.py             # Script Ä‘á»ƒ cháº¡y cÃ¡c spiders tá»« command line
â”œâ”€â”€ scrapy.cfg                # Cáº¥u hÃ¬nh dá»± Ã¡n Scrapy
â”œâ”€â”€ test/                     # CÃ¡c file vÃ  script Ä‘á»ƒ test
â””â”€â”€ web/                      # Giao diá»‡n Frontend (HTML, CSS, JS)
    â”œâ”€â”€ css/
    â”œâ”€â”€ js/
    â””â”€â”€ index.html
```

## ğŸ“– HÆ°á»›ng dáº«n sá»­ dá»¥ng

### Cháº¡y Spiders
Sá»­ dá»¥ng script `run_spider.py` Ä‘á»ƒ thá»±c thi viá»‡c thu tháº­p dá»¯ liá»‡u.

```bash
# Cháº¡y má»™t spider cá»¥ thá»ƒ
python run_spider.py --spider topcv --keyword "Product Manager"

# Cháº¡y táº¥t cáº£ 10 spiders
python run_spider.py --spider all --keyword "IT"
```

### Cháº¡y API Server
API cung cáº¥p dá»¯ liá»‡u Ä‘Ã£ thu tháº­p cho giao diá»‡n web.
```bash
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```
- **Health check**: `http://localhost:8000/health`
- **TÃ¬m kiáº¿m jobs**: `http://localhost:8000/jobs?keyword=python`

### Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u (Data Quality)
Ãp dá»¥ng mÃ´ hÃ¬nh kiá»ƒm tra hai lá»›p + Ä‘á»“ng bá»™:

- Lá»›p 1 (Raw Gating - Soda Core): kiá»ƒm tra báº£ng `raw` ngay sau khi crawl Ä‘á»ƒ Ä‘áº£m báº£o dá»¯ liá»‡u sáºµn sÃ ng.
```bash
# VÃ­ dá»¥ (cháº¡y thá»§ cÃ´ng)
soda scan -d job_database -c soda/configuration.yml soda/checks/raw_jobs_check1.yml
soda scan -d job_database -c soda/configuration.yml soda/checks/raw_jobs_check2.yml
soda scan -d job_database -c soda/configuration.yml soda/checks/raw_jobs_check3.yml
```

- Äá»“ng bá»™ (EL) sang DuckDB: dÃ¹ng Airbyte Ä‘á»ƒ sync tá»« PostgreSQL â†’ DuckDB trÆ°á»›c khi transform.
- Lá»›p 2 (Business Validation - dbt tests): cháº¡y cÃ¡c kiá»ƒm tra cho cÃ¡c model sau khi `dbt run` trong DuckDB.
```bash
# VÃ­ dá»¥ vá»›i dbt-duckdb
cd path/to/dbt_duckdb_project
 dbt run && dbt test
```

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

- **Scrapy & Selenium**: LÃµi thu tháº­p dá»¯ liá»‡u, vá»›i kháº£ nÄƒng vÆ°á»£t qua Cloudflare.
- **PostgreSQL & Docker**: LÆ°u trá»¯ dá»¯ liá»‡u thÃ´, dá»… dÃ ng cÃ i Ä‘áº·t vÃ  quáº£n lÃ½.
- **Soda Core + dbt tests**: Äáº£m báº£o tÃ­nh toÃ n váº¹n vÃ  cháº¥t lÆ°á»£ng dá»¯ liá»‡u (raw + transformed).
- **FastAPI**: XÃ¢y dá»±ng API hiá»‡u nÄƒng cao.
- **VÃ  cÃ¡c cÃ´ng cá»¥ khÃ¡c trong DE Stack**: Airflow, dbt, DuckDB, Superset.

## ğŸ¤ ÄÃ³ng gÃ³p
Náº¿u báº¡n cÃ³ Ã½ tÆ°á»Ÿng cáº£i thiá»‡n dá»± Ã¡n, Ä‘á»«ng ngáº§n ngáº¡i táº¡o Pull Request hoáº·c má»Ÿ má»™t Issue.
