# Job Scraping Project

Dá»± Ã¡n web scraping Ä‘á»ƒ láº¥y dá»¯ liá»‡u viá»‡c lÃ m tá»« cÃ¡c trang tuyá»ƒn dá»¥ng Viá»‡t Nam nhÆ° JobsGO, JobOKO, 123job vÃ  CareerViet.

## ğŸ¯ TÃ­nh nÄƒng

- **Input**: Tá»« khÃ³a viá»‡c lÃ m
- **Output**: Dá»¯ liá»‡u viá»‡c lÃ m Ä‘Æ°á»£c lÆ°u vÃ o SQL Server
- **Sites**: JobsGO, JobOKO, 123job, CareerViet
- **Data**: Job title, company, salary, location, requirements, job_deadline, etc.

## ğŸ“‹ CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### 2. Cáº¥u hÃ¬nh SQL Server

Chá»‰nh sá»­a file `CrawlJob/settings.py`:

```python
# SQL Server Database Configuration
SQL_SERVER = "localhost"  # Thay Ä‘á»•i thÃ nh SQL Server instance cá»§a báº¡n
SQL_DATABASE = "JobDatabase"  # Thay Ä‘á»•i thÃ nh tÃªn database
SQL_USERNAME = "sa"  # Thay Ä‘á»•i thÃ nh username
SQL_PASSWORD = "your_password"  # Thay Ä‘á»•i thÃ nh password
```

### 3. Táº¡o database

Táº¡o database `JobDatabase` trong SQL Server. Spider sáº½ tá»± Ä‘á»™ng táº¡o báº£ng `jobs` khi cháº¡y láº§n Ä‘áº§u.

## ğŸš€ Sá»­ dá»¥ng

### CÃ¡ch 1: Sá»­ dá»¥ng script run_spider.py

```bash
# Cháº¡y spider JobsGO
python run_spider.py --spider jobsgo --keyword "python developer"

# Cháº¡y spider JobOKO
python run_spider.py --spider joboko --keyword "java developer"

# Cháº¡y spider 123job
python run_spider.py --spider 123job --keyword "data analyst"

# Cháº¡y spider CareerViet
python run_spider.py --spider careerviet --keyword "data analyst"

# Cháº¡y táº¥t cáº£ spider
python run_spider.py --spider all --keyword "developer"

# LÆ°u káº¿t quáº£ vÃ o file JSON
python run_spider.py --spider jobsgo --keyword "marketing" --output "marketing_jobs.json"
```

### CÃ¡ch 2: Sá»­ dá»¥ng Scrapy command

```bash
# Cháº¡y spider JobsGO
scrapy crawl jobsgo -a keyword="python developer"

# Cháº¡y spider JobOKO
scrapy crawl joboko -a keyword="java developer"

# Cháº¡y spider 123job
scrapy crawl 123job -a keyword="data analyst"

# Cháº¡y spider CareerViet
scrapy crawl careerviet -a keyword="data analyst"
```

## ğŸ“Š Cáº¥u trÃºc dá»¯ liá»‡u

Báº£ng `jobs` trong SQL Server:

| Field | Type | Description |
|-------|------|-------------|
| id | INT | Primary key |
| job_title | NVARCHAR(500) | TÃªn cÃ´ng viá»‡c |
| company_name | NVARCHAR(500) | TÃªn cÃ´ng ty |
| salary | NVARCHAR(200) | Má»©c lÆ°Æ¡ng |
| location | NVARCHAR(200) | Äá»‹a Ä‘iá»ƒm |
| job_type | NVARCHAR(100) | Loáº¡i cÃ´ng viá»‡c (Full-time, Part-time) |
| experience_level | NVARCHAR(200) | YÃªu cáº§u kinh nghiá»‡m |
| education_level | NVARCHAR(200) | YÃªu cáº§u há»c váº¥n |
| job_industry | NVARCHAR(200) | NgÃ nh nghá» |
| job_position | NVARCHAR(200) | Chá»©c vá»¥/Vá»‹ trÃ­ |
| job_description | NVARCHAR(MAX) | MÃ´ táº£ cÃ´ng viá»‡c |
| requirements | NVARCHAR(MAX) | YÃªu cáº§u cÃ´ng viá»‡c |
| benefits | NVARCHAR(MAX) | PhÃºc lá»£i |
| job_deadline | NVARCHAR(200) | Háº¡n cuá»‘i ná»™p CV |
| source_site | NVARCHAR(100) | Nguá»“n dá»¯ liá»‡u |
| job_url | NVARCHAR(1000) | URL cÃ´ng viá»‡c |
| search_keyword | NVARCHAR(200) | Tá»« khÃ³a tÃ¬m kiáº¿m |
| scraped_at | NVARCHAR(50) | Thá»i gian scrape |
| created_at | DATETIME | Thá»i gian táº¡o record |

## ğŸ› ï¸ Cáº¥u trÃºc project

```
CrawlJob/
â”œâ”€â”€ CrawlJob/
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”œâ”€â”€ jobsgo_spider.py     # Spider cho JobsGO
â”‚   â”‚   â”œâ”€â”€ joboko_spider.py     # Spider cho JobOKO
â”‚   â”‚   â”œâ”€â”€ job123_spider.py     # Spider cho 123job
â”‚   â”‚   â””â”€â”€ careerviet_spider.py # Spider cho CareerViet
â”‚   â”œâ”€â”€ items.py                 # Äá»‹nh nghÄ©a cáº¥u trÃºc dá»¯ liá»‡u
â”‚   â”œâ”€â”€ pipelines.py             # Pipeline xá»­ lÃ½ dá»¯ liá»‡u (SQL Server)
â”‚   â”œâ”€â”€ settings.py              # Cáº¥u hÃ¬nh project
â”‚   â”œâ”€â”€ selenium_middleware.py   # (TÃ¹y chá»n) Middleware Selenium - hiá»‡n Ä‘ang táº¯t
â”‚   â””â”€â”€ utils.py                 # Tiá»‡n Ã­ch há»— trá»£ (encode_input, encode_joboko_input)
â”œâ”€â”€ run_spider.py                # Script cháº¡y spider
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ scrapy.cfg                   # Cáº¥u hÃ¬nh Scrapy
â””â”€â”€ README.md                    # HÆ°á»›ng dáº«n sá»­ dá»¥ng
```

## âš™ï¸ Cáº¥u hÃ¬nh nÃ¢ng cao

### Thay Ä‘á»•i delay giá»¯a cÃ¡c request

Chá»‰nh sá»­a `DOWNLOAD_DELAY` trong `settings.py`:

```python
DOWNLOAD_DELAY = 2  # Delay 2 giÃ¢y giá»¯a cÃ¡c request
```

### Thay Ä‘á»•i sá»‘ lÆ°á»£ng request Ä‘á»“ng thá»i

```python
CONCURRENT_REQUESTS = 8  # Sá»‘ request Ä‘á»“ng thá»i (tuá»³ chá»n)
```

### ThÃªm User Agent

```python
USER_AGENT = "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Mobile Safari/537.36"
```

## ğŸ”§ Troubleshooting

### Lá»—i káº¿t ná»‘i SQL Server

1. Kiá»ƒm tra SQL Server Ä‘ang cháº¡y
2. Kiá»ƒm tra thÃ´ng tin Ä‘Äƒng nháº­p trong `settings.py`
3. Äáº£m báº£o database Ä‘Ã£ Ä‘Æ°á»£c táº¡o

### Lá»—i scraping

1. Kiá»ƒm tra internet connection
2. Thá»­ tÄƒng `DOWNLOAD_DELAY`
3. Kiá»ƒm tra website cÃ³ thay Ä‘á»•i cáº¥u trÃºc HTML khÃ´ng

### Lá»—i CSS selector

CÃ¡c spider sá»­ dá»¥ng CSS selector vÃ  XPath linh hoáº¡t Ä‘á»ƒ tÃ¬m dá»¯ liá»‡u:
- **JobsGO**: Sá»­ dá»¥ng XPath vá»›i label-based extraction cho cÃ¡c trÆ°á»ng nhÆ° Má»©c lÆ°Æ¡ng, Háº¡n ná»™p, Äá»‹a Ä‘iá»ƒm
- **JobOKO**: Sá»­ dá»¥ng CSS selector/XPath theo cáº¥u trÃºc HTML hiá»‡n táº¡i
- **123job**: Sá»­ dá»¥ng URL slug tÃ¬m kiáº¿m vÃ  label-based extraction trÃªn trang chi tiáº¿t
- **CareerViet**: Sá»­ dá»¥ng query `tim-viec-lam?keyword=...` vÃ  label-based extraction

Náº¿u website thay Ä‘á»•i cáº¥u trÃºc, cáº§n cáº­p nháº­t selector trong spider.

## ğŸ“ Ghi chÃº

- Spider cÃ³ delay giá»¯a cÃ¡c request Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i server
- Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u vÃ o SQL Server vá»›i encoding UTF-8
- CÃ³ thá»ƒ má»Ÿ rá»™ng thÃªm cÃ¡c trang tuyá»ƒn dá»¥ng khÃ¡c
- Spider tá»± Ä‘á»™ng táº¡o báº£ng vÃ  cá»™t náº¿u chÆ°a tá»“n táº¡i
- Pipeline tá»± Ä‘á»™ng thÃªm cá»™t `job_deadline` (vÃ  `job_position`) náº¿u cáº§n thiáº¿t

## ğŸ¤ ÄÃ³ng gÃ³p

Äá»ƒ thÃªm spider cho trang tuyá»ƒn dá»¥ng má»›i:

1. Táº¡o file spider má»›i trong `spiders/`
2. Káº¿ thá»«a tá»« `scrapy.Spider`
3. Implement cÃ¡c method `start_requests()`, `parse_search_results()`, `parse_job_detail()`
4. ThÃªm spider vÃ o `run_spider.py`

## ğŸ“„ License

MIT License
