**OPERATIONS (UPDATED: DuckDB postgres_scanner Implementation)**

**Pipeline Execution Order (Airflow):**
1. `run_spiders` - Execute all 10 Scrapy spiders
2. `soda_scan_check1` - Schema validation, required fields, duplicates
3. `soda_scan_check2` - Cross-spider completeness (ensures all 10 sites)
4. `soda_scan_check3` - Site-specific field completion validation
5. `duckdb_sync` - PostgreSQL â†’ DuckDB using postgres_scanner
6. `dbt_run` - Transform models
7. `dbt_test` - Business rule validations

**Local Development Commands:**

**Scrapy Operations:**
```bash
# Run single spider
scrapy crawl topcv -a keyword="python developer"

# Run all spiders (via run_spider.py)
python run_spider.py --spider all --keyword "IT"

# Run with specific settings
scrapy crawl topcv -s DOWNLOAD_DELAY=3 -s CONCURRENT_REQUESTS_PER_DOMAIN=2
```

**DuckDB Sync Operations:**
```bash
# Full refresh sync
SYNC_MODE=full python pg_to_duckdb/sync_pg_to_duckdb.py

# Incremental sync (default, cursor=scraped_at)
python pg_to_duckdb/sync_pg_to_duckdb.py

# With custom parameters
DUCKDB_PATH=/path/to/jobs.duckdb DUCKDB_SCHEMA=staging PG_TABLE=jobs PG_CURSOR_COLUMN=scraped_at python pg_to_duckdb/sync_pg_to_duckdb.py
```

**Data Quality Operations:**
```bash
# Run individual Soda checks
soda scan -d postgres_db -c soda/configuration.yml soda/checks/raw_jobs_check1.yml
soda scan -d postgres_db -c soda/configuration.yml soda/checks/raw_jobs_check2.yml
soda scan -d postgres_db -c soda/configuration.yml soda/checks/raw_jobs_check3.yml

# Run all checks sequentially (as in pipeline)
soda scan -d postgres_db -c soda/configuration.yml soda/checks/
```

**Environment Variables:**
```bash
# Database Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=job_database
POSTGRES_USER=scrapy_user
POSTGRES_PASSWORD=scrapy_password

# DuckDB Configuration
DUCKDB_PATH=/path/to/jobs.duckdb
DUCKDB_SCHEMA=staging
PG_TABLE=jobs
PG_CURSOR_COLUMN=scraped_at
SYNC_MODE=incremental  # or 'full'

# Scrapy Configuration
DOWNLOAD_DELAY=2
CONCURRENT_REQUESTS=16
USER_AGENT="Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36..."
```

**API Operations:**
```bash
# Start FastAPI server
cd api && python main.py
# Access at http://localhost:8000/jobs?keyword=python&page=1&page_size=20
```

**Frontend Operations:**
```bash
# Serve frontend (if needed)
cd web && python -m http.server 8080
# Access at http://localhost:8080
```

**Docker Operations:**
```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f [service_name]

# Stop services
docker-compose down
```

**Monitoring & Debugging:**
```bash
# View Scrapy logs
tail -f logs/crawl_*.log

# Check Soda scan results
soda scan -d postgres_db -c soda/configuration.yml --results-file results.json

# Test database connections
python -c "import psycopg2; conn = psycopg2.connect(host='localhost', dbname='job_database', user='scrapy_user', password='scrapy_password'); print('PostgreSQL connected')"

# Test DuckDB connection
python -c "import duckdb; con = duckdb.connect('jobs.duckdb'); print('DuckDB connected'); con.close()"
```

**Troubleshooting Notes:**
- **Selenium Issues:** Ensure Chrome and ChromeDriver are installed and compatible
- **Rate Limiting:** Adjust DOWNLOAD_DELAY and AUTOTHROTTLE settings per site
- **DuckDB Locks:** Ensure single-writer discipline; avoid concurrent access
- **Quality Failures:** Check individual Soda checks to identify problematic spiders
- **Memory Issues:** Monitor large crawls; consider smaller batches for testing

**Development Workflow:**
1. Test individual spiders: `scrapy crawl spider_name -a keyword="test"`
2. Validate data quality: Run Soda checks manually
3. Test sync process: Run DuckDB sync with small dataset
4. Full pipeline test: Execute complete Airflow DAG
5. API testing: Use curl or browser to test endpoints