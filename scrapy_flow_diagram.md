# Complete Scrapy Flow with Selenium Integration

## üéØ Lu·ªìng ho·∫°t ƒë·ªông ho√†n ch·ªânh khi ch·∫°y TopCV Spider

### S∆° ƒë·ªì t·ªïng quan:

```mermaid
graph TD
    subgraph "Phase 1: Startup & Initialization"
        A["`**run_spider.py**<br/>Python script kh·ªüi ƒë·ªông`"] --> B["`**get_project_settings()**<br/>Load c·∫•u h√¨nh Scrapy`"]
        B --> C["`**settings.py**<br/>ITEM_PIPELINES<br/>DOWNLOADER_MIDDLEWARES<br/>SQL_SERVER config`"]
        C --> D["`**Register Components**<br/>- SeleniumMiddleware<br/>- SQLServerPipeline<br/>- CrawljobPipeline`"]
        D --> E["`**Load Spider Modules**<br/>Import topcv_spider.py<br/>Import items.py, utils.py`"]
    end

    subgraph "Phase 2: Spider Instance Creation"
        E --> F["`**TopcvSpider.__init__()**<br/>keyword='data analyst'<br/>allowed_domains=['topcv.vn']`"]
        F --> G["`**SeleniumMiddleware.__init__()**<br/>Chrome driver kh·ªüi t·∫°o<br/>Headless mode + stealth config`"]
        G --> H["`**SQLServerPipeline.from_crawler()**<br/>Load DB connection settings`"]
    end

    subgraph "Phase 3: Crawling Process"
        H --> I["`**TopcvSpider.start_requests()**<br/>Generate search URL<br/>yield Request to search page`"]
        I --> J["`**SeleniumMiddleware.process_request()**<br/>Check: spider.name == 'topcv'?`"]
        
        J -->|Yes - TopCV Request| K["`**Selenium WebDriver**<br/>driver.get(url)<br/>Wait for JS rendering<br/>Extract page_source`"]
        J -->|No - Other Spider| L["`**HTTP Downloader**<br/>Standard Scrapy download`"]
        
        K --> M["`**HtmlResponse**<br/>Rendered HTML from Selenium<br/>Ready for parsing`"]
        L --> M
        
        M --> N["`**parse_search_results()**<br/>Extract job URLs<br/>yield Request(job_url)`"]
        N --> O["`**Selenium renders job detail page**<br/>Same process for each job`"]
        O --> P["`**parse_job_detail()**<br/>Extract: title, company, salary<br/>location, deadline, etc.`"]
        
        P --> Q["`**JobItem created**<br/>All fields populated<br/>yield item`"]
    end

    subgraph "Phase 4: Item Processing Pipeline"
        Q --> R["`**CrawljobPipeline.process_item()**<br/>Item passes through<br/>(currently just returns item)`"]
        R --> S["`**SQLServerPipeline.process_item()**<br/>Insert into SQL Server<br/>jobs table`"]
        S --> T["`**Database INSERT**<br/>16 fields including<br/>job_deadline, search_keyword`"]
    end

    subgraph "Phase 5: Shutdown & Cleanup"
        T --> U["`**Spider finished**<br/>All pages crawled<br/>All items processed`"]
        U --> V["`**SQLServerPipeline.close_spider()**<br/>Close DB connection`"]
        V --> W["`**SeleniumMiddleware.spider_closed()**<br/>driver.quit()<br/>Chrome process killed`"]
        W --> X["`**Process complete**<br/>JSON file written<br/>Terminal returns`"]
    end

    style A fill:#e1f5fe
    style K fill:#f3e5f5
    style S fill:#e8f5e8
    style X fill:#fff3e0
```

## üìã Chi ti·∫øt t·ª´ng b∆∞·ªõc:

### 1. **Kh·ªüi ƒë·ªông (run_spider.py)**
```python
# Parse arguments: --spider topcv --keyword "data analyst" --output topcv.json
# Load Scrapy settings
# Register spider instance v·ªõi CrawlerProcess
```

### 2. **Load c·∫•u h√¨nh (settings.py)**
```python
DOWNLOADER_MIDDLEWARES = {
    "CrawlJob.selenium_middleware.SeleniumMiddleware": 543,
}
ITEM_PIPELINES = {
    "CrawlJob.pipelines.CrawljobPipeline": 300,
    "CrawlJob.pipelines.SQLServerPipeline": 400,
}
```

### 3. **Selenium Middleware Setup**
```python
# Chrome driver v·ªõi stealth options:
# - headless mode
# - fake user agent
# - disable automation flags
```

### 4. **Spider Crawling**
```python
start_requests() ‚Üí Request("https://www.topcv.vn/tim-viec-lam-data-analyst")
‚Üì
SeleniumMiddleware.process_request() ‚Üí Selenium renders page
‚Üì
parse_search_results() ‚Üí Extract job URLs
‚Üì
For each job: Selenium renders ‚Üí parse_job_detail() ‚Üí yield JobItem
```

### 5. **Item Pipeline**
```python
JobItem ‚Üí CrawljobPipeline.process_item() ‚Üí SQLServerPipeline.process_item()
‚Üì
INSERT INTO jobs (job_title, company_name, salary, ..., job_deadline, ...)
```

### 6. **Cleanup**
```python
# Close database connections
# Quit Selenium driver
# Save JSON output file
```

---

## üîÑ **Selenium vs Normal HTTP Flow Comparison**

| Step | Normal HTTP | With Selenium |
|------|-------------|---------------|
| Request | `requests.get(url)` | `driver.get(url)` |
| Rendering | Static HTML only | Full JavaScript rendering |
| Wait time | Instant | 3-5 seconds for JS |
| Anti-bot | Often blocked (403) | Harder to detect |
| Resource usage | Low | High (Chrome process) |
| Success rate | Low for TopCV | Higher bypass rate |

---

## ‚ö° **Key Benefits c·ªßa Selenium Integration**

1. **Bypass JavaScript rendering**: TopCV d√πng JS ƒë·ªÉ load job listings
2. **Anti-bot evasion**: Chrome driver kh√≥ ph√°t hi·ªán h∆°n HTTP requests
3. **Dynamic content**: Wait cho elements load tr∆∞·ªõc khi parse
4. **Realistic behavior**: Gi·ªëng user th·∫≠t browse website

## üéØ **Files ƒë∆∞·ª£c s·ª≠ d·ª•ng trong lu·ªìng**

| File | Role | Khi n√†o ƒë∆∞·ª£c g·ªçi |
|------|------|------------------|
| `run_spider.py` | Entry point | ƒê·∫ßu ti√™n - kh·ªüi ƒë·ªông |
| `settings.py` | Configuration | Scrapy init - load config |
| `selenium_middleware.py` | Request processor | M·ªói request ƒë·∫øn TopCV |
| `topcv_spider.py` | Data extractor | Parse search + job pages |
| `items.py` | Data structure | Khi spider yield item |
| `pipelines.py` | Data processor | Sau khi item ƒë∆∞·ª£c yield |
| `utils.py` | Helper functions | Khi c·∫ßn encode/clean data |
