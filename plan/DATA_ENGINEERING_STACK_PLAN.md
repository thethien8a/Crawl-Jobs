# ğŸš€ **DATA ENGINEERING STACK IMPLEMENTATION PLAN**
## **CrawlJob: Professional Data Engineering Project**

---

## ğŸ“‹ **TABLE OF CONTENTS**

1. [ğŸ¯ Project Overview](#-project-overview)
2. [ğŸ—ï¸ Architecture Design](#ï¸-architecture-design)

---

## ğŸ¯ **PROJECT OVERVIEW**

### **Current Status**
- âœ… **10 Spiders** hoáº¡t Ä‘á»™ng hoÃ n háº£o
- âœ… **PostgreSQL** database vá»›i 10,000+ records
- âœ… **FastAPI** backend vá»›i REST endpoints
- âœ… **Web Dashboard** vá»›i Bootstrap 5
- âœ… **Automated daily crawling**

### **Data Engineering Goal**
Chuyá»ƒn Ä‘á»•i CrawlJob thÃ nh **Professional Data Engineering Project** vá»›i:
- **Apache Airflow**: Workflow orchestration
- **dbt**: Data transformation layer
- **Soda Core + dbt tests**: Data quality validation (Raw Gate + Business Rules)
â€“ **Apache Superset**: Data visualization vÃ  analytics

### **Benefits**
- ğŸ¢ **Professional**: Industry-standard data engineering stack
- ğŸ“Š **Advanced Analytics**: Rich dashboards vÃ  insights
- ğŸ”§ **Automation**: Fully automated pipelines
- ğŸ“ˆ **Scalability**: Easy to scale as project grows
- ğŸ’¼ **Career Growth**: Valuable skills for data engineering

---

## ğŸ—ï¸ **ARCHITECTURE DESIGN**

### **Current Architecture**
```
CrawlJob Spiders â†’ PostgreSQL â†’ FastAPI â†’ Web Dashboard
```

### **Target Data Engineering Architecture**

#### **Detailed Data Flow**

```mermaid
flowchart TD
    %% Layers
    subgraph ingestion["ğŸ”„ Data Ingestion"]
        spiders["ğŸ•·ï¸ CrawlJob Spiders<br/>10 Job Sites"]
        airflow["âš¡ Apache Airflow<br/>Orchestrator (Schedules/Triggers)"]
    end

    subgraph storage["ğŸ’¾ Data Storage"]
        postgres["ğŸ˜ PostgreSQL<br/>Raw & Serving (OLTP)"]
        duckdb["ğŸ¦† DuckDB<br/>Analytics Marts (OLAP)"]
    end

    subgraph processing["âš™ï¸ Data Processing"]
        soda["ğŸ§ª Soda Core<br/>Raw Gate (Postgres)"]
        airbyte["ğŸ§² Airbyte<br/>EL Postgres â†’ DuckDB"]
        dbt["ğŸ”¨ dbt-duckdb<br/>Transform & Tests (in DuckDB)"]
    end

    subgraph presentation["ğŸ“Š Presentation & Access"]
        superset["Apache Superset<br/>BI Dashboards"]
        fastapi["ğŸš€ FastAPI<br/>REST API"]
        webapp["ğŸŒ Job Search Website<br/>End-User Portal"]
    end

    %% Orchestration (control-plane)
    airflow -. trigger .-> spiders
    airflow -. run .-> soda
    airflow -. run .-> airbyte
    airflow -. run .-> dbt

    %% Data plane
    spiders -->|"Insert Raw Jobs"| postgres
    soda -->|"Validate Raw"| postgres
    airbyte -->|"Sync raw/staging"| duckdb
    dbt -->|"Read & Materialize"| duckdb

    %% Serving
    fastapi -->|"Query"| postgres
    webapp -->|"Use"| fastapi
    superset -->|"Connect"| duckdb

    %% Styles
    classDef ingestionStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef storageStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef processStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef presentStyle fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px

    class spiders,airflow ingestionStyle
    class postgres,duckdb storageStyle
    class dbt,airbyte,soda processStyle
    class superset,fastapi,webapp presentStyle
```

#### Data Flow chi tiáº¿t cho Apache Superset

1) Äiá»u phá»‘i theo lá»‹ch (Airflow)
- Airflow cháº¡y theo lá»‹ch (vÃ­ dá»¥ 02:00 háº±ng ngÃ y) vÃ  láº§n lÆ°á»£t trigger cÃ¡c bÆ°á»›c: cháº¡y spiders â†’ kiá»ƒm tra cháº¥t lÆ°á»£ng (Soda Core) â†’ Ä‘á»“ng bá»™ EL (Airbyte: PostgreSQL â†’ DuckDB) â†’ biáº¿n Ä‘á»•i dá»¯ liá»‡u (dbt-duckdb) â†’ cáº­p nháº­t kho OLAP (DuckDB).

2) Thu tháº­p dá»¯ liá»‡u (Spiders â†’ PostgreSQL)
- CÃ¡c spiders thu tháº­p dá»¯ liá»‡u tá»« 10 trang, chuáº©n hÃ³a tá»‘i thiá»ƒu vÃ  ghi trá»±c tiáº¿p vÃ o PostgreSQL (schema/raw), kÃ¨m timestamps/metadata phá»¥c vá»¥ kiá»ƒm soÃ¡t phiÃªn crawl.

3) Kiá»ƒm tra cháº¥t lÆ°á»£ng (Raw Gate â€“ Soda Core)
- Soda Core cháº¡y trÃªn báº£ng raw á»Ÿ PostgreSQL: kiá»ƒm tra schema, tÃ­nh há»£p lá»‡ (URL), khÃ´ng null, row_count, vÃ  freshness (scraped_at).
- Náº¿u FAIL: Airflow dá»«ng pipeline, gá»­i cáº£nh bÃ¡o; dá»¯ liá»‡u OLAP cÅ© váº«n Ä‘Æ°á»£c giá»¯ nguyÃªn Ä‘á»ƒ dashboard Superset khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng.
- Náº¿u PASS: tiáº¿p tá»¥c bÆ°á»›c biáº¿n Ä‘á»•i. (Sau-transform) Sá»­ dá»¥ng `dbt test` Ä‘á»ƒ kiá»ƒm tra cÃ¡c model.

4) Äá»“ng bá»™ dá»¯ liá»‡u (Airbyte â€“ EL)
- Airbyte sync tá»« PostgreSQL (raw/staging) â†’ DuckDB (OLAP), Æ°u tiÃªn incremental.
- Quáº£n lÃ½ lá»‹ch cháº¡y vÃ  retry/monitoring qua Airflow.

5) Biáº¿n Ä‘á»•i dá»¯ liá»‡u (dbt-duckdb â€“ ELT)
- dbt-duckdb Ä‘á»c dá»¯ liá»‡u trong DuckDB â†’ táº¡o cÃ¡c mÃ´ hÃ¬nh staging/dim/fact/agg.
- Káº¿t quáº£ Ä‘Æ°á»£c materialize trá»±c tiáº¿p trong DuckDB thÃ nh cÃ¡c báº£ng/khung nhÃ¬n analytics-ready.

5) Kho phÃ¢n tÃ­ch (DuckDB â€“ OLAP)
- DuckDB lÆ°u trá»¯ cÃ¡c mÃ´ hÃ¬nh phá»¥c vá»¥ phÃ¢n tÃ­ch (vÃ­ dá»¥: dim_companies, fct_jobs, agg_jobs_by_industryâ€¦).
- File DuckDB Ä‘Æ°á»£c Ä‘áº·t táº¡i má»™t Ä‘Æ°á»ng dáº«n á»•n Ä‘á»‹nh Ä‘á»ƒ phá»¥c vá»¥ káº¿t ná»‘i tá»« Power BI.

6) Káº¿t ná»‘i Apache Superset
- Superset káº¿t ná»‘i tá»›i DuckDB qua SQLAlchemy (duckdb-engine) Ä‘á»ƒ Ä‘á»c cÃ¡c báº£ng phÃ¢n tÃ­ch. TÃ¹y chá»n káº¿t ná»‘i:
    - SQLAlchemy URI: `duckdb:///D:/path/to/warehouse.duckdb`, hoáº·c
    - (PhÆ°Æ¡ng Ã¡n thay tháº¿) Náº¿u Ä‘á»ƒ marts trong PostgreSQL, Superset cÃ³ thá»ƒ káº¿t ná»‘i trá»±c tiáº¿p PostgreSQL.

7) LÃ m má»›i dá»¯ liá»‡u (Refresh)
- Desktop: Refresh thá»§ cÃ´ng Ä‘á»ƒ phÃ¡t triá»ƒn/kiá»ƒm thá»­.
- Service: DÃ¹ng feature Database refresh cá»§a Superset (hoáº·c cron Airflow Ä‘á»ƒ trigger materialization) sau khi pipeline hoÃ n táº¥t; dashboard dÃ¹ng nguá»“n DuckDB cáº­p nháº­t.

8) TrÃ¬nh bÃ y vÃ  tiÃªu thá»¥
- Superset sá»­ dá»¥ng cÃ¡c báº£ng trong DuckDB Ä‘á»ƒ dá»±ng dashboard (Jobs by Industry, Salary Distribution, Trendsâ€¦). NgÆ°á»i dÃ¹ng xem dashboard trÃªn giao diá»‡n Superset.

9) á»¨ng dá»¥ng web ngÆ°á»i dÃ¹ng (khÃ´ng liÃªn quan Power BI)
- Job Search Website truy cáº­p dá»¯ liá»‡u qua FastAPI â†’ PostgreSQL (OLTP) Ä‘á»ƒ phá»¥c vá»¥ tra cá»©u/tÃ¬m kiáº¿m theo thá»i gian thá»±c; khÃ´ng truy váº¥n DuckDB.

```mermaid
flowchart LR
    Airflow[Apache Airflow] -. trigger .-> Spiders[CrawlJob Spiders]
    Spiders -->|Raw jobs| Postgres[(PostgreSQL OLTP)]
    Airflow -. run .-> Soda[Soda Core]
    Soda -->|Validate raw| Postgres
    Airflow -. run .-> dbt[dbt]
    dbt -->|Read| Postgres
    dbt -->|Materialize marts| DuckDB[(DuckDB OLAP)]
    Superset[Apache Superset] -->|Connect| DuckDB

    classDef c1 fill:#e1f5fe,stroke:#01579b,stroke-width:1px
    classDef c2 fill:#f3e5f5,stroke:#4a148c,stroke-width:1px
    class Airflow,Spiders c1
    class Postgres,DuckDB c2
```

#### Data Flow chi tiáº¿t cho Job Search Website

1) NgÆ°á»i dÃ¹ng â†’ Giao diá»‡n Web (Frontend)
- NgÆ°á»i dÃ¹ng nháº­p tá»« khÃ³a/bá»™ lá»c (keyword, site, location, page, page_size, sortâ€¦). Giao diá»‡n gá»­i HTTP request tá»›i FastAPI.

2) Frontend â†’ FastAPI (API Layer)
- Endpoint chÃ­nh: `GET /jobs` vá»›i cÃ¡c query params Ä‘Ã£ há»— trá»£: `keyword`, `site`, `page`, `page_size` (cÃ³ thá»ƒ má»Ÿ rá»™ng `location`, `sort_by`).
- FastAPI validate tham sá»‘, chuáº©n hÃ³a, log truy váº¥n, Ã¡p háº¡n má»©c page_size an toÃ n (vÃ­ dá»¥ 10â€“50).

3) FastAPI â†’ PostgreSQL (Query OLTP)
- API dá»±ng cÃ¢u truy váº¥n cÃ³ paginate (LIMIT/OFFSET) vÃ  cÃ¡c Ä‘iá»u kiá»‡n lá»c; dÃ¹ng truy váº¥n tham sá»‘ (parameterized) Ä‘á»ƒ an toÃ n.
- Khuyáº¿n nghá»‹ chá»‰ má»¥c (indexes): `(job_title)`, `(company_name)`, `(location)`, `(posted_date)`, vÃ  `(source_site, posted_date)` Ä‘á»ƒ tá»‘i Æ°u lá»c/sáº¯p xáº¿p.

4) PostgreSQL â†’ FastAPI (Káº¿t quáº£)
- PostgreSQL tráº£ vá» danh sÃ¡ch job chuáº©n hÃ³a (18+ fields) cÃ¹ng tá»•ng sá»‘ báº£n ghi (total) náº¿u cÃ³ truy váº¥n Ä‘áº¿m.
- FastAPI tráº£ JSON vá» frontend theo schema: `items`, `total`, `page`, `page_size`.

5) FastAPI â†’ Frontend (Hiá»ƒn thá»‹)
- Frontend render danh sÃ¡ch viá»‡c lÃ m, phÃ¢n trang/scroll, vÃ  hiá»ƒn thá»‹ metadata (source_site, scraped_at, posted_dateâ€¦).
- Cho tráº£i nghiá»‡m tá»‘t hÆ¡n: debounce tÃ¬m kiáº¿m, hiá»ƒn thá»‹ loader, giá»¯ state bá»™ lá»c.

6) TÃ­nh tÆ°Æ¡i dá»¯ liá»‡u
- Dá»¯ liá»‡u Ä‘á»c tá»« PostgreSQL Ä‘Ã£ Ä‘Æ°á»£c Ä‘i qua pipeline Airflow vÃ  cá»•ng GE (cháº¥t lÆ°á»£ng Ä‘áº¡t chuáº©n) trÆ°á»›c Ä‘Ã³.
- Web luÃ´n Ä‘á»c nguá»“n OLTP nÃªn khÃ´ng bá»‹ phá»¥ thuá»™c vÃ o DuckDB/BI.

7) Äá»™ tin cáº­y & Hiá»‡u nÄƒng
- Timeout há»£p lÃ½ táº¡i API (vÃ­ dá»¥ 3â€“5s), retry nháº¹ phÃ­a frontend; phÃ¢n trang báº¯t buá»™c Ä‘á»ƒ báº£o vá»‡ DB.
- (TÃ¹y chá»n) Cache ngáº¯n háº¡n táº¡i API (in-memory/ETag) cho truy váº¥n láº·p láº¡i; báº­t nÃ©n (gzip) khi tráº£ JSON.

8) Nháº­t kÃ½ & GiÃ¡m sÃ¡t
- Log request/response vÃ  thá»i gian truy váº¥n (latency) Ä‘á»ƒ tá»‘i Æ°u tiáº¿p; theo dÃµi lá»—i 4xx/5xx.

```mermaid
flowchart LR
    User[End User] --> UI[Web UI]
    UI -->|HTTP GET /jobs?query...| FastAPI[FastAPI API]
    FastAPI -->|Parameterized SQL| Postgres[(PostgreSQL OLTP)]
    Postgres -->|Rows + total| FastAPI
    FastAPI -->|JSON items,total,page,page_size| UI

    classDef api fill:#e8f5e8,stroke:#1b5e20,stroke-width:1px
    classDef db fill:#f3e5f5,stroke:#4a148c,stroke-width:1px
    class FastAPI,UI api
    class Postgres db
```

#### Data Flow chi tiáº¿t cho Orchestration & Monitoring (Airflow)

1) LÃªn lá»‹ch & Ä‘iá»u phá»‘i
- Airflow DAG cháº¡y theo cron (vÃ­ dá»¥ 02:00). CÃ¡c task: `run_spiders` â†’ `soda_validate_raw` â†’ `dbt_run` â†’ `dbt_test` â†’ `publish_duckdb` â†’ `notify_success`.

2) Retry & SLA
- Má»—i task cÃ³ `retries` vÃ  `retry_delay` há»£p lÃ½; Ä‘áº·t `sla` Ä‘á»ƒ cáº£nh bÃ¡o khi quÃ¡ thá»i gian.

3) Logging & Artifacts
- Log chi tiáº¿t cá»§a tá»«ng task Ä‘Æ°á»£c lÆ°u vÃ o thÆ° má»¥c logs; artifacts gá»“m log `soda scan`, file DuckDB má»›i, vÃ  dbt target (manifest/run_results).

4) Alerting
- KÃªnh cáº£nh bÃ¡o: Email/Slack khi task fail/SLA miss. Ná»™i dung Ä‘Ã­nh kÃ¨m link log vÃ  tham chiáº¿u tá»›i log `soda scan`/`dbt test`.

5) Observability
- Theo dÃµi tráº¡ng thÃ¡i DAG trÃªn Airflow UI (Gantt/Graph). Ghi nháº­n metrics (thá»i gian cháº¡y, tá»‰ lá»‡ fail) Ä‘á»ƒ tá»‘i Æ°u.

```mermaid
flowchart TD
    start([Scheduled 02:00]) --> run_spiders[Task: run_spiders]
    run_spiders --> ge_raw[Task: ge_validate_raw]
    ge_raw -->|PASS| dbt_run[Task: dbt_run]
    ge_raw -->|FAIL| alert1([Alert + Stop])
    dbt_run --> ge_marts{Run ge_validate_marts?}
    ge_marts -->|YES| ge_marts_task[Task: ge_validate_marts] --> publish[Task: publish_duckdb]
    ge_marts -->|NO| publish
    publish --> notify[Task: notify_success]

    classDef t fill:#fff3e0,stroke:#e65100,stroke-width:1px
    class run_spiders,ge_raw,dbt_run,ge_marts_task,publish,notify t
```

#### Data Quality Implementation (Soda Core + dbt tests)

1) Soda Core (Raw Gate)
- Khai bÃ¡o data source Postgres trong `soda/configuration.yml`.
- Äá»‹nh nghÄ©a checks trong `soda/checks/raw_jobs_check1.yml`, `raw_jobs_check2.yml`, `raw_jobs_check3.yml`.
- Cháº¡y tuáº§n tá»± 3 checks trong Airflow (BashOperator). Fail dá»«ng pipeline.

2) dbt tests (Post-Transform)
- Viáº¿t tests trong `schema.yml` cá»§a cÃ¡c model (built-in + dbt-expectations náº¿u cáº§n).
- Cháº¡y `dbt test` sau `dbt run`. Fail thÃ¬ alert vÃ  dá»«ng publish.

#### Data Flow chi tiáº¿t cho dbt Docs & Lineage

1) Sinh tÃ i liá»‡u
- Cháº¡y `dbt docs generate` sau `dbt run` Ä‘á»ƒ táº¡o catalog + lineage diagrams; lÆ°u trong `target/` vÃ  (tuá»³ chá»n) publish ná»™i bá»™.

2) Exposures
- Khai bÃ¡o `exposures` trong dbt Ä‘á»ƒ mÃ´ táº£ dashboard Power BI vÃ  web app nhÆ° consumer chÃ­nh; giÃºp theo dÃµi tÃ¡c Ä‘á»™ng thay Ä‘á»•i.

3) Source Freshness
- Cháº¡y `dbt source freshness` theo lá»‹ch Ä‘á»ƒ Ä‘o Ä‘á»™ tÆ°Æ¡i cá»§a nguá»“n (PostgreSQL/raw), pháº£n há»“i vÃ o monitoring/alerting.

```mermaid
flowchart TD
    dbt_run[dbt run] --> models[Staging/Dim/Fact/Agg Models]
    dbt_run --> target_duckdb[(DuckDB marts)]
    dbt_docs[dbt docs generate] --> catalog[Catalog + Lineage]
    exposures[dbt exposures] --> consumers[Superset, Web App]
    freshness[dbt source freshness] --> status[Freshness Status]
```

#### Data Flow chi tiáº¿t cho Data Export/Sharing (Parquet/External)

1) Export tá»« DuckDB
- Sau `dbt run`, cÃ³ thá»ƒ export báº£ng phÃ¢n tÃ­ch tá»« DuckDB sang Parquet/CSV trong `data/exports/` Ä‘á»ƒ chia sáº» cho data science/Ä‘á»‘i tÃ¡c.

2) TÃ­ch há»£p cÃ´ng cá»¥ khÃ¡c
- CÃ¡c cÃ´ng cá»¥ nhÆ° Pandas, Spark, hoáº·c Power BI (qua Parquet folder) cÃ³ thá»ƒ tiÃªu thá»¥ dá»¯ liá»‡u nÃ y mÃ  khÃ´ng cáº§n truy cáº­p trá»±c tiáº¿p DB.

3) Quáº£n trá»‹ phiÃªn báº£n
- Äáº·t quy táº¯c Ä‘áº·t tÃªn (kÃ¨m timestamp) vÃ  dá»n dáº¹p phiÃªn báº£n cÅ© báº±ng job Ä‘á»‹nh ká»³ Ä‘á»ƒ tá»‘i Æ°u dung lÆ°á»£ng.

```mermaid
flowchart TD
    MARTS["DuckDB marts"] --> EXPORT["Export to Parquet or CSV"]
    EXPORT --> PANDAS["Pandas"]
    EXPORT --> SPARK["Spark"]
    EXPORT --> PBI["Superset (via Parquet folder)"]
    AF["Airflow optional"] --> EXPORT
```

### **Technology Stack**
- **Orchestration**: Apache Airflow
- **OLTP Database**: PostgreSQL
- **OLAP Database**: DuckDB
- **Transformation**: dbt-duckdb
- **Data Quality**: Soda Core (raw) + dbt tests (post-transform)
- **Visualization**: Apache Superset
- **Backend**: FastAPI
- **Frontend**: Bootstrap 5
- **Containerization**: Docker
- **Version Control**: Git & GitHub