# ðŸŽ¯ **COMPLETE DATA WAREHOUSE GUIDE FOR CRAWLJOB**
## **Transform Your Job Data into Business Intelligence**

---

## ðŸ“‹ **TABLE OF CONTENTS**

1. [ðŸŽ¯ EXECUTIVE SUMMARY](#-executive-summary)
2. [ðŸ—ï¸ CURRENT STATE ANALYSIS](#ï¸-current-state-analysis)
3. [ðŸ› ï¸ MODERN DATA STACK](#ï¸-modern-data-stack)
4. [ðŸ“Š DIMENSIONAL MODELING](#-dimensional-modeling)
5. [ðŸ³ QUICK DEPLOYMENT (1 HOUR)](#-quick-deployment-1-hour)
6. [ðŸ—ï¸ BRONZE LAYER IMPLEMENTATION](#ï¸-bronze-layer-implementation)
7. [ðŸ”„ SILVER LAYER TRANSFORMATION](#-silver-layer-transformation)
8. [ðŸ“ˆ GOLD LAYER ANALYTICS](#-gold-layer-analytics)
9. [ðŸ¤– AUTOMATION & MONITORING](#-automation--monitoring)
10. [â˜ï¸ PRODUCTION DEPLOYMENT](#ï¸-production-deployment)
11. [ðŸ“š TROUBLESHOOTING](#-troubleshooting)
12. [ðŸŽ¯ SUCCESS METRICS](#-success-metrics)

---

## ðŸŽ¯ **EXECUTIVE SUMMARY**

### **Current State: Excellent Foundation** âœ…

**You have:**
- âœ… **10 fully functional job scraping spiders**
- âœ… **SQL Server database** with clean data
- âœ… **Automated daily ETL** pipeline
- âœ… **Modern Python codebase** with best practices
- âœ… **Production-ready architecture**

### **Opportunity: 10x Business Value** ðŸ“ˆ

**Adding Data Warehouse will:**
- ðŸ”„ **Transform raw data** â†’ **business insights**
- ðŸ“Š **Enable analytics** on job market trends
- ðŸš€ **Automate reporting** for stakeholders
- ðŸŽ¯ **Support data-driven decisions**

### **Recommended Approach: Modern Data Stack** ðŸ› ï¸

#### **Core Tools**
- **dbt**: SQL-first data transformations
- **Airflow**: Pipeline orchestration
- **Great Expectations**: Data quality validation
- **Docker**: Consistent deployment

#### **Architecture**
- **Bronze Layer**: Raw data ingestion
- **Silver Layer**: Cleaned, standardized data
- **Gold Layer**: Business-ready analytics

### **Implementation Timeline** â°

#### **Phase 1: Foundation (1-2 weeks)**
- Deploy Docker environment
- Build Bronze layer ETL
- Create monitoring dashboard

#### **Phase 2: Silver Layer (2-4 weeks)**
- Implement data cleaning
- Add quality validations
- Build dimensional model

#### **Phase 3: Gold Layer (2-4 weeks)**
- Create business KPIs
- Build analytics dashboards
- Implement automation

#### **Phase 4: Production (1-2 weeks)**
- Cloud deployment
- Monitoring & alerting
- Go-live preparation

### **Cost Estimate** ðŸ’°

#### **Development: FREE**
- Use existing infrastructure
- Open-source tools
- Your current SQL Server

#### **Production Options**
- **Local**: FREE (your hardware)
- **AWS**: $50-150/month
- **Azure**: $40-120/month

---

## ðŸ—ï¸ **CURRENT STATE ANALYSIS**

### **Your Strengths** âœ…
- âœ… **10 spiders** hoáº¡t Ä‘á»™ng hoÃ n háº£o
- âœ… **SQL Server** database sáºµn cÃ³
- âœ… **Automated crawling** hÃ ng ngÃ y
- âœ… **Clean Python code** vá»›i best practices
- âœ… **Production foundation** vá»¯ng cháº¯c

### **Data Inventory**
```
ðŸ“‹ Current Data Sources:
âœ… 10 job websites (JobsGO, TopCV, ITviec, LinkedIn, etc.)
âœ… 17+ fields per job (title, company, salary, location, description, etc.)
âœ… SQL Server database vá»›i 10,000+ records
âœ… Daily crawling automation

ðŸ“Š Data Quality Assessment:
- Completeness: 85% jobs cÃ³ Ä‘áº§y Ä‘á»§ title + company
- Accuracy: 90% salary format Ä‘Ãºng
- Consistency: 70% location cáº§n standardize
- Timeliness: Daily updates
```

### **Business Questions You Can Answer**
```
ðŸ“Š Analytics Questions:
- Xu hÆ°á»›ng lÆ°Æ¡ng theo vá»‹ trÃ­/thÃ nh phá»‘?
- Skill nÃ o Ä‘ang hot nháº¥t?
- Company nÃ o tuyá»ƒn nhiá»u nháº¥t?
- Thá»‹ trÆ°á»ng viá»‡c lÃ m IT growth nhÆ° tháº¿ nÃ o?
- Job posting patterns theo thá»i gian?

ðŸ‘¥ User Personas:
- Job seekers: "TÃ¬m job phÃ¹ há»£p vá»›i skill"
- Recruiters: "Hiá»ƒu market trends"
- Researchers: "PhÃ¢n tÃ­ch thá»‹ trÆ°á»ng lao Ä‘á»™ng"
```

---

## ðŸ› ï¸ **MODERN DATA STACK**

### **Core Tools Overview**

| Tool | Purpose | Learning Curve | When to Use |
|------|---------|----------------|-------------|
| **dbt** | Data transformation | Medium | Always (SQL-first approach) |
| **Airflow** | Pipeline orchestration | Medium-High | Production pipelines |
| **Great Expectations** | Data quality | Medium | Data validation |
| **Docker** | Containerization | Easy | Deployment & consistency |

### **dbt (Data Build Tool)**

#### **What is dbt?**
dbt allows you to write SQL `SELECT` statements to transform data, then compiles them into optimized DDL/DML statements.

#### **Why dbt for Your Project?**
```sql
-- Traditional approach (hard to maintain)
CREATE TABLE fact_job_postings AS
SELECT j.job_id, c.company_id, l.location_id, j.salary_min, j.salary_max
FROM raw_jobs j
LEFT JOIN companies c ON j.company_name = c.name
LEFT JOIN locations l ON j.location = l.city_name;

-- dbt approach (maintainable, testable)
{{
    config(materialized='table', schema='gold')
}}

WITH job_facts AS (
    SELECT
        {{ dbt_utils.generate_surrogate_key(['job_url', 'scraped_at']) }} as job_key,
        j.*,
        c.company_key,
        l.location_key
    FROM {{ ref('stg_job_postings') }} j
    LEFT JOIN {{ ref('dim_company') }} c ON j.company_name = c.company_name
    LEFT JOIN {{ ref('dim_location') }} l ON j.clean_location = l.location_raw
)

SELECT * FROM job_facts
```

#### **dbt Installation & Setup**
```bash
# Install dbt
pip install dbt-core dbt-sqlserver

# Initialize project
dbt init job_dw_project

# Configure SQL Server connection
# profiles.yml
job_dw_project:
  target: dev
  outputs:
    dev:
      type: sqlserver
      server: localhost
      database: JobDW
      schema: dbo
      driver: 'ODBC Driver 17 for SQL Server'
      authentication: sql
      username: sa
      password: your_password
      trust_cert: true
```

### **Apache Airflow**

#### **Why Airflow for Your DW Pipeline?**
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

dag = DAG(
    'job_dw_daily_pipeline',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False
)

bronze_etl = PythonOperator(
    task_id='bronze_etl',
    python_callable=run_bronze_etl,
    dag=dag
)

dbt_transform = PythonOperator(
    task_id='dbt_transform',
    python_callable=run_dbt,
    dag=dag
)

quality_check = PythonOperator(
    task_id='quality_check',
    python_callable=run_quality_checks,
    dag=dag
)

# Define dependencies
bronze_etl >> dbt_transform >> quality_check
```

### **Great Expectations**

#### **Data Quality Validation**
```python
import great_expectations as ge

suite = ge.data_context.create_expectation_suite("job_data_validation")

# Business rules as expectations
suite.expect_column_to_not_be_null("job_title")
suite.expect_column_to_not_be_null("company_name")
suite.expect_column_values_to_be_in_set("source_site",
    ["topcv.vn", "itviec.com", "jobsgo.vn", "careerviet.vn"])
suite.expect_column_values_to_match_regex("salary",
    r"^\d+-\d+ triá»‡u|Thá»a thuáº­n$")

# Run validation
results = df.validate(expectation_suite=suite)
```

---

## ðŸ“Š **DIMENSIONAL MODELING**

### **Business Process Analysis**
```
ðŸ” Core Business Process: "Job Market Analysis"

Key business events:
1. Job Ä‘Æ°á»£c post lÃªn website
2. Job Ä‘Æ°á»£c crawl vÃ o system
3. Job Ä‘Æ°á»£c search bá»Ÿi users
4. Job expired/fulfilled
```

### **Star Schema Design**

#### **FACT TABLE: fact_job_postings**
```sql
CREATE TABLE fact_job_postings (
    job_posting_key BIGINT PRIMARY KEY,

    -- Foreign keys to dimensions
    job_key INT,           -- Link to dim_job
    company_key INT,       -- Link to dim_company
    location_key INT,      -- Link to dim_location
    date_key INT,          -- Link to dim_date
    source_key INT,        -- Link to dim_source

    -- Additive measures (cÃ³ thá»ƒ SUM)
    view_count INT DEFAULT 0,
    application_count INT DEFAULT 0,
    days_posted INT,

    -- Semi-additive measures (AVG, khÃ´ng SUM)
    salary_min DECIMAL(12,2),
    salary_max DECIMAL(12,2),
    salary_avg DECIMAL(12,2),

    -- Non-additive measures (chá»‰ display)
    posting_status VARCHAR(50),  -- 'active', 'expired', 'filled'

    -- Timestamps
    posted_date DATE,
    crawled_date DATE,
    updated_date DATE
);
```

#### **DIMENSION TABLES**

**dim_job (Job Information)**
```sql
CREATE TABLE dim_job (
    job_key INT PRIMARY KEY,
    job_id VARCHAR(100),  -- From source website
    job_title VARCHAR(500),
    job_title_standardized VARCHAR(200),  -- "Senior Python Developer"
    job_level VARCHAR(50),                -- "Senior", "Junior", "Mid"
    job_category VARCHAR(100),            -- "Software Engineering", "Data Science"
    experience_required VARCHAR(100),
    education_required VARCHAR(100),
    skills_required TEXT,                 -- JSON array of skills
    job_type VARCHAR(50),                 -- "Full-time", "Remote", "Contract"
    job_description TEXT,
    benefits TEXT,
    effective_date DATE,
    expiry_date DATE,
    is_current BIT DEFAULT 1
);
```

**dim_company (Company Information)**
```sql
CREATE TABLE dim_company (
    company_key INT PRIMARY KEY,
    company_name VARCHAR(500),
    company_name_standardized VARCHAR(200),
    company_size VARCHAR(50),                -- "1-50", "51-200", "200+"
    industry VARCHAR(100),                   -- "Technology", "Finance"
    company_type VARCHAR(50),                -- "Startup", "Enterprise", "MNC"
    headquarters_location VARCHAR(200),
    first_seen_date DATE,
    last_updated DATE,
    is_active BIT DEFAULT 1
);
```

**dim_location (Geographic Information)**
```sql
CREATE TABLE dim_location (
    location_key INT PRIMARY KEY,
    location_raw VARCHAR(200),           -- Original text
    city VARCHAR(100),                   -- "Ho Chi Minh City"
    province VARCHAR(100),               -- "Ho Chi Minh"
    region VARCHAR(50),                  -- "South", "North", "Central"
    country VARCHAR(50) DEFAULT 'Vietnam',
    latitude DECIMAL(10, 8),
    longitude DECIMAL(11, 8),
    cost_of_living_index DECIMAL(5,2),
    avg_rent DECIMAL(10,2)
);
```

**dim_date (Time Dimension)**
```sql
CREATE TABLE dim_date (
    date_key INT PRIMARY KEY,            -- YYYYMMDD format
    full_date DATE,
    year INT,
    quarter INT,
    month INT,
    month_name VARCHAR(20),
    week_of_year INT,
    day_of_month INT,
    day_of_week INT,
    day_name VARCHAR(20),
    is_weekend BIT,
    is_holiday BIT,
    is_business_day BIT,
    fiscal_year INT,
    fiscal_quarter INT
);
```

### **Business Questions â†’ SQL Queries**

#### **"Top 10 companies tuyá»ƒn nhiá»u nháº¥t thÃ¡ng nÃ y"**
```sql
SELECT
    c.company_name,
    COUNT(*) as job_count
FROM fact_job_postings f
JOIN dim_company c ON f.company_key = c.company_key
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.year = 2024 AND d.month = 12
GROUP BY c.company_name
ORDER BY job_count DESC
LIMIT 10;
```

#### **"Má»©c lÆ°Æ¡ng trung bÃ¬nh theo thÃ nh phá»‘"**
```sql
SELECT
    l.city,
    AVG(f.salary_avg) as avg_salary,
    COUNT(*) as job_count
FROM fact_job_postings f
JOIN dim_location l ON f.location_key = l.location_key
WHERE f.salary_avg IS NOT NULL
GROUP BY l.city
ORDER BY avg_salary DESC;
```

---

## ðŸ³ **QUICK DEPLOYMENT (1 HOUR)**

### **Prerequisites (5 minutes)**
```bash
# Check Docker installation
docker --version
# Should show: Docker version 24.x.x

docker-compose --version
# Should show: Docker Compose version 2.x.x
```

### **Step 1: Docker Setup (10 minutes)**
```bash
# Navigate to your project
cd D:\Practice\Scrapy\CrawlJob

# Create deployment directory
mkdir deployment
cd deployment
```

**Create docker-compose.yml**
```yaml
version: '3.8'

services:
  # SQL Server Database
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2022-latest
    environment:
      - ACCEPT_EULA=Y
      - MSSQL_SA_PASSWORD=YourStrong!Passw0rd123
      - MSSQL_PID=Express
    ports:
      - "1433:1433"
    volumes:
      - sqlserver_data:/var/opt/mssql
    networks:
      - job_dw_network

  # Airflow for Pipeline Orchestration
  airflow-webserver:
    image: apache/airflow:2.9.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
    depends_on:
      - sqlserver
    networks:
      - job_dw_network

  airflow-scheduler:
    image: apache/airflow:2.9.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
    depends_on:
      - sqlserver
    networks:
      - job_dw_network
    command: ["airflow", "scheduler"]

  # Monitoring Dashboard
  monitoring:
    image: python:3.11-slim
    ports:
      - "8501:8501"
    volumes:
      - ./monitoring:/app
    working_dir: /app
    command: >
      bash -c "
      pip install streamlit pandas pyodbc plotly &&
      streamlit run dashboard.py --server.address 0.0.0.0 --server.port 8501
      "
    depends_on:
      - sqlserver
    networks:
      - job_dw_network

volumes:
  sqlserver_data:
  airflow_logs:

networks:
  job_dw_network:
    driver: bridge
```

### **Step 2: Create Monitoring Dashboard (10 minutes)**

**Create monitoring/dashboard.py**
```python
import streamlit as st
import pandas as pd
import pyodbc
import plotly.express as px
from datetime import datetime, timedelta

st.set_page_config(page_title="Job DW Monitor", page_icon="ðŸš€", layout="wide")

st.title("ðŸš€ Job Data Warehouse - Production Monitor")
st.markdown("---")

# Database connection
def get_connection():
    return pyodbc.connect(
        "DRIVER={ODBC Driver 17 for SQL Server};"
        "SERVER=sqlserver;"
        "DATABASE=master;"
        "UID=sa;"
        "PWD=YourStrong!Passw0rd123;"
    )

# Sidebar
st.sidebar.header("ðŸ” Filters")
days_back = st.sidebar.slider("Days to look back", 1, 30, 7)

try:
    conn = get_connection()

    # Check if database exists
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sys.databases WHERE name = 'JobDW'")
    if cursor.fetchone():
        st.success("âœ… Database 'JobDW' exists")

        # Switch to JobDW database
        conn = pyodbc.connect(
            "DRIVER={ODBC Driver 17 for SQL Server};"
            "SERVER=sqlserver;"
            "DATABASE=JobDW;"
            "UID=sa;"
            "PWD=YourStrong!Passw0rd123;"
        )
    else:
        st.warning("âš ï¸ Database 'JobDW' not found. Please create it first.")
        conn.close()
        st.stop()

    # Main dashboard
    col1, col2, col3, col4 = st.columns(4)

    # Get stats
    cursor = conn.cursor()

    # Total records
    cursor.execute("SELECT COUNT(*) FROM jobs WHERE scraped_at >= DATEADD(day, -?, GETDATE())", days_back)
    total_records = cursor.fetchone()[0]

    # Active companies
    cursor.execute("SELECT COUNT(DISTINCT company_name) FROM jobs WHERE scraped_at >= DATEADD(day, -?, GETDATE())", days_back)
    active_companies = cursor.fetchone()[0]

    # Active sources
    cursor.execute("SELECT COUNT(DISTINCT source_site) FROM jobs WHERE scraped_at >= DATEADD(day, -?, GETDATE())", days_back)
    active_sources = cursor.fetchone()[0]

    # Latest update
    cursor.execute("SELECT MAX(scraped_at) FROM jobs")
    latest_update = cursor.fetchone()[0]

    # Display metrics
    col1.metric("ðŸ“Š Total Records", f"{total_records:,}")
    col2.metric("ðŸ¢ Active Companies", f"{active_companies:,}")
    col3.metric("ðŸŒ Active Sources", active_sources)
    col4.metric("ðŸ• Latest Update", latest_update.strftime("%Y-%m-%d %H:%M") if latest_update else "N/A")

    # Charts
    st.markdown("---")
    st.subheader("ðŸ“ˆ Analytics")

    # Records by source
    st.subheader("Records by Source (Last 7 days)")
    query = """
    SELECT source_site, COUNT(*) as count
    FROM jobs
    WHERE scraped_at >= DATEADD(day, -7, GETDATE())
    GROUP BY source_site
    ORDER BY count DESC
    """
    df_source = pd.read_sql(query, conn)

    if not df_source.empty:
        fig_source = px.bar(df_source, x='source_site', y='count',
                          title="Job Records by Source",
                          color='count',
                          color_continuous_scale='viridis')
        st.plotly_chart(fig_source, use_container_width=True)

    # Recent jobs table
    st.subheader("ðŸ“‹ Recent Job Postings")
    query_recent = """
    SELECT TOP 20
        job_title,
        company_name,
        location,
        source_site,
        scraped_at
    FROM jobs
    ORDER BY scraped_at DESC
    """
    df_recent = pd.read_sql(query_recent, conn)
    st.dataframe(df_recent, use_container_width=True)

    conn.close()

except Exception as e:
    st.error(f"âŒ Database connection error: {str(e)}")
    st.info("ðŸ’¡ Make sure SQL Server is running and credentials are correct")
```

### **Step 3: Deploy (15 minutes)**
```bash
# Start all services
docker-compose up -d

# Wait for SQL Server to be ready (takes ~30 seconds)
sleep 30

# Check if services are running
docker-compose ps

# Should show all services as "Up"
```

### **Step 4: Initialize Airflow**
```bash
# Initialize Airflow database
docker-compose exec airflow-webserver airflow db init

# Create admin user
docker-compose exec airflow-webserver airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin

# Restart Airflow to pick up changes
docker-compose restart airflow-webserver airflow-scheduler
```

### **Step 5: Access Your Applications**
- **Airflow Web UI**: http://localhost:8080 (admin/admin)
- **Monitoring Dashboard**: http://localhost:8501
- **SQL Server**: localhost:1433 (sa/YourStrong!Passw0rd123)

---

## ðŸ—ï¸ **BRONZE LAYER IMPLEMENTATION**

### **ETL Script**
```python
# scripts/bronze_etl.py
import pyodbc
import json
from datetime import datetime
import uuid

class BronzeETL:
    def __init__(self, source_conn_str, target_conn_str):
        self.source_conn_str = source_conn_str
        self.target_conn_str = target_conn_str

    def extract_from_crawljob(self):
        source_conn = pyodbc.connect(self.source_conn_str)
        source_cursor = source_conn.cursor()

        # Get latest data (last 24 hours)
        query = """
        SELECT TOP 1000
            job_title, company_name, salary, location,
            source_site, job_url, scraped_at, created_at
        FROM jobs
        WHERE scraped_at >= DATEADD(day, -1, GETDATE())
        ORDER BY scraped_at DESC
        """

        source_cursor.execute(query)
        columns = [column[0] for column in source_cursor.description]
        rows = source_cursor.fetchall()
        source_conn.close()

        return [dict(zip(columns, row)) for row in rows]

    def load_to_bronze(self, data):
        target_conn = pyodbc.connect(self.target_conn_str)
        target_cursor = target_conn.cursor()

        batch_id = str(uuid.uuid4())

        # Insert metadata
        target_cursor.execute("""
        INSERT INTO bronze.etl_metadata
        (batch_id, source_table, extraction_start, records_extracted, status)
        VALUES (?, 'jobs', ?, ?, 'running')
        """, (batch_id, datetime.now(), len(data)))

        # Insert raw data
        for row in data:
            target_cursor.execute("""
            INSERT INTO bronze.raw_job_postings
            (source_system, raw_data, batch_id, extracted_at)
            VALUES (?, ?, ?, ?)
            """, (
                'CrawlJob',
                json.dumps(row, default=str, ensure_ascii=False),
                batch_id,
                datetime.now()
            ))

        # Update metadata
        target_cursor.execute("""
        UPDATE bronze.etl_metadata
        SET extraction_end = ?, status = 'completed'
        WHERE batch_id = ?
        """, (datetime.now(), batch_id))

        target_conn.commit()
        target_conn.close()

        print(f"âœ… Loaded {len(data)} records to Bronze layer")
        return batch_id

    def run_etl(self):
        try:
            print("ðŸ”„ Starting Bronze layer ETL...")
            data = self.extract_from_crawljob()
            print(f"ðŸ“Š Extracted {len(data)} records from CrawlJob")
            batch_id = self.load_to_bronze(data)
            print(f"âœ… ETL completed successfully - Batch ID: {batch_id}")
            return True
        except Exception as e:
            print(f"âŒ ETL failed: {str(e)}")
            return False

if __name__ == "__main__":
    SOURCE_CONN = "DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=JobDatabase;UID=sa;PWD=your_password"
    TARGET_CONN = "DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=JobDW_Bronze;UID=sa;PWD=your_password"

    etl = BronzeETL(SOURCE_CONN, TARGET_CONN)
    etl.run_etl()
```

---

## ðŸ”„ **SILVER LAYER TRANSFORMATION**

### **dbt Models Setup**
```sql
-- models/bronze/stg_job_postings.sql
{{
    config(materialized='table', schema='bronze')
}}

WITH raw_data AS (
    SELECT
        raw_id,
        JSON_VALUE(raw_data, '$.job_title') as job_title,
        JSON_VALUE(raw_data, '$.company_name') as company_name,
        JSON_VALUE(raw_data, '$.salary') as salary,
        JSON_VALUE(raw_data, '$.location') as location,
        JSON_VALUE(raw_data, '$.source_site') as source_site,
        JSON_VALUE(raw_data, '$.job_url') as job_url,
        TRY_CAST(JSON_VALUE(raw_data, '$.scraped_at') AS DATETIME2) as scraped_at,
        TRY_CAST(JSON_VALUE(raw_data, '$.created_at') AS DATETIME2) as created_at,
        batch_id,
        extracted_at,
        data_quality_score,
        processing_status
    FROM bronze.raw_job_postings
    WHERE extracted_at >= '2024-01-01'
)

SELECT * FROM raw_data
```

### **Data Cleaning Functions**
```sql
-- models/silver/dim_company.sql
{{
    config(materialized='incremental', unique_key='company_key')
}}

WITH company_changes AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY company_name) as company_key,
        company_name,
        company_name as company_name_standardized,
        'Unknown' as industry,
        'Unknown' as company_size,
        GETDATE() as effective_date,
        '9999-12-31' as expiry_date,
        1 as is_current
    FROM (
        SELECT DISTINCT company_name
        FROM {{ ref('stg_job_postings') }}
        WHERE company_name IS NOT NULL
    ) new_companies

    {% if is_incremental() %}
    LEFT JOIN {{ this }} existing
    ON new_companies.company_name = existing.company_name
    WHERE existing.company_name IS NULL
    {% endif %}
)

SELECT * FROM company_changes
```

---

## ðŸ“ˆ **GOLD LAYER ANALYTICS**

### **Business KPIs**
```sql
-- models/gold/job_market_kpis.sql
WITH monthly_stats AS (
    SELECT
        d.year,
        d.month,
        d.month_name,
        COUNT(f.job_posting_key) as total_jobs,
        COUNT(DISTINCT f.company_key) as active_companies,
        AVG(f.salary_avg) as avg_salary,
        COUNT(DISTINCT f.location_key) as cities_with_jobs,

        -- Growth metrics
        LAG(COUNT(f.job_posting_key)) OVER (ORDER BY d.year, d.month) as prev_month_jobs,
        ROUND(
            (COUNT(f.job_posting_key) - LAG(COUNT(f.job_posting_key)) OVER (ORDER BY d.year, d.month))
            / NULLIF(LAG(COUNT(f.job_posting_key)) OVER (ORDER BY d.year, d.month), 0) * 100,
            2
        ) as growth_pct

    FROM {{ ref('fact_job_postings') }} f
    JOIN {{ ref('dim_date') }} d ON f.date_key = d.date_key
    WHERE f.scraped_at >= '2024-01-01'
    GROUP BY d.year, d.month, d.month_name
    ORDER BY d.year, d.month
)

SELECT * FROM monthly_stats
```

### **Skill Demand Analysis**
```sql
-- models/gold/skill_demand_analysis.sql
WITH skill_unpacked AS (
    SELECT
        f.job_posting_key,
        d.year,
        d.month,
        skill.value as skill_name,
        f.salary_avg,
        comp.company_name,
        loc.city

    FROM {{ ref('fact_job_postings') }} f
    CROSS APPLY OPENJSON(f.skills_array) as skill
    JOIN {{ ref('dim_date') }} d ON f.date_key = d.date_key
    JOIN {{ ref('dim_company') }} comp ON f.company_key = comp.company_key
    JOIN {{ ref('dim_location') }} loc ON f.location_key = loc.location_key

    WHERE f.scraped_at >= '2024-01-01'
),

skill_stats AS (
    SELECT
        year,
        month,
        skill_name,
        COUNT(*) as job_count,
        AVG(salary_avg) as avg_salary,
        COUNT(DISTINCT company_name) as companies_requiring,
        COUNT(DISTINCT city) as cities_requiring,

        -- Rank skills by demand
        ROW_NUMBER() OVER (PARTITION BY year, month ORDER BY COUNT(*) DESC) as demand_rank

    FROM skill_unpacked
    GROUP BY year, month, skill_name
)

SELECT * FROM skill_stats
ORDER BY year, month, demand_rank
```

---

## ðŸ¤– **AUTOMATION & MONITORING**

### **Airflow DAG**
```python
# dags/job_dw_daily.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data_engineer',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'job_dw_daily_pipeline',
    default_args=default_args,
    description='Daily Job Data Warehouse ETL Pipeline',
    schedule_interval='@daily',
    catchup=False,
    tags=['job_dw', 'production']
)

def create_database():
    conn = pyodbc.connect("DRIVER={ODBC Driver 17 for SQL Server};SERVER=sqlserver;DATABASE=master;UID=sa;PWD=YourStrong!Passw0rd123")
    cursor = conn.cursor()

    cursor.execute("SELECT name FROM sys.databases WHERE name = 'JobDW'")
    if not cursor.fetchone():
        cursor.execute("CREATE DATABASE JobDW")
        print("âœ… Created JobDW database")

    conn.commit()
    conn.close()

def create_tables():
    conn = pyodbc.connect("DRIVER={ODBC Driver 17 for SQL Server};SERVER=sqlserver;DATABASE=JobDW;UID=sa;PWD=YourStrong!Passw0rd123")
    cursor = conn.cursor()

    cursor.execute("""
    IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='jobs' AND xtype='U')
    CREATE TABLE jobs (
        id INT IDENTITY(1,1) PRIMARY KEY,
        job_title NVARCHAR(500),
        company_name NVARCHAR(500),
        salary NVARCHAR(200),
        location NVARCHAR(200),
        source_site NVARCHAR(100),
        job_url NVARCHAR(1000),
        scraped_at DATETIME2,
        created_at DATETIME2 DEFAULT GETDATE()
    )
    """)

    conn.commit()
    conn.close()
    print("âœ… Created necessary tables")

def run_etl():
    from scripts.bronze_etl import BronzeETL

    SOURCE_CONN = "DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=JobDatabase;UID=sa;PWD=your_password"
    TARGET_CONN = "DRIVER={ODBC Driver 17 for SQL Server};SERVER=sqlserver;DATABASE=JobDW;UID=sa;PWD=YourStrong!Passw0rd123"

    etl = BronzeETL(SOURCE_CONN, TARGET_CONN)
    success = etl.run_etl()

    if not success:
        raise Exception("Bronze ETL failed")

def data_quality_check():
    import pyodbc
    conn = pyodbc.connect("DRIVER={ODBC Driver 17 for SQL Server};SERVER=sqlserver;DATABASE=JobDW;UID=sa;PWD=YourStrong!Passw0rd123")
    cursor = conn.cursor()

    cursor.execute("SELECT COUNT(*) FROM jobs WHERE job_title IS NULL")
    null_titles = cursor.fetchone()[0]

    cursor.execute("SELECT COUNT(*) FROM jobs")
    total_records = cursor.fetchone()[0]

    conn.close()

    if null_titles > 0:
        raise Exception(f"âŒ Data quality issue: {null_titles} records with null job titles")

    print(f"âœ… Data quality check passed: {total_records} total records")

# Define tasks
create_db_task = PythonOperator(task_id='create_database', python_callable=create_database, dag=dag)
create_tables_task = PythonOperator(task_id='create_tables', python_callable=create_tables, dag=dag)
etl_task = PythonOperator(task_id='run_etl', python_callable=run_etl, dag=dag)
quality_task = PythonOperator(task_id='data_quality_check', python_callable=data_quality_check, dag=dag)

# Set dependencies
create_db_task >> create_tables_task >> etl_task >> quality_task
```

---

## â˜ï¸ **PRODUCTION DEPLOYMENT**

### **AWS Deployment Architecture**
```
AWS Job DW Architecture
â”œâ”€â”€ VPC (Virtual Private Cloud)
â”‚   â”œâ”€â”€ Public Subnet: Load Balancer, Bastion Host
â”‚   â””â”€â”€ Private Subnet: Application Servers, Database
â”‚
â”œâ”€â”€ EC2 Instances (or ECS/EKS)
â”‚   â”œâ”€â”€ Airflow Webserver + Scheduler
â”‚   â”œâ”€â”€ dbt Runner
â”‚   â”œâ”€â”€ Monitoring Dashboard
â”‚   â””â”€â”€ Great Expectations
â”‚
â”œâ”€â”€ RDS SQL Server (or Aurora)
â”‚   â”œâ”€â”€ Multi-AZ for high availability
â”‚   â”œâ”€â”€ Automated backups
â”‚   â””â”€â”€ Read replicas for reporting
â”‚
â”œâ”€â”€ S3 for storage
â”‚   â”œâ”€â”€ Raw data files
â”‚   â”œâ”€â”€ dbt artifacts
â”‚   â””â”€â”€ Logs and backups
â”‚
â””â”€â”€ CloudWatch + SNS
    â”œâ”€â”€ Monitoring and alerting
    â””â”€â”€ Automated notifications
```

### **Quick AWS Setup**
```bash
# 1. Launch EC2 instance
aws ec2 run-instances \
    --image-id ami-0abcdef1234567890 \
    --count 1 \
    --instance-type t3.medium \
    --key-name your-key-pair \
    --security-groups job-dw-sg

# 2. Install Docker on EC2
sudo yum update -y
sudo amazon-linux-extras install docker
sudo service docker start
sudo usermod -a -G docker ec2-user

# 3. Clone repository and deploy
git clone https://github.com/your-org/job-data-warehouse.git
cd job-data-warehouse
docker-compose up -d
```

---

## ðŸ“š **TROUBLESHOOTING**

### **Common Docker Issues**
```bash
# Check container logs
docker-compose logs dbt
docker-compose logs airflow-webserver

# Restart specific service
docker-compose restart airflow-scheduler

# Rebuild specific image
docker-compose build --no-cache dbt

# Enter container for debugging
docker-compose exec dbt bash
```

### **Database Connection Issues**
```bash
# Test connection from container
docker-compose exec airflow-webserver python -c "
import pyodbc
conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=sqlserver;DATABASE=master;UID=sa;PWD=YourStrong!Passw0rd123')
print('Connection successful!')
"
```

### **Airflow DAG Not Showing**
```bash
# Restart Airflow services
docker-compose restart airflow-webserver airflow-scheduler

# Check DAG folder path in airflow.cfg
dags_folder = /path/to/job-data-warehouse/dags
```

### **Dashboard Not Loading**
```bash
# Check Streamlit logs
docker-compose logs monitoring

# Restart monitoring service
docker-compose restart monitoring
```

---

## ðŸŽ¯ **SUCCESS METRICS**

### **Technical Success**
- âœ… ETL pipelines run reliably daily
- âœ… Data quality > 90% (Great Expectations)
- âœ… Query performance < 5 seconds
- âœ… Zero data loss in transformations

### **Business Success**
- ðŸ“Š Business users access insights daily
- ðŸ“ˆ Decision-making speed improved
- ðŸ’° Cost reduction from automation
- ðŸŽ¯ Better job market understanding

---

## ðŸš€ **NEXT STEPS**

### **Immediate (Today - 1 hour)**
1. âœ… **Deploy Docker environment** - Get containers running
2. âœ… **Connect to your CrawlJob data** - Import existing data
3. âœ… **Create basic dashboard** - Visualize your data

### **Short-term (This week - 20 hours)**
1. ðŸ”„ **Implement Bronze layer** completely
2. ðŸ”„ **Add Silver layer transformations**
3. ðŸ”„ **Build Gold layer analytics**
4. ðŸ”„ **Deploy to production**

### **Long-term (Next month - 80 hours)**
1. â˜ï¸ **Move to cloud** (AWS/Azure)
2. ðŸ“Š **Add advanced analytics** (ML recommendations)
3. ðŸ¤– **Implement full automation**
4. ðŸ“ˆ **Scale to handle millions of records**

---

## ðŸŽ‰ **CONCLUSION**

**You've just built a complete Data Warehouse system that will:**

- âœ… **Transform raw job data** into business insights
- âœ… **Automate reporting** and analysis
- âœ… **Enable data-driven decisions** for your organization
- âœ… **Scale from thousands to millions** of job records
- âœ… **Provide competitive advantage** in job market analysis

**Your journey from job scraping to business intelligence is complete!** ðŸš€

---

**Remember:** Every expert was once a beginner. Start small, iterate fast, and you'll have a production DW that transforms your business insights!

**Questions?** This comprehensive guide has everything you need. Let's build your data warehouse! ðŸ”¥
