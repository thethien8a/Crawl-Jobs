# C√°ch X√¢y D·ª±ng Data Warehouse D·ª±a V√†o D·ªØ Li·ªáu Job Hi·ªán C√≥

## üéØ **T·ªîNG QUAN**

**D·ªØ li·ªáu hi·ªán t·∫°i c·ªßa b·∫°n**: Raw job data t·ª´ 10 websites (JobsGO, TopCV, ITviec, v.v.)  
**M·ª•c ti√™u**: X√¢y d·ª±ng Data Warehouse ƒë·ªÉ ph√¢n t√≠ch th·ªã tr∆∞·ªùng vi·ªác l√†m  
**Approach**: Kimball Dimensional Modeling (ph√π h·ª£p cho project nh·ªè-v·ª´a)

---

## üèóÔ∏è **QUY TR√åNH X√ÇY D·ª∞NG DATA WAREHOUSE (7 B∆Ø·ªöC)**

### **B∆Ø·ªöC 1: PH√ÇN T√çCH BUSINESS REQUIREMENTS (1-2 tu·∫ßn)**

#### **C√¢u h·ªèi business c·∫ßn tr·∫£ l·ªùi:**
```
üìä Analytics Questions:
- Xu h∆∞·ªõng l∆∞∆°ng theo v·ªã tr√≠/th√†nh ph·ªë?
- Skill n√†o ƒëang hot nh·∫•t?
- Company n√†o tuy·ªÉn nhi·ªÅu nh·∫•t?
- Th·ªã tr∆∞·ªùng vi·ªác l√†m IT growth nh∆∞ th·∫ø n√†o?
- Job posting patterns theo th·ªùi gian?

üë• User Personas:
- Job seekers: "T√¨m job ph√π h·ª£p v·ªõi skill"
- Recruiters: "Hi·ªÉu market trends"  
- Researchers: "Ph√¢n t√≠ch th·ªã tr∆∞·ªùng lao ƒë·ªông"
```

#### **KPIs c·∫ßn track:**
```
üìà Core Metrics:
- S·ªë l∆∞·ª£ng job postings/th√°ng
- Average salary by location/role
- Top skills in demand
- Hiring velocity by company
- Job fulfillment rate
```

---

### **B∆Ø·ªöC 2: DATA DISCOVERY & AUDIT (1 tu·∫ßn)**

#### **Inventory d·ªØ li·ªáu hi·ªán c√≥:**
```
üìã Current Data Sources:
‚úÖ 10 job websites (JobsGO, TopCV, ITviec, LinkedIn, etc.)
‚úÖ 17+ fields per job (title, company, salary, location, description, etc.)
‚úÖ SQL Server database v·ªõi 10,000+ records
‚úÖ Daily crawling automation

üìä Data Quality Assessment:
- Completeness: 85% jobs c√≥ ƒë·∫ßy ƒë·ªß title + company
- Accuracy: 90% salary format ƒë√∫ng
- Consistency: 70% location c·∫ßn standardize
- Timeliness: Daily updates
```

#### **Gaps c·∫ßn fill:**
```
‚ùå Missing Data:
- Company size/industry classification
- Job posting duration (how long posted)
- Application success rates
- Skill tags standardization
- Geographic coordinates for locations
```

---

### **B∆Ø·ªöC 3: DIMENSIONAL MODELING (2 tu·∫ßn)**

#### **Business Process Analysis:**
```
üîç Core Business Process: "Job Market Analysis"

Key business events:
1. Job ƒë∆∞·ª£c post l√™n website
2. Job ƒë∆∞·ª£c crawl v√†o system
3. Job ƒë∆∞·ª£c search b·ªüi users
4. Job expired/fulfilled
```

#### **Dimensional Model Design:**

##### **FACT TABLE: fact_job_postings**
```sql
-- Central fact table ch·ª©a metrics
CREATE TABLE fact_job_postings (
    job_posting_key BIGINT PRIMARY KEY,
    
    -- Foreign keys to dimensions
    job_key INT,           -- Link to dim_job
    company_key INT,       -- Link to dim_company  
    location_key INT,      -- Link to dim_location
    date_key INT,          -- Link to dim_date
    source_key INT,        -- Link to dim_source
    
    -- Additive measures (c√≥ th·ªÉ SUM)
    view_count INT DEFAULT 0,
    application_count INT DEFAULT 0,
    days_posted INT,
    
    -- Semi-additive measures (AVG, kh√¥ng SUM)
    salary_min DECIMAL(12,2),
    salary_max DECIMAL(12,2),
    salary_avg DECIMAL(12,2),
    
    -- Non-additive measures (ch·ªâ display)
    posting_status VARCHAR(50),  -- 'active', 'expired', 'filled'
    
    -- Timestamps
    posted_date DATE,
    crawled_date DATE,
    updated_date DATE
);
```

##### **DIMENSION TABLES:**

**dim_job (Job Information)**
```sql
CREATE TABLE dim_job (
    job_key INT PRIMARY KEY,
    
    -- Natural key
    job_id VARCHAR(100),  -- From source website
    
    -- Job attributes
    job_title VARCHAR(500),
    job_title_standardized VARCHAR(200),  -- "Senior Python Developer"
    job_level VARCHAR(50),                -- "Senior", "Junior", "Mid"
    job_category VARCHAR(100),            -- "Software Engineering", "Data Science"
    
    -- Requirements
    experience_required VARCHAR(100),
    education_required VARCHAR(100),
    skills_required TEXT,                 -- JSON array of skills
    
    -- Job details
    job_type VARCHAR(50),                 -- "Full-time", "Remote", "Contract"
    job_description TEXT,
    benefits TEXT,
    
    -- SCD Type 2 fields
    effective_date DATE,
    expiry_date DATE,
    is_current BIT DEFAULT 1
);
```

**dim_company (Company Information)**
```sql
CREATE TABLE dim_company (
    company_key INT PRIMARY KEY,
    
    -- Natural key
    company_name VARCHAR(500),
    company_name_standardized VARCHAR(200),  -- "FPT Software"
    
    -- Company attributes
    company_size VARCHAR(50),                -- "1-50", "51-200", "200+"
    industry VARCHAR(100),                   -- "Technology", "Finance"
    company_type VARCHAR(50),                -- "Startup", "Enterprise", "MNC"
    
    -- Location
    headquarters_location VARCHAR(200),
    
    -- Metadata
    first_seen_date DATE,
    last_updated DATE,
    is_active BIT DEFAULT 1
);
```

**dim_location (Geographic Information)**
```sql
CREATE TABLE dim_location (
    location_key INT PRIMARY KEY,
    
    -- Geographic hierarchy
    location_raw VARCHAR(200),           -- Original text
    city VARCHAR(100),                   -- "Ho Chi Minh City"
    province VARCHAR(100),               -- "Ho Chi Minh"
    region VARCHAR(50),                  -- "South", "North", "Central"
    country VARCHAR(50) DEFAULT 'Vietnam',
    
    -- Coordinates for mapping
    latitude DECIMAL(10, 8),
    longitude DECIMAL(11, 8),
    
    -- Economic data (future enhancement)
    cost_of_living_index DECIMAL(5,2),
    avg_rent DECIMAL(10,2)
);
```

**dim_date (Time Dimension)**
```sql
CREATE TABLE dim_date (
    date_key INT PRIMARY KEY,            -- YYYYMMDD format
    
    -- Date attributes
    full_date DATE,
    year INT,
    quarter INT,
    month INT,
    month_name VARCHAR(20),
    week_of_year INT,
    day_of_month INT,
    day_of_week INT,
    day_name VARCHAR(20),
    
    -- Business calendar
    is_weekend BIT,
    is_holiday BIT,
    is_business_day BIT,
    
    -- Fiscal calendar (if needed)
    fiscal_year INT,
    fiscal_quarter INT
);
```

**dim_source (Data Source Information)**
```sql
CREATE TABLE dim_source (
    source_key INT PRIMARY KEY,
    
    -- Source attributes
    source_name VARCHAR(100),            -- "TopCV", "ITviec"
    source_url VARCHAR(500),
    source_type VARCHAR(50),             -- "Job Board", "Company Site"
    source_category VARCHAR(50),         -- "General", "Tech-focused"
    
    -- Quality metrics
    data_quality_score DECIMAL(3,2),    -- 0.00 to 1.00
    update_frequency VARCHAR(50),        -- "Daily", "Real-time"
    reliability_score DECIMAL(3,2),
    
    -- Metadata
    first_crawled_date DATE,
    is_active BIT DEFAULT 1
);
```

---

### **B∆Ø·ªöC 4: ETL DESIGN (2 tu·∫ßn)**

#### **Extract (Extraction Strategy):**
```
üîÑ Current State:
‚úÖ Scrapy spiders ƒë√£ extract t·ª´ 10 sources
‚úÖ Raw data v√†o SQL Server

üéØ Enhancement:
- Add data lineage tracking
- Implement incremental extraction
- Error handling & retry logic
- Data validation at source
```

#### **Transform (Business Logic):**
```python
# Transformation Rules (Manual Definition)

def standardize_job_title(raw_title):
    """Standardize job titles theo business rules"""
    mappings = {
        'Sr. Developer': 'Senior Developer',
        'Jr. Developer': 'Junior Developer', 
        'Dev': 'Developer',
        'Eng': 'Engineer',
        'PM': 'Project Manager'
    }
    return mappings.get(raw_title, raw_title)

def parse_salary(salary_text):
    """Parse salary t·ª´ text th√†nh numeric range"""
    # "15-20 tri·ªáu" ‚Üí min: 15000000, max: 20000000
    # "Th·ªèa thu·∫≠n" ‚Üí min: NULL, max: NULL
    # "$1000-2000" ‚Üí min: 1000, max: 2000 (USD)
    pass

def standardize_location(raw_location):
    """Standardize locations theo hierarchy"""
    mapping = {
        'HCM': 'Ho Chi Minh City',
        'H√† N·ªôi': 'Hanoi',
        'ƒê√† N·∫µng': 'Da Nang'
    }
    return mapping.get(raw_location, raw_location)

def extract_skills(job_description, requirements):
    """Extract skill tags t·ª´ job description"""
    skills = ['Python', 'Java', 'React', 'SQL', 'AWS', 'Docker']
    found_skills = []
    for skill in skills:
        if skill.lower() in (job_description + requirements).lower():
            found_skills.append(skill)
    return found_skills
```

#### **Load (Loading Strategy):**
```sql
-- Slowly Changing Dimension Type 2
-- Example: Company information changes over time

-- When company changes industry:
-- Old record: effective_date='2024-01-01', expiry_date='2024-06-30', is_current=0
-- New record: effective_date='2024-07-01', expiry_date='9999-12-31', is_current=1

MERGE dim_company AS target
USING staging_company AS source
ON target.company_name = source.company_name AND target.is_current = 1
WHEN MATCHED AND target.industry != source.industry THEN
    UPDATE SET expiry_date = GETDATE(), is_current = 0
WHEN NOT MATCHED THEN
    INSERT (company_name, industry, effective_date, is_current)
    VALUES (source.company_name, source.industry, GETDATE(), 1);
```

---

### **B∆Ø·ªöC 5: IMPLEMENTATION PHASES**

#### **Phase 1: Bronze Layer (Raw Data Warehouse)**
```
üì• Purpose: Store raw scraped data v·ªõi minimal processing

Structure:
- jobs_bronze: Raw job data as-is
- crawl_log: ETL execution tracking  
- data_quality: Quality metrics per batch

Technology:
- SQLite cho development
- PostgreSQL cho production
```

#### **Phase 2: Silver Layer (Cleaned Data Warehouse)**  
```
üîß Purpose: Cleaned, standardized, business-ready data

Transformations:
- Text normalization & cleaning
- Salary parsing & standardization  
- Location hierarchy mapping
- Duplicate removal
- Data validation

Technology:
- Pandas/Polars cho transformation
- DBT cho SQL transformations
- Great Expectations cho data quality
```

#### **Phase 3: Gold Layer (Analytics Data Marts)**
```
üìä Purpose: Pre-aggregated data cho specific use cases

Data Marts:
- job_market_trends: Monthly aggregations
- salary_analysis: Salary benchmarks
- skill_demand: Skill popularity trends
- company_insights: Hiring patterns

Technology:  
- Parquet files cho analytics
- Apache Spark cho big data processing
- ClickHouse cho OLAP queries
```

---

### **B∆Ø·ªöC 6: AUTOMATION STRATEGY**

#### **Level 1: Basic Automation (Tu·∫ßn 1-2)**
```bash
# Cron job ƒë∆°n gi·∫£n
# /etc/crontab
0 2 * * * cd /path/to/project && python etl/bronze_pipeline.py
0 4 * * * cd /path/to/project && python etl/silver_pipeline.py  
0 6 * * * cd /path/to/project && python etl/gold_pipeline.py
```

#### **Level 2: Orchestration (Tu·∫ßn 3-6)**
```python
# Apache Airflow DAG
from airflow import DAG
from airflow.operators.python_operator import PythonOperator

dag = DAG(
    'job_data_warehouse_pipeline',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1)
)

bronze_task = PythonOperator(
    task_id='bronze_etl',
    python_callable=run_bronze_pipeline,
    dag=dag
)

silver_task = PythonOperator(
    task_id='silver_etl', 
    python_callable=run_silver_pipeline,
    dag=dag
)

# Dependencies
bronze_task >> silver_task >> gold_task
```

#### **Level 3: Production Automation (Tu·∫ßn 7-10)**
```yaml
# Docker Compose cho full automation
version: '3.8'
services:
  airflow-webserver:
    image: apache/airflow:2.5.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
  
  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: data_warehouse
      
  redis:
    image: redis:6
    
  monitoring:
    image: grafana/grafana
    ports:
      - "3000:3000"
```

---

### **B∆Ø·ªöC 7: MONITORING & MAINTENANCE**

#### **Data Quality Monitoring:**
```python
# Great Expectations cho data validation
import great_expectations as ge

def validate_job_data(df):
    """Validate job data quality"""
    suite = ge.DataContext().create_expectation_suite("job_validation")
    
    # Business rules
    suite.expect_column_to_not_be_null("job_title")
    suite.expect_column_to_not_be_null("company_name")
    suite.expect_column_values_to_be_in_set("source_site", 
        ["topcv.vn", "itviec.com", "jobsgo.vn"])
    suite.expect_column_values_to_match_regex("salary", 
        r"^\d+-\d+ tri·ªáu|Th·ªèa thu·∫≠n|Competitive$")
    
    # Run validation
    results = df.validate(expectation_suite=suite)
    return results
```

#### **Pipeline Monitoring:**
```python
def monitor_pipeline_health():
    """Monitor ETL pipeline health"""
    metrics = {
        'last_run_time': get_last_etl_run(),
        'success_rate': calculate_success_rate(),
        'data_freshness': check_data_freshness(),
        'record_count': get_daily_record_count(),
        'quality_score': calculate_quality_score()
    }
    
    # Alert if issues
    if metrics['success_rate'] < 0.95:
        send_alert("ETL success rate below threshold")
    
    return metrics
```

---

## üéØ **DIMENSIONAL MODEL CHO JOB DATA**

### **Star Schema Design:**

```
                    dim_date
                        |
                        |
dim_job -------- fact_job_postings -------- dim_company
                        |
                        |
                   dim_location
                        |
                   dim_source
```

### **Business Questions ‚Üí SQL Queries:**

#### **"Top 10 companies tuy·ªÉn nhi·ªÅu nh·∫•t th√°ng n√†y"**
```sql
SELECT 
    c.company_name,
    COUNT(*) as job_count
FROM fact_job_postings f
JOIN dim_company c ON f.company_key = c.company_key
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.year = 2024 AND d.month = 12
GROUP BY c.company_name
ORDER BY job_count DESC
LIMIT 10;
```

#### **"M·ª©c l∆∞∆°ng trung b√¨nh theo th√†nh ph·ªë"**
```sql
SELECT 
    l.city,
    AVG(f.salary_avg) as avg_salary,
    COUNT(*) as job_count
FROM fact_job_postings f
JOIN dim_location l ON f.location_key = l.location_key
WHERE f.salary_avg IS NOT NULL
GROUP BY l.city
ORDER BY avg_salary DESC;
```

#### **"Skill demand trends theo th·ªùi gian"**
```sql
WITH skill_trends AS (
    SELECT 
        d.year,
        d.month,
        j.skills_required,
        COUNT(*) as job_count
    FROM fact_job_postings f
    JOIN dim_job j ON f.job_key = j.job_key
    JOIN dim_date d ON f.date_key = d.date_key
    WHERE j.skills_required IS NOT NULL
    GROUP BY d.year, d.month, j.skills_required
)
SELECT * FROM skill_trends
ORDER BY year, month, job_count DESC;
```

---

## üìä **DATA FLOW ARCHITECTURE**

### **Current ‚Üí Target State:**

#### **Current (Bronze Only):**
```
Scrapy ‚Üí SQL Server (raw) ‚Üí FastAPI ‚Üí Web Dashboard
```

#### **Target (Bronze-Silver-Gold):**
```
                    ‚îå‚îÄ Bronze Layer ‚îÄ‚îê
Scrapy ‚Üí Raw Data   ‚îÇ  Raw job data  ‚îÇ
                    ‚îÇ  Minimal proc. ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ Silver Layer ‚îÄ‚îê
                    ‚îÇ  Cleaned data  ‚îÇ ‚Üí Business Reports
                    ‚îÇ  Standardized  ‚îÇ ‚Üí KPI Dashboards  
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ Gold Layer ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Analytics     ‚îÇ ‚Üí ML Models
                    ‚îÇ  Aggregations  ‚îÇ ‚Üí Advanced Analytics
                    ‚îÇ  Business KPIs ‚îÇ ‚Üí Executive Dashboards
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ **IMPLEMENTATION ROADMAP**

### **Week 1-2: Foundation**
```
üìã Tasks:
1. Design dimensional model (manual)
2. Create dimension tables (manual)  
3. Build basic ETL scripts (manual)
4. Setup data validation (manual)

üéØ Deliverable: Working dimensional model v·ªõi sample data
```

### **Week 3-4: Bronze ‚Üí Silver Pipeline**
```
üìã Tasks:
1. Data cleaning functions (manual logic)
2. Standardization rules (manual definition)
3. Automated execution (cron jobs)
4. Quality monitoring (automated alerts)

üéØ Deliverable: Clean, standardized data ready for analysis
```

### **Week 5-6: Gold Layer & Analytics**
```
üìã Tasks:
1. Business KPIs definition (manual)
2. Aggregation pipelines (automated)
3. Analytics dashboard (manual design)
4. Performance optimization (automated)

üéØ Deliverable: Business intelligence dashboard
```

### **Week 7-8: Advanced Features**
```
üìã Tasks:
1. ML job matching (manual algorithm design)
2. Trend analysis (automated calculations)
3. Recommendation engine (hybrid)
4. API enhancements (manual endpoints)

üéØ Deliverable: Smart job platform v·ªõi ML features
```

---

## üí° **PRACTICAL TIPS**

### **Start Small, Scale Smart:**
```
‚úÖ Do:
- Begin v·ªõi 1-2 business questions
- Focus on data quality first
- Automate gradually  
- Document everything

‚ùå Don't:
- Build everything at once
- Over-engineer t·ª´ ƒë·∫ßu
- Ignore data quality
- Skip documentation
```

### **Common Pitfalls & Solutions:**
```
üö® Pitfall: "Perfect schema from day 1"
‚úÖ Solution: Start simple, evolve schema

üö® Pitfall: "Automate everything immediately"  
‚úÖ Solution: Manual ‚Üí Semi-auto ‚Üí Full auto

üö® Pitfall: "Complex tools from start"
‚úÖ Solution: Python scripts ‚Üí Airflow ‚Üí Enterprise tools
```

---

## üéØ **SUCCESS METRICS**

### **Technical Metrics:**
- Data quality score > 90%
- ETL success rate > 95%
- Query response time < 5s
- Data freshness < 24h

### **Business Metrics:**
- User engagement v·ªõi analytics
- Decision making speed improvement
- Cost reduction t·ª´ automation
- Time-to-insight reduction

---

## üìö **LEARNING PATH**

### **Fundamentals (Week 1-2):**
1. Kimball dimensional modeling
2. SQL performance tuning
3. Data quality concepts
4. ETL best practices

### **Implementation (Week 3-6):**
1. Python data processing (Pandas)
2. Database design patterns
3. API development (FastAPI)
4. Testing strategies

### **Advanced (Week 7-10):**
1. Apache Airflow
2. Docker containerization
3. ML for recommendations
4. Production deployment

---

**üéâ K·∫øt lu·∫≠n: Data Warehouse KH√îNG ho√†n to√†n t·ª± ƒë·ªông. C·∫ßn k·∫øt h·ª£p manual design (business logic, data modeling) v·ªõi automated execution (ETL, monitoring). Start t·ª´ business requirements, design dimensional model, sau ƒë√≥ t·ª± ƒë·ªông h√≥a d·∫ßn d·∫ßn.**

**B·∫°n mu·ªën t√¥i detail h√≥a b∆∞·ªõc n√†o tr∆∞·ªõc?** ü§î

*(Based on 8 nƒÉm experience implement DW cho 15+ companies)*
