# ğŸš€ **DEVELOPER GETTING STARTED GUIDE**
## **CrawlJob Data Warehouse Project**

---

## ğŸ“‹ **TABLE OF CONTENTS**

1. [ğŸ¯ Project Overview](#-project-overview)
2. [ğŸ—ï¸ Current System Status](#ï¸-current-system-status)
3. [ğŸ› ï¸ Development Environment Setup](#ï¸-development-environment-setup)
4. [ğŸ“Š Data Warehouse Implementation](#-data-warehouse-implementation)
5. [ğŸ”§ Quick Start Commands](#-quick-start-commands)
6. [ğŸ“ Project Structure](#-project-structure)
7. [ğŸ¯ What You Need To Do](#-what-you-need-to-do)

---

## ğŸ¯ **PROJECT OVERVIEW**

### **What is CrawlJob?**
CrawlJob lÃ  há»‡ thá»‘ng web scraping chuyÃªn nghiá»‡p thu tháº­p dá»¯ liá»‡u viá»‡c lÃ m tá»« **10 trang tuyá»ƒn dá»¥ng Viá»‡t Nam**:
- JobsGO, JobOKO, 123Job, CareerViet, JobStreet
- LinkedIn, TopCV, ITviec, CareerLink, VietnamWorks

### **Current Status: âœ… PRODUCTION READY**
- âœ… **10 spiders** hoáº¡t Ä‘á»™ng hoÃ n háº£o
- âœ… **SQL Server database** vá»›i 10,000+ records
- âœ… **FastAPI backend** vá»›i REST endpoints
- âœ… **Web dashboard** vá»›i Bootstrap 5
- âœ… **Automated daily crawling**

### **Next Phase: ğŸ—ï¸ DATA WAREHOUSE**
Chuyá»ƒn Ä‘á»•i tá»« **Job Scraping System** â†’ **Business Intelligence Platform**

---

## ğŸ—ï¸ **CURRENT SYSTEM STATUS**

### **âœ… What's Already Built**

#### **1. Scrapy Spiders (10 sites)**
```bash
# Location: CrawlJob/spiders/
â”œâ”€â”€ jobsgo_spider.py      # JobsGO.vn - CSS selectors
â”œâ”€â”€ joboko_spider.py      # JobOKO.com - CSS selectors  
â”œâ”€â”€ job123_spider.py      # 123Job.vn - CSS selectors
â”œâ”€â”€ careerviet_spider.py  # CareerViet.vn - CSS selectors
â”œâ”€â”€ jobstreet_spider.py   # JobStreet.vn - CSS selectors
â”œâ”€â”€ linkedin_spider.py    # LinkedIn.com - Selenium
â”œâ”€â”€ topcv_spider.py       # TopCV.vn - JavaScript parsing
â”œâ”€â”€ itviec_spider.py      # ITviec.com - Undetected ChromeDriver
â”œâ”€â”€ careerlink_spider.py  # CareerLink.vn - Selenium
â””â”€â”€ vietnamworks_spider.py # VietnamWorks.com - Selenium
```

#### **2. Database Schema**
```sql
-- Current table: jobs
CREATE TABLE jobs (
    id INT IDENTITY(1,1) PRIMARY KEY,
    job_title NVARCHAR(500),
    company_name NVARCHAR(200),
    salary NVARCHAR(100),
    location NVARCHAR(200),
    job_type NVARCHAR(100),
    experience_level NVARCHAR(100),
    education_level NVARCHAR(100),
    job_industry NVARCHAR(200),
    job_position NVARCHAR(200),
    job_description NVARCHAR(MAX),
    requirements NVARCHAR(MAX),
    benefits NVARCHAR(MAX),
    job_deadline NVARCHAR(100),
    source_site NVARCHAR(100),
    job_url NVARCHAR(1000),
    search_keyword NVARCHAR(200),
    scraped_at NVARCHAR(50),
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME NULL
);
```

#### **3. API Endpoints**
```python
# Location: api/main.py
GET /health                    # Health check
GET /jobs?keyword=X&page=Y     # Job search with pagination
```

#### **4. Web Dashboard**
```bash
# Location: web/
â”œâ”€â”€ index.html          # Main dashboard
â”œâ”€â”€ css/styles.css      # Styling
â”œâ”€â”€ css/responsive.css  # Mobile responsive
â”œâ”€â”€ js/main.js          # Core logic
â”œâ”€â”€ js/api.js           # API communication
â””â”€â”€ js/ui.js            # UI helpers
```

---

## ğŸ› ï¸ **DEVELOPMENT ENVIRONMENT SETUP**

### **Prerequisites**
```bash
# Required software
âœ… Python 3.12.2
âœ… SQL Server (local or remote)
âœ… Docker Desktop
âœ… Git
âœ… VS Code (recommended)
```

### **Step 1: Clone & Setup**
```bash
# Clone project
git clone <repository-url>
cd D:\Practice\Scrapy\CrawlJob

# Create virtual environment
python -m venv venv
venv\Scripts\activate.bat

# Install dependencies
pip install -r requirements.txt
```

### **Step 2: Database Setup**
```bash
# Create .env file
cp env.example .env

# Edit .env with your database credentials
SQL_SERVER=localhost
SQL_DATABASE=JobDatabase
SQL_USERNAME=sa
SQL_PASSWORD=your_password
```

### **Step 3: Test Current System**
```bash
# Test individual spider
python run_spider.py --spider jobsgo --keyword "Python Developer"

# Test API
uvicorn api.main:app --reload
# Access: http://localhost:8000/health

# Test web dashboard
# Open: web/index.html in browser
```

---

## ğŸ“Š **DATA WAREHOUSE IMPLEMENTATION**

### **ğŸ¯ Your Main Task: Build Data Warehouse**

#### **Architecture Overview**
```
Current: CrawlJob â†’ SQL Server (OLTP)
Target:  CrawlJob â†’ Data Warehouse (OLAP)
         â”œâ”€â”€ Bronze Layer (Raw Data)
         â”œâ”€â”€ Silver Layer (Cleaned Data)  
         â””â”€â”€ Gold Layer (Analytics)
```

#### **Technology Stack**
- **dbt**: Data transformation vÃ  modeling
- **Apache Airflow**: Pipeline orchestration
- **Great Expectations**: Data quality validation
- **Docker**: Containerization
- **SQL Server**: Database (existing)

### **Implementation Phases**

#### **Phase 1: Foundation (Week 1-2)**
- [ ] Deploy Docker environment
- [ ] Build Bronze layer ETL
- [ ] Create monitoring dashboard

#### **Phase 2: Silver Layer (Week 2-4)**
- [ ] Implement data cleaning
- [ ] Add quality validations
- [ ] Build dimensional model

#### **Phase 3: Gold Layer (Week 4-6)**
- [ ] Create business KPIs
- [ ] Build analytics dashboards
- [ ] Implement automation

#### **Phase 4: Production (Week 6-8)**
- [ ] Cloud deployment
- [ ] Monitoring & alerting
- [ ] Go-live preparation

---

## ğŸ”§ **QUICK START COMMANDS**

### **Current System Commands**
```bash
# Run individual spider
python run_spider.py --spider jobsgo --keyword "Data Analyst"

# Run all spiders
python run_spider.py --spider all --keyword "IT Developer"

# Start API server
uvicorn api.main:app --reload

# Check database
sqlcmd -S localhost -U sa -P password -Q "SELECT COUNT(*) FROM jobs"
```

### **Data Warehouse Commands**
```bash
# Deploy Docker environment
cd deployment
docker-compose up -d

# Access Airflow
# http://localhost:8080 (admin/admin)

# Access monitoring dashboard
# http://localhost:8501

# Run dbt
dbt run --project-dir dbt_project
dbt test --project-dir dbt_project
```

---

## ğŸ“ **PROJECT STRUCTURE**

```
D:\Practice\Scrapy\CrawlJob\
â”œâ”€â”€ ğŸ“‹ plan/                          # ğŸ“‹ ALL PROJECT PLANS HERE
â”‚   â”œâ”€â”€ COMPLETE_DATA_WAREHOUSE_GUIDE.md  # ğŸ¯ MASTER PLAN FILE
â”‚   â”œâ”€â”€ DEVELOPER_GETTING_STARTED.md      # ğŸ“– This file
â”‚   â”œâ”€â”€ ARCHITECTURE_OVERVIEW.md          # ğŸ—ï¸ Technical architecture
â”‚   â”œâ”€â”€ IMPLEMENTATION_ROADMAP.md         # ğŸš€ Step-by-step guide
â”‚   â”œâ”€â”€ API_REFERENCE.md                 # ğŸ“š API documentation
â”‚   â”œâ”€â”€ DEPLOYMENT_GUIDE.md              # â˜ï¸ Production deployment
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md               # ğŸ”§ Common issues
â”‚   â””â”€â”€ DATABASE_SCHEMA.md               # ğŸ—„ï¸ Database design
â”‚
â”œâ”€â”€ ğŸ•·ï¸ CrawlJob/                      # ğŸ•·ï¸ Scrapy spiders (10 sites)
â”‚   â”œâ”€â”€ spiders/
â”‚   â”œâ”€â”€ items.py
â”‚   â”œâ”€â”€ pipelines.py
â”‚   â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ ğŸš€ api/                           # ğŸš€ FastAPI backend
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ ğŸŒ web/                           # ğŸŒ Frontend dashboard
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ css/
â”‚   â””â”€â”€ js/
â”‚
â”œâ”€â”€ ğŸ³ deployment/                    # ğŸ³ Docker & deployment
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ dags/
â”‚   â””â”€â”€ monitoring/
â”‚
â”œâ”€â”€ ğŸ“Š dbt_project/                   # ğŸ“Š Data transformation
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ ğŸ“ logs/                          # ğŸ“ Crawling logs
â”œâ”€â”€ ğŸ’¾ outputs/                       # ğŸ’¾ JSON output files
â””â”€â”€ ğŸ§ª test/                          # ğŸ§ª Testing framework
```

---

## ğŸ¯ **WHAT YOU NEED TO DO**

### **Immediate Tasks (Today)**

#### **1. Understand Current System**
- [ ] Read `plan/COMPLETE_DATA_WAREHOUSE_GUIDE.md`
- [ ] Test current spiders: `python run_spider.py --spider jobsgo --keyword "test"`
- [ ] Check database: `sqlcmd -S localhost -U sa -P password -Q "SELECT TOP 10 * FROM jobs"`
- [ ] Test API: `curl http://localhost:8000/health`

#### **2. Setup Development Environment**
- [ ] Install Docker Desktop
- [ ] Create `deployment/` directory
- [ ] Copy Docker configuration from guide
- [ ] Test Docker setup

#### **3. Start Data Warehouse Implementation**
- [ ] Deploy Docker environment
- [ ] Create Bronze layer ETL
- [ ] Build basic monitoring dashboard

### **This Week Tasks**

#### **Week 1: Foundation**
- [ ] Complete Docker deployment
- [ ] Implement Bronze layer ETL
- [ ] Create monitoring dashboard
- [ ] Test data flow from CrawlJob â†’ Bronze

#### **Week 2: Silver Layer**
- [ ] Setup dbt project
- [ ] Create staging models
- [ ] Implement data cleaning
- [ ] Build dimensional models

### **Success Criteria**

#### **Technical Success**
- [ ] ETL pipelines run reliably daily
- [ ] Data quality > 90%
- [ ] Query performance < 5 seconds
- [ ] Zero data loss in transformations

#### **Business Success**
- [ ] Business users access insights daily
- [ ] Decision-making speed improved
- [ ] Cost reduction from automation
- [ ] Better job market understanding

---

## ğŸš€ **NEXT STEPS**

### **Start Here**
1. **Read the master plan**: `plan/COMPLETE_DATA_WAREHOUSE_GUIDE.md`
2. **Setup environment**: Follow this guide
3. **Deploy Docker**: Quick deployment section
4. **Implement Bronze layer**: ETL implementation

### **Key Documents to Read**
- ğŸ“– `plan/ARCHITECTURE_OVERVIEW.md` - Technical architecture
- ğŸ“– `plan/IMPLEMENTATION_ROADMAP.md` - Step-by-step implementation
- ğŸ“– `plan/DEPLOYMENT_GUIDE.md` - Production deployment
- ğŸ“– `plan/TROUBLESHOOTING.md` - Common issues and solutions

### **Support Resources**
- ğŸ“š dbt documentation: https://docs.getdbt.com/
- ğŸ“š Airflow documentation: https://airflow.apache.org/docs/
- ğŸ“š Docker documentation: https://docs.docker.com/

---

## ğŸ‰ **CONCLUSION**

**You're ready to build a professional Data Warehouse!**

**Current Status**: âœ… Production-ready job scraping system
**Next Goal**: ğŸ—ï¸ Transform into Business Intelligence platform
**Timeline**: 8 weeks to complete implementation
**Technology**: Modern data stack (dbt, Airflow, Docker)

**Start with**: Quick deployment (1 hour) to get MVP running!

---

**Questions?** Check the troubleshooting guide or ask for help. Let's build something amazing! ğŸ”¥
