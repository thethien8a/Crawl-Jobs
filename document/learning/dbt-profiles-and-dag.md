# dbt Profiles v√† DAG Dependencies - H∆∞·ªõng d·∫´n Chi ti·∫øt

## üìö M·ª•c l·ª•c
1. [dbt Profiles - Qu·∫£n l√Ω K·∫øt n·ªëi Database](#1-dbt-profiles---qu·∫£n-l√Ω-k·∫øt-n·ªëi-database)
2. [DAG Dependencies - C√°ch dbt S·∫Øp x·∫øp Th·ª© t·ª± Ch·∫°y Models](#2-dag-dependencies---c√°ch-dbt-s·∫Øp-x·∫øp-th·ª©-t·ª±-ch·∫°y-models)
3. [Best Practices](#3-best-practices)
4. [Troubleshooting](#4-troubleshooting)

---

## 1. dbt Profiles - Qu·∫£n l√Ω K·∫øt n·ªëi Database

### 1.1. Profiles.yml l√† g√¨?

**Profiles.yml** l√† file c·∫•u h√¨nh ch·ª©a th√¥ng tin **k·∫øt n·ªëi ƒë·∫øn data warehouse** (credentials, connection strings). File n√†y **KH√îNG N√äN commit v√†o Git** n·∫øu ch·ª©a th√¥ng tin nh·∫°y c·∫£m (passwords).

### 1.2. V·ªã tr√≠ m·∫∑c ƒë·ªãnh c·ªßa profiles.yml

**Theo m·∫∑c ƒë·ªãnh**, dbt t√¨m `profiles.yml` ·ªü:

| OS | ƒê∆∞·ªùng d·∫´n |
|----|-----------|
| **Windows** | `C:\Users\<username>\.dbt\profiles.yml` |
| **macOS** | `~/.dbt/profiles.yml` |
| **Linux** | `~/.dbt/profiles.yml` |

### 1.3. Override v·ªã tr√≠ profiles.yml

C√≥ 2 c√°ch ƒë·ªÉ dbt t√¨m `profiles.yml` ·ªü v·ªã tr√≠ kh√°c:

#### **C√°ch 1: S·ª≠ d·ª•ng flag `--profiles-dir`**
```bash
dbt run --profiles-dir .
dbt run --profiles-dir /path/to/profiles
```

**√ù nghƒ©a:**
- `--profiles-dir .` ‚Üí T√¨m `profiles.yml` ·ªü **th∆∞ m·ª•c hi·ªán t·∫°i**
- H·ªØu √≠ch khi ƒë∆∞a `profiles.yml` v√†o Git (v·ªõi hardcoded paths ho·∫∑c env vars)

#### **C√°ch 2: S·ª≠ d·ª•ng bi·∫øn m√¥i tr∆∞·ªùng `DBT_PROFILES_DIR`**
```bash
# Windows (CMD)
set DBT_PROFILES_DIR=D:\Practice\Scrapy\CrawlJob\dbt_crawjob
dbt run

# Windows (PowerShell)
$env:DBT_PROFILES_DIR="D:\Practice\Scrapy\CrawlJob\dbt_crawjob"
dbt run

# Linux/macOS
export DBT_PROFILES_DIR=/path/to/dbt_project
dbt run
```

**L·ª£i √≠ch:** Kh√¥ng c·∫ßn th√™m `--profiles-dir` m·ªói l·∫ßn ch·∫°y command.

### 1.4. C·∫•u tr√∫c profiles.yml

#### **V√≠ d·ª• c∆° b·∫£n (DuckDB):**
```yaml
crawljob:                          # Profile name (ph·∫£i kh·ªõp v·ªõi dbt_project.yml)
  target: dev                       # Target m·∫∑c ƒë·ªãnh
  outputs:
    dev:                            # Development environment
      type: duckdb                  # Adapter type
      path: "D:\\Practice\\Scrapy\\CrawlJob\\DuckDB\\warehouse.duckdb"
    
    prod:                           # Production environment
      type: duckdb
      path: "/prod/warehouse.duckdb"
```

#### **V√≠ d·ª• v·ªõi PostgreSQL:**
```yaml
my_project:
  target: dev
  outputs:
    dev:
      type: postgres
      host: localhost
      port: 5432
      user: myuser
      password: mypassword          # ‚ö†Ô∏è Kh√¥ng commit v√†o Git!
      dbname: dev_db
      schema: analytics
      threads: 4
    
    prod:
      type: postgres
      host: prod-db.example.com
      port: 5432
      user: "{{ env_var('PROD_DB_USER') }}"          # ‚úÖ D√πng env vars
      password: "{{ env_var('PROD_DB_PASSWORD') }}"
      dbname: prod_db
      schema: analytics
      threads: 8
```

#### **V√≠ d·ª• v·ªõi Snowflake:**
```yaml
snowflake_project:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: my_account
      user: dev_user
      password: "{{ env_var('SNOWFLAKE_PASSWORD') }}"
      role: DEV_ROLE
      database: DEV_DB
      warehouse: DEV_WH
      schema: analytics
      threads: 4
```

### 1.5. Li√™n k·∫øt profiles.yml v·ªõi dbt_project.yml

Trong `dbt_project.yml`, ph·∫£i ch·ªâ ƒë·ªãnh profile name:

```yaml
name: 'crawljob'
version: '1.0.0'
profile: 'crawljob'        # ‚Üê Ph·∫£i kh·ªõp v·ªõi t√™n profile trong profiles.yml
```

### 1.6. Switching gi·ªØa Environments (dev/prod)

```bash
# Ch·∫°y v·ªõi target dev (m·∫∑c ƒë·ªãnh)
dbt run --profiles-dir .

# Ch·∫°y v·ªõi target prod
dbt run --profiles-dir . --target prod

# Debug connection
dbt debug --profiles-dir . --target dev
```

### 1.7. Best Practices cho Profiles

| ‚úÖ **N√äN** | ‚ùå **KH√îNG N√äN** |
|----------|---------------|
| D√πng `env_var()` cho credentials | Hard-code passwords v√†o profiles.yml |
| C√≥ targets ri√™ng cho dev/prod | D√πng chung 1 target cho m·ªçi m√¥i tr∆∞·ªùng |
| `.gitignore` profiles.yml n·∫øu c√≥ sensitive data | Commit passwords v√†o Git |
| D√πng `--profiles-dir .` cho CI/CD | Rely on `~/.dbt/` trong CI |
| Validate connection v·ªõi `dbt debug` | Ch·∫°y `dbt run` m√† kh√¥ng ki·ªÉm tra connection tr∆∞·ªõc |

### 1.8. S·ª≠ d·ª•ng Environment Variables

**Trong profiles.yml:**
```yaml
crawljob:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: "{{ env_var('DUCKDB_PATH') }}"  # ƒê·ªçc t·ª´ env var
```

**Set env var tr∆∞·ªõc khi ch·∫°y dbt:**
```bash
# Windows CMD
set DUCKDB_PATH=D:\Practice\Scrapy\CrawlJob\DuckDB\warehouse.duckdb
dbt run --profiles-dir .

# Linux/macOS
export DUCKDB_PATH=/path/to/warehouse.duckdb
dbt run --profiles-dir .
```

‚ö†Ô∏è **L∆ØU √ù:** DuckDB adapter c√≥ th·ªÉ kh√¥ng h·ªó tr·ª£ env_var() t·ªët, n√™n d√πng hardcoded path trong profiles.yml.

---

## 2. DAG Dependencies - C√°ch dbt S·∫Øp x·∫øp Th·ª© t·ª± Ch·∫°y Models

### 2.1. DAG (Directed Acyclic Graph) l√† g√¨?

**DAG** l√† **ƒë·ªì th·ªã c√≥ h∆∞·ªõng kh√¥ng c√≥ chu tr√¨nh**, d√πng ƒë·ªÉ bi·ªÉu di·ªÖn **dependencies** gi·ªØa c√°c models.

```
Source (bronze.jobs)
    ‚Üì
Model A (silver.stg_jobs)
    ‚Üì         ‚Üì
Model B      Model C (gold.dim_company, gold.fct_jobs)
```

**ƒê·∫∑c ƒëi·ªÉm:**
- **Directed (C√≥ h∆∞·ªõng):** A ‚Üí B nghƒ©a l√† B ph·ª• thu·ªôc A
- **Acyclic (Kh√¥ng chu tr√¨nh):** Kh√¥ng c√≥ v√≤ng l·∫∑p (A ‚Üí B ‚Üí A)

### 2.2. C√°ch dbt Ph√°t hi·ªán Dependencies

dbt **T·ª∞ ƒê·ªòNG** ph√°t hi·ªán dependencies b·∫±ng c√°ch parse **Jinja functions** trong SQL:

#### **H√†m 1: `{{ source('schema', 'table') }}`**

Tham chi·∫øu ƒë·∫øn **external table** (ƒë√£ t·ªìn t·∫°i trong warehouse, kh√¥ng do dbt t·∫°o).

**V√≠ d·ª•:**
```yaml
# models/sources.yml
version: 2
sources:
  - name: bronze               # Source name
    schema: bronze             # Schema trong warehouse
    tables:
      - name: jobs             # Table name
```

```sql
-- models/silver/stg_jobs.sql
SELECT *
FROM {{ source('bronze', 'jobs') }}  -- ‚Üê Compile th√†nh: bronze.jobs
```

**Compile th√†nh SQL:**
```sql
SELECT * FROM bronze.jobs
```

#### **H√†m 2: `{{ ref('model_name') }}`**

Tham chi·∫øu ƒë·∫øn **model kh√°c** trong dbt project.

**V√≠ d·ª•:**
```sql
-- models/gold/fct_jobs.sql
SELECT *
FROM {{ ref('stg_jobs') }}  -- ‚Üê Tham chi·∫øu model stg_jobs
```

**Compile th√†nh SQL:**
```sql
SELECT * FROM silver.stg_jobs
```

**Dependencies:**
```
stg_jobs (silver) ‚Üí fct_jobs (gold)
```

#### **H√†m 3: `{{ this }}`**

Tham chi·∫øu ƒë·∫øn **ch√≠nh model hi·ªán t·∫°i** (d√πng trong incremental models).

**V√≠ d·ª•:**
```sql
-- models/silver/stg_jobs.sql
{% if is_incremental() %}
WHERE scraped_at > (SELECT MAX(scraped_at) FROM {{ this }})
{% endif %}
```

**Compile th√†nh SQL:**
```sql
WHERE scraped_at > (SELECT MAX(scraped_at) FROM silver.stg_jobs)
```

### 2.3. V√≠ d·ª•: Build DAG cho Project CrawlJob

#### **Step 1: Define Sources**
```yaml
# models/sources.yml
version: 2
sources:
  - name: bronze
    schema: bronze
    tables:
      - name: jobs  # Level 0 - External table
```

#### **Step 2: Silver Layer Model**
```sql
-- models/silver/stg_jobs.sql
{{ config(
  materialized='incremental',
  schema='silver',
  unique_key=['job_url','scraped_date']
) }}

SELECT
  job_url,
  TRIM(job_title) AS job_title,
  LOWER(company_name) AS company_name,
  salary,
  location,
  scraped_at
FROM {{ source('bronze','jobs') }}  -- ‚Üê Depends on bronze.jobs

{% if is_incremental() %}
WHERE scraped_at > (SELECT MAX(scraped_at) FROM {{ this }})
{% endif %}
```

**Dependencies:** `bronze.jobs ‚Üí stg_jobs`

#### **Step 3: Gold Layer Models**
```sql
-- models/gold/dim_company.sql
{{ config(
  materialized='table',
  schema='gold'
) }}

SELECT DISTINCT
  company_name,
  COUNT(*) AS total_jobs
FROM {{ ref('stg_jobs') }}  -- ‚Üê Depends on stg_jobs
GROUP BY company_name
```

```sql
-- models/gold/fct_jobs.sql
{{ config(
  materialized='incremental',
  schema='gold',
  unique_key='job_url'
) }}

SELECT
  job_url,
  job_title,
  company_name,
  salary,
  location
FROM {{ ref('stg_jobs') }}  -- ‚Üê Depends on stg_jobs
```

**Dependencies:**
```
bronze.jobs (source)
    ‚Üì
stg_jobs (silver)
    ‚Üì              ‚Üì
dim_company    fct_jobs (gold)
```

### 2.4. dbt Execution Order (Topological Sort)

Khi ch·∫°y `dbt run`, dbt s·∫Ω:

#### **Step 1: Parse t·∫•t c·∫£ models**
- ƒê·ªçc t·∫•t c·∫£ `.sql`, `.yml` files
- T√¨m `{{ source() }}`, `{{ ref() }}` trong SQL

#### **Step 2: Build DAG**
```
Level 0: bronze.jobs (source - skip)
Level 1: stg_jobs (depends on bronze.jobs)
Level 2: dim_company, fct_jobs (depends on stg_jobs)
```

#### **Step 3: Topological Sort**
S·∫Øp x·∫øp th·ª© t·ª± execute t·ª´ upstream ‚Üí downstream:
```
1. stg_jobs (run first)
2. dim_company, fct_jobs (run in parallel - c√πng level)
```

#### **Step 4: Execute**
```sql
-- 1. CREATE silver.stg_jobs
CREATE TABLE silver.stg_jobs AS
SELECT ... FROM bronze.jobs;

-- 2. CREATE gold.dim_company (parallel)
CREATE TABLE gold.dim_company AS
SELECT ... FROM silver.stg_jobs;

-- 3. CREATE gold.fct_jobs (parallel)
CREATE TABLE gold.fct_jobs AS
SELECT ... FROM silver.stg_jobs;
```

### 2.5. Visualize DAG

#### **C√°ch 1: dbt Docs**
```bash
cd dbt_crawjob
dbt docs generate --profiles-dir .
dbt docs serve
```

‚Üí M·ªü browser t·∫°i `http://localhost:8080`, xem **Lineage Graph**.

#### **C√°ch 2: dbt ls (List Resources)**
```bash
# Xem t·∫•t c·∫£ models
dbt ls --profiles-dir .

# Xem dependencies c·ªßa stg_jobs
dbt ls --select +stg_jobs      # Upstream (models m√† stg_jobs ph·ª• thu·ªôc)
dbt ls --select stg_jobs+      # Downstream (models ph·ª• thu·ªôc stg_jobs)
dbt ls --select +stg_jobs+     # Both upstream & downstream

# Xem graph structure
dbt ls --select +stg_jobs+ --output json
```

### 2.6. Node Selection Syntax

dbt cung c·∫•p **powerful syntax** ƒë·ªÉ ch·∫°y subset models:

| Syntax | √ù nghƒ©a | V√≠ d·ª• |
|--------|---------|-------|
| `dbt run -s model_name` | Ch·ªâ ch·∫°y 1 model | `dbt run -s stg_jobs` |
| `dbt run -s +model_name` | Ch·∫°y model + t·∫•t c·∫£ upstream | `dbt run -s +fct_jobs` |
| `dbt run -s model_name+` | Ch·∫°y model + t·∫•t c·∫£ downstream | `dbt run -s stg_jobs+` |
| `dbt run -s +model_name+` | Ch·∫°y model + upstream + downstream | `dbt run -s +stg_jobs+` |
| `dbt run -s tag:daily` | Ch·∫°y models c√≥ tag `daily` | `dbt run -s tag:daily` |
| `dbt run -s path:models/silver` | Ch·∫°y models trong folder | `dbt run -s path:models/silver` |
| `dbt run -s @model_name` | Ch·∫°y model + parents (1 level up) | `dbt run -s @fct_jobs` |
| `dbt run --exclude model_name` | Ch·∫°y t·∫•t c·∫£ tr·ª´ model | `dbt run --exclude stg_jobs` |

**V√≠ d·ª• th·ª±c t·∫ø:**
```bash
# Ch·∫°y ch·ªâ silver layer
dbt run -s path:models/silver --profiles-dir .

# Ch·∫°y stg_jobs v√† t·∫•t c·∫£ models downstream (gold)
dbt run -s stg_jobs+ --profiles-dir .

# Ch·∫°y t·∫•t c·∫£ models c·∫ßn cho fct_jobs (upstream + ch√≠nh n√≥)
dbt run -s +fct_jobs --profiles-dir .

# Ch·∫°y models c√≥ tag "hourly"
dbt run -s tag:hourly --profiles-dir .
```

### 2.7. Parallel Execution

dbt ch·∫°y models **song song** n·∫øu ch√∫ng ·ªü **c√πng level** trong DAG.

**Config threads trong profiles.yml:**
```yaml
crawljob:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: "warehouse.duckdb"
      threads: 4  # ‚Üê S·ªë l∆∞·ª£ng models ch·∫°y song song
```

**V√≠ d·ª•:**
```
stg_jobs (run first - 1 thread)
    ‚Üì
dim_company, fct_jobs, dim_location, agg_daily (run parallel - 4 threads)
```

---

## 3. Best Practices

### 3.1. Naming Conventions

| Layer | Prefix | Schema | V√≠ d·ª• |
|-------|--------|--------|-------|
| **Staging** | `stg_` | `silver` | `stg_jobs`, `stg_companies` |
| **Intermediate** | `int_` | `silver` | `int_job_enriched` |
| **Marts (Dimensions)** | `dim_` | `gold` | `dim_company`, `dim_location` |
| **Marts (Facts)** | `fct_` | `gold` | `fct_jobs`, `fct_applications` |
| **Aggregates** | `agg_` | `gold` | `agg_jobs_daily` |

### 3.2. Source() vs Ref()

| H√†m | Khi n√†o d√πng | V√≠ d·ª• |
|-----|-------------|--------|
| `{{ source() }}` | External tables (kh√¥ng do dbt t·∫°o) | `{{ source('bronze','jobs') }}` |
| `{{ ref() }}` | Models trong dbt project | `{{ ref('stg_jobs') }}` |

**‚ùå KH√îNG BAO GI·ªú hard-code table names:**
```sql
-- ‚ùå SAI
SELECT * FROM bronze.jobs
SELECT * FROM silver.stg_jobs

-- ‚úÖ ƒê√öNG
SELECT * FROM {{ source('bronze','jobs') }}
SELECT * FROM {{ ref('stg_jobs') }}
```

**L√Ω do:**
- dbt kh√¥ng track dependencies n·∫øu hard-code
- Kh√¥ng refactor ƒë∆∞·ª£c khi ƒë·ªïi schema/table names

### 3.3. Incremental Models Best Practices

```sql
{{ config(
  materialized='incremental',
  unique_key=['job_url', 'scraped_date'],  -- ‚Üê Composite key
  incremental_strategy='merge'              -- ‚Üê Merge (upsert) ho·∫∑c append
) }}

SELECT
  job_url,
  job_title,
  scraped_at
FROM {{ source('bronze','jobs') }}

{% if is_incremental() %}
-- ‚úÖ Filter ch·ªâ l·∫•y d·ªØ li·ªáu m·ªõi
WHERE scraped_at > (SELECT COALESCE(MAX(scraped_at), '1970-01-01') FROM {{ this }})
{% endif %}
```

**Chi·∫øn l∆∞·ª£c incremental:**
- **`merge`**: Upsert (update existing + insert new) - **Recommend cho most cases**
- **`append`**: Ch·ªâ insert m·ªõi (kh√¥ng update) - **D√πng cho immutable data**
- **`delete+insert`**: X√≥a partition c≈© + insert m·ªõi - **D√πng cho partitioned tables**

### 3.4. Schema Organization

```
models/
‚îú‚îÄ‚îÄ sources.yml           # Define all sources
‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îî‚îÄ‚îÄ schema.yml        # Tests cho bronze tables
‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îú‚îÄ‚îÄ stg_jobs.sql
‚îÇ   ‚îú‚îÄ‚îÄ stg_companies.sql
‚îÇ   ‚îî‚îÄ‚îÄ schema.yml        # Tests + descriptions
‚îú‚îÄ‚îÄ gold/
‚îÇ   ‚îú‚îÄ‚îÄ dim_company.sql
‚îÇ   ‚îú‚îÄ‚îÄ fct_jobs.sql
‚îÇ   ‚îú‚îÄ‚îÄ agg_jobs_daily.sql
‚îÇ   ‚îî‚îÄ‚îÄ schema.yml
```

### 3.5. Tags v√† Meta

D√πng tags ƒë·ªÉ organize v√† select models:

```yaml
# models/silver/schema.yml
version: 2
models:
  - name: stg_jobs
    description: "Staging layer for jobs"
    meta:
      owner: "data_team"
      priority: "high"
    config:
      tags: ["daily", "core"]
    columns:
      - name: job_url
        tests:
          - unique
          - not_null
```

**Ch·∫°y models theo tags:**
```bash
dbt run -s tag:daily
dbt run -s tag:core
dbt test -s tag:daily
```

---

## 4. Troubleshooting

### 4.1. L·ªói: "Profile not found"

**L·ªói:**
```
Runtime Error
  Could not find profile named 'crawljob'
```

**Nguy√™n nh√¢n:**
- `profiles.yml` kh√¥ng t·ªìn t·∫°i ho·∫∑c ·ªü sai v·ªã tr√≠
- Profile name trong `dbt_project.yml` kh√¥ng kh·ªõp v·ªõi `profiles.yml`

**Gi·∫£i ph√°p:**
```bash
# Ki·ªÉm tra profile
dbt debug --profiles-dir .

# ƒê·∫£m b·∫£o profile name kh·ªõp
# dbt_project.yml
profile: 'crawljob'

# profiles.yml
crawljob:  # ‚Üê Ph·∫£i gi·ªëng nhau
  target: dev
```

### 4.2. L·ªói: Circular Dependencies

**L·ªói:**
```
Compilation Error
  Found a cycle in the dependency graph: model_a ‚Üí model_b ‚Üí model_a
```

**Nguy√™n nh√¢n:** Model A ref() Model B, v√† Model B ref() Model A.

**Gi·∫£i ph√°p:** Refactor ƒë·ªÉ break cycle:
```sql
-- T·∫°o model intermediate
-- models/intermediate/int_shared.sql
SELECT ... FROM {{ source('bronze','jobs') }}

-- models/model_a.sql
SELECT ... FROM {{ ref('int_shared') }}

-- models/model_b.sql
SELECT ... FROM {{ ref('int_shared') }}
```

### 4.3. L·ªói: Model kh√¥ng ch·∫°y ƒë√∫ng th·ª© t·ª±

**Nguy√™n nh√¢n:** Kh√¥ng d√πng `{{ ref() }}` ho·∫∑c `{{ source() }}`.

**Gi·∫£i ph√°p:**
```sql
-- ‚ùå SAI - dbt kh√¥ng bi·∫øt dependency
SELECT * FROM silver.stg_jobs

-- ‚úÖ ƒê√öNG
SELECT * FROM {{ ref('stg_jobs') }}
```

### 4.4. L·ªói: DuckDB kh√¥ng ƒë·ªçc env_var()

**L·ªói:**
```
Database Error in model ...
  No such table: warehouse.duckdb
```

**Nguy√™n nh√¢n:** DuckDB adapter kh√¥ng h·ªó tr·ª£ env_var() trong path.

**Gi·∫£i ph√°p:** Hardcode path trong profiles.yml:
```yaml
dev:
  type: duckdb
  path: "D:\\Practice\\Scrapy\\CrawlJob\\DuckDB\\warehouse.duckdb"  # ‚Üê Hardcoded
```

### 4.5. L·ªói: Incremental model kh√¥ng filter d·ªØ li·ªáu m·ªõi

**L·ªói:** M·ªói l·∫ßn ch·∫°y `dbt run`, model incremental v·∫´n x·ª≠ l√Ω to√†n b·ªô d·ªØ li·ªáu.

**Nguy√™n nh√¢n:** Thi·∫øu `{% if is_incremental() %}` filter.

**Gi·∫£i ph√°p:**
```sql
{{ config(materialized='incremental') }}

SELECT * FROM {{ source('bronze','jobs') }}

{% if is_incremental() %}
-- ‚úÖ Th√™m filter
WHERE scraped_at > (SELECT MAX(scraped_at) FROM {{ this }})
{% endif %}
```

---

## 5. T·ªïng k·∫øt

### ‚úÖ Checklist cho dbt Project

- [ ] `profiles.yml` ƒë√£ c·∫•u h√¨nh ƒë√∫ng v·ªõi warehouse
- [ ] `dbt debug --profiles-dir .` ch·∫°y th√†nh c√¥ng
- [ ] T·∫•t c·∫£ models d√πng `{{ source() }}` v√† `{{ ref() }}`, kh√¥ng hard-code table names
- [ ] Incremental models c√≥ `{% if is_incremental() %}` filter
- [ ] Schema organization: bronze ‚Üí silver ‚Üí gold
- [ ] Tests ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong `schema.yml`
- [ ] DAG kh√¥ng c√≥ circular dependencies
- [ ] Tags ƒë∆∞·ª£c s·ª≠ d·ª•ng cho scheduling (daily, hourly, etc.)

### üéØ Key Takeaways

1. **Profiles.yml**: Qu·∫£n l√Ω k·∫øt n·ªëi database, d√πng `--profiles-dir .` cho portability
2. **DAG**: dbt t·ª± ƒë·ªông ph√°t hi·ªán dependencies qua `{{ source() }}` v√† `{{ ref() }}`
3. **Execution Order**: Topological sort ‚Üí upstream models ch·∫°y tr∆∞·ªõc downstream
4. **Best Practices**: Lu√¥n d√πng `{{ ref() }}` v√† `{{ source() }}`, kh√¥ng hard-code table names
5. **Node Selection**: D√πng `-s`, `+`, tags ƒë·ªÉ ch·∫°y subset models

### üìö Resources

- [dbt Docs - Profiles](https://docs.getdbt.com/docs/core/connect-data-platform/profiles.yml)
- [dbt Docs - ref() and source()](https://docs.getdbt.com/reference/dbt-jinja-functions/ref)
- [dbt Docs - Node Selection](https://docs.getdbt.com/reference/node-selection/syntax)
- [dbt Docs - Incremental Models](https://docs.getdbt.com/docs/build/incremental-models)
- [dbt Best Practices Guide](https://docs.getdbt.com/guides/best-practices)
