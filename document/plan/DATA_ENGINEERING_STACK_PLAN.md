# ğŸš€ **DATA ENGINEERING STACK IMPLEMENTATION PLAN**
## **CrawlJob: Professional Data Engineering Project**

---

## ğŸ“‹ **TABLE OF CONTENTS**

1. [ğŸ¯ Project Overview](#-project-overview)
2. [ğŸ—ï¸ Architecture Design](#ï¸-architecture-design)
3. [ğŸ“š Documentation References](#-documentation-references)

---

## ğŸ“š **DOCUMENTATION REFERENCES**

### **Detailed Architecture Documents**
- ğŸ“ **[Data Warehouse Architecture](DATA_WAREHOUSE_ARCHITECTURE.md)**: 
  - Complete Bronze-Silver-Gold layer design
  - Star Schema with Fact & Dimension tables
  - SCD (Slowly Changing Dimensions) implementation
  - Query patterns and use cases
  - Performance optimization strategies

- ğŸ“Š **[SCD Guide](../learning/data-warehouse-scd-guide.md)**:
  - SCD Types 0-6 explained with examples
  - When to use each type
  - dbt implementation patterns
  - CrawlJob-specific SCD mapping

### **Learning Resources**
- ğŸ”¨ [dbt Introduction](../learning/dbt-introduction.md)
- ğŸ“ [dbt Testing Guide](../learning/dbt-testing-guide.md)
- ğŸ¦† [DuckDB Guideline](../learning/duckdb-guideline.md)

---

## ğŸ¯ **PROJECT OVERVIEW**

### **Current Status**
- âœ… **10 Spiders** hoáº¡t Ä‘á»™ng hoÃ n háº£o
- âœ… **PostgreSQL** database vá»›i 10,000+ records
- âœ… **FastAPI** backend vá»›i REST endpoints
- âœ… **Web Dashboard** vá»›i Bootstrap 5
- âœ… **Automated daily crawling**

### **Data Engineering Goal**
Chuyá»ƒn Ä‘á»•i CrawlJob thÃ nh **Professional Data Engineering Project** vá»›i:
- **Apache Airflow**: Workflow orchestration
- **dbt**: Data transformation layer
- **Soda Core + dbt tests**: Data quality validation (Raw Gate + Business Rules)
â€“ **Apache Superset**: Data visualization vÃ  analytics

### **Benefits**
- ğŸ¢ **Professional**: Industry-standard data engineering stack
- ğŸ“Š **Advanced Analytics**: Rich dashboards vÃ  insights
- ğŸ”§ **Automation**: Fully automated pipelines
- ğŸ“ˆ **Scalability**: Easy to scale as project grows
- ğŸ’¼ **Career Growth**: Valuable skills for data engineering

---

## ğŸ—ï¸ **ARCHITECTURE DESIGN**

### **Current Architecture**
```
CrawlJob Spiders â†’ PostgreSQL â†’ FastAPI â†’ Web Dashboard
```

### **Target Data Engineering Architecture**

#### **Detailed Data Flow**

```mermaid
flowchart TD
    %% Layers
    subgraph ingestion["ğŸ”„ Data Ingestion"]
        spiders["ğŸ•·ï¸ CrawlJob Spiders<br/>10 Job Sites"]
        airflow["âš¡ Apache Airflow<br/>Orchestrator (Schedules/Triggers)"]
    end

    subgraph storage["ğŸ’¾ Data Storage"]
        postgres["ğŸ˜ PostgreSQL<br/>Raw & Serving (OLTP)"]
        duckdb["ğŸ¦† DuckDB<br/>Analytics Marts (OLAP)"]
    end

    subgraph processing["âš™ï¸ Data Processing"]
        soda["ğŸ§ª Soda Core<br/>Raw Gate (Postgres)"]
        scanner["ğŸ”Œ DuckDB postgres_scanner<br/>EL Postgres â†’ DuckDB"]
        dbt["ğŸ”¨ dbt-duckdb<br/>Transform & Tests (in DuckDB)"]
    end

    subgraph presentation["ğŸ“Š Presentation & Access"]
        superset["Apache Superset<br/>BI Dashboards"]
        fastapi["ğŸš€ FastAPI<br/>REST API"]
        webapp["ğŸŒ Job Search Website<br/>End-User Portal"]
    end

    %% Orchestration (control-plane)
    airflow -. trigger .-> spiders
    airflow -. run .-> soda
    airflow -. run .-> scanner
    airflow -. run .-> dbt

    %% Data plane
    spiders -->|"Insert Raw Jobs"| postgres
    soda -->|"Validate Raw"| postgres
    scanner -->|"Sync raw/staging"| duckdb
    dbt -->|"Read & Materialize"| duckdb

    %% Serving
    fastapi -->|"Query"| postgres
    webapp -->|"Use"| fastapi
    superset -->|"Connect"| duckdb

    %% Styles
    classDef ingestionStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef storageStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef processStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef presentStyle fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px

    class spiders,airflow ingestionStyle
    class postgres,duckdb storageStyle
    class dbt,scanner,soda processStyle
    class superset,fastapi,webapp presentStyle
```

#### Data Flow chi tiáº¿t cho Apache Superset

1) Äiá»u phá»‘i theo lá»‹ch (Airflow)
- Airflow cháº¡y theo lá»‹ch (vÃ­ dá»¥ 02:00 háº±ng ngÃ y) vÃ  láº§n lÆ°á»£t trigger cÃ¡c bÆ°á»›c: cháº¡y spiders â†’ kiá»ƒm tra cháº¥t lÆ°á»£ng (Soda Core) â†’ Ä‘á»“ng bá»™ EL (DuckDB postgres_scanner: PostgreSQL â†’ DuckDB) â†’ biáº¿n Ä‘á»•i dá»¯ liá»‡u (dbt-duckdb) â†’ cáº­p nháº­t kho OLAP (DuckDB).

2) Thu tháº­p dá»¯ liá»‡u (Spiders â†’ PostgreSQL)
- CÃ¡c spiders thu tháº­p dá»¯ liá»‡u tá»« 10 trang, chuáº©n hÃ³a tá»‘i thiá»ƒu vÃ  ghi trá»±c tiáº¿p vÃ o PostgreSQL (schema/raw), kÃ¨m timestamps/metadata phá»¥c vá»¥ kiá»ƒm soÃ¡t phiÃªn crawl.

3) Kiá»ƒm tra cháº¥t lÆ°á»£ng (Raw Gate â€“ Soda Core)
- Soda Core cháº¡y trÃªn báº£ng raw á»Ÿ PostgreSQL: kiá»ƒm tra schema, tÃ­nh há»£p lá»‡ (URL), khÃ´ng null, row_count, vÃ  freshness (scraped_at).
- Náº¿u FAIL: Airflow dá»«ng pipeline, gá»­i cáº£nh bÃ¡o; dá»¯ liá»‡u OLAP cÅ© váº«n Ä‘Æ°á»£c giá»¯ nguyÃªn Ä‘á»ƒ dashboard Superset khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng.
- Náº¿u PASS: tiáº¿p tá»¥c bÆ°á»›c biáº¿n Ä‘á»•i. (Sau-transform) Sá»­ dá»¥ng `dbt test` Ä‘á»ƒ kiá»ƒm tra cÃ¡c model.

4) Äá»“ng bá»™ dá»¯ liá»‡u (DuckDB postgres_scanner â€“ EL)
- DuckDB `postgres_scanner` sync tá»« PostgreSQL (raw/staging) â†’ DuckDB (OLAP), há»— trá»£ full refresh hoáº·c incremental theo cá»™t thá»i gian (máº·c Ä‘á»‹nh `scraped_at`).
- Quáº£n lÃ½ lá»‹ch cháº¡y vÃ  tham sá»‘ hoÃ¡ qua Airflow (BashOperator).

5) Biáº¿n Ä‘á»•i dá»¯ liá»‡u (dbt-duckdb â€“ ELT)
- dbt-duckdb Ä‘á»c dá»¯ liá»‡u trong DuckDB â†’ táº¡o cÃ¡c mÃ´ hÃ¬nh staging/dim/fact/agg.
- Káº¿t quáº£ Ä‘Æ°á»£c materialize trá»±c tiáº¿p trong DuckDB thÃ nh cÃ¡c báº£ng/khung nhÃ¬n analytics-ready.

5) Kho phÃ¢n tÃ­ch (DuckDB â€“ OLAP)
- DuckDB lÆ°u trá»¯ cÃ¡c mÃ´ hÃ¬nh phá»¥c vá»¥ phÃ¢n tÃ­ch (vÃ­ dá»¥: dim_companies, fct_jobs, agg_jobs_by_industryâ€¦).
- File DuckDB Ä‘Æ°á»£c Ä‘áº·t táº¡i má»™t Ä‘Æ°á»ng dáº«n á»•n Ä‘á»‹nh Ä‘á»ƒ phá»¥c vá»¥ káº¿t ná»‘i tá»« Superset.

6) Káº¿t ná»‘i Apache Superset
- Superset káº¿t ná»‘i tá»›i DuckDB qua SQLAlchemy (duckdb-engine) Ä‘á»ƒ Ä‘á»c cÃ¡c báº£ng phÃ¢n tÃ­ch. VÃ­ dá»¥ káº¿t ná»‘i:
    - SQLAlchemy URI: `duckdb:///D:/path/to/warehouse.duckdb`

7) LÃ m má»›i dá»¯ liá»‡u (Refresh)
- Desktop: Refresh thá»§ cÃ´ng Ä‘á»ƒ phÃ¡t triá»ƒn/kiá»ƒm thá»­.
- Service: DÃ¹ng cron Airflow Ä‘á»ƒ trigger sync + transform; dashboard dÃ¹ng nguá»“n DuckDB cáº­p nháº­t.

8) TrÃ¬nh bÃ y vÃ  tiÃªu thá»¥
- Superset sá»­ dá»¥ng cÃ¡c báº£ng trong DuckDB Ä‘á»ƒ dá»±ng dashboard (Jobs by Industry, Salary Distribution, Trendsâ€¦). NgÆ°á»i dÃ¹ng xem dashboard trÃªn giao diá»‡n Superset.

9) á»¨ng dá»¥ng web ngÆ°á»i dÃ¹ng
- Job Search Website truy cáº­p dá»¯ liá»‡u qua FastAPI â†’ PostgreSQL (OLTP) Ä‘á»ƒ phá»¥c vá»¥ tra cá»©u/tÃ¬m kiáº¿m theo thá»i gian thá»±c; khÃ´ng truy váº¥n DuckDB.

```mermaid
flowchart LR
    Airflow[Apache Airflow] -. trigger .-> Spiders[CrawlJob Spiders]
    Spiders -->|Raw jobs| Postgres[(PostgreSQL OLTP)]
    Airflow -. run .-> Soda[Soda Core]
    Soda -->|Validate raw| Postgres
    Airflow -. run .-> Scanner[DuckDB postgres_scanner]
    Scanner -->|Sync| DuckDB[(DuckDB OLAP)]
    Airflow -. run .-> dbt[dbt]
    dbt -->|Materialize marts| DuckDB
    Superset[Apache Superset] -->|Connect| DuckDB

    classDef c1 fill:#e1f5fe,stroke:#01579b,stroke-width:1px
    classDef c2 fill:#f3e5f5,stroke:#4a148c,stroke-width:1px
    class Airflow,Spiders c1
    class Postgres,DuckDB c2
```

#### Data Flow chi tiáº¿t cho Orchestration & Monitoring (Airflow)

1) LÃªn lá»‹ch & Ä‘iá»u phá»‘i
- Airflow DAG cháº¡y theo cron (vÃ­ dá»¥ 02:00). CÃ¡c task: `run_spiders` â†’ `soda_validate_raw` â†’ `duckdb_sync` â†’ `dbt_run` â†’ `dbt_test` â†’ `notify_success`.

2) Retry & SLA
- Má»—i task cÃ³ `retries` vÃ  `retry_delay` há»£p lÃ½; Ä‘áº·t `sla` Ä‘á»ƒ cáº£nh bÃ¡o khi quÃ¡ thá»i gian.

3) Logging & Artifacts
- Log chi tiáº¿t cá»§a tá»«ng task Ä‘Æ°á»£c lÆ°u vÃ o thÆ° má»¥c logs; artifacts gá»“m log `soda scan`, file DuckDB, vÃ  dbt target (manifest/run_results).

4) Alerting
- KÃªnh cáº£nh bÃ¡o: Email/Slack khi task fail/SLA miss.

5) Observability
- Theo dÃµi tráº¡ng thÃ¡i DAG trÃªn Airflow UI (Gantt/Graph).

#### Data Quality Implementation (Soda Core + dbt tests)

1) Soda Core (Raw Gate)
- Khai bÃ¡o data source Postgres trong `soda/configuration.yml`.
- Äá»‹nh nghÄ©a checks trong `soda/checks/raw_jobs_check1.yml`, `raw_jobs_check2.yml`, `raw_jobs_check3.yml`.
- Cháº¡y tuáº§n tá»± 3 checks trong Airflow (BashOperator). Fail dá»«ng pipeline.

2) dbt tests (Post-Transform)
- Viáº¿t tests trong `schema.yml` cá»§a cÃ¡c model (built-in + dbt-expectations náº¿u cáº§n).
- Cháº¡y `dbt test` sau `dbt run`. Fail thÃ¬ alert vÃ  dá»«ng publish.

#### EL Implementation (DuckDB postgres_scanner)

- Script: `scripts/sync_pg_to_duckdb.py`
- Env vars:
  - `POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD`
  - `DUCKDB_PATH`, `DUCKDB_SCHEMA` (default: `raw`)
  - `PG_TABLE` (default: `jobs`), `PG_CURSOR_COLUMN` (default: `scraped_at`)
  - `SYNC_MODE` = `full` | `incremental` (default: `incremental`)
- Full refresh:
```bash
set SYNC_MODE=full & python scripts/sync_pg_to_duckdb.py
```
- Incremental:
```bash
python scripts/sync_pg_to_duckdb.py
```

#### Data Flow chi tiáº¿t cho dbt Docs & Lineage

```mermaid
flowchart TD
    dbt_run[dbt run] --> models[Staging/Dim/Fact/Agg Models]
    dbt_run --> target_duckdb[(DuckDB marts)]
    dbt_docs[dbt docs generate] --> catalog[Catalog + Lineage]
    exposures[dbt exposures] --> consumers[Superset, Web App]
    freshness[dbt source freshness] --> status[Freshness Status]
```

#### Data Flow chi tiáº¿t cho Data Export/Sharing (Parquet/External)

1) Export tá»« DuckDB
- Sau `dbt run`, cÃ³ thá»ƒ export báº£ng phÃ¢n tÃ­ch tá»« DuckDB sang Parquet/CSV trong `data/exports/` Ä‘á»ƒ chia sáº» cho data science/Ä‘á»‘i tÃ¡c.

2) TÃ­ch há»£p cÃ´ng cá»¥ khÃ¡c
- CÃ¡c cÃ´ng cá»¥ nhÆ° Pandas, Spark, hoáº·c Power BI (qua Parquet folder) cÃ³ thá»ƒ tiÃªu thá»¥ dá»¯ liá»‡u nÃ y mÃ  khÃ´ng cáº§n truy cáº­p trá»±c tiáº¿p DB.

3) Quáº£n trá»‹ phiÃªn báº£n
- Äáº·t quy táº¯c Ä‘áº·t tÃªn (kÃ¨m timestamp) vÃ  dá»n dáº¹p phiÃªn báº£n cÅ© báº±ng job Ä‘á»‹nh ká»³ Ä‘á»ƒ tá»‘i Æ°u dung lÆ°á»£ng.

```mermaid
flowchart TD
    MARTS["DuckDB marts"] --> EXPORT["Export to Parquet or CSV"]
    EXPORT --> PANDAS["Pandas"]
    EXPORT --> SPARK["Spark"]
    EXPORT --> PBI["Superset (via Parquet folder)"]
    AF["Airflow optional"] --> EXPORT
```

### **Technology Stack**
- **Orchestration**: Apache Airflow
- **OLTP Database**: PostgreSQL
- **OLAP Database**: DuckDB
- **Transformation**: dbt-duckdb
- **EL**: DuckDB postgres_scanner script
- **Data Quality**: Soda Core (raw) + dbt tests (post-transform)
- **Visualization**: Apache Superset
- **Backend**: FastAPI
- **Frontend**: Bootstrap 5
- **Containerization**: Docker
- **Version Control**: Git & GitHub
