# Project: CrawlJob

## üéØ Current State
- **Phase**: Data acquisition complete (MVP scrapers working)
- **Progress**: 4/4 spiders, pipeline insert, JSON export, env-based DB config
- **Next Goal**: Data reliability, dedup/upsert, and API access for consumers

## ‚úÖ Completed Tasks
- [x] Implement spiders: JobsGO, JobOKO, 123job, CareerViet
- [x] SQL Server pipeline: create table if missing, insert items
- [x] CLI runner `run_spider.py` with FEEDS to export JSON
- [x] Environment-based DB configuration via `.env` (python-dotenv)
- [x] Basic throttling config and custom User-Agent

## üîÑ Pending Tasks
### Phase 1: Quick Wins (HIGH PRIORITY)
- [ ] Dedup & Unique Constraint (45 minutes)
  - **Objective**: NgƒÉn ch√®n tr√πng job theo `job_url` (ho·∫∑c `job_url` + `source_site`).
  - **Why?**: Tr√°nh d·ªØ li·ªáu tr√πng l·∫∑p khi crawl nhi·ªÅu l·∫ßn.
  - **Files to modify**: `CrawlJob/pipelines.py` (check t·ªìn t·∫°i tr∆∞·ªõc khi insert), README (h∆∞·ªõng d·∫´n unique index).
  - **Dependencies**: B·∫£ng `jobs` hi·ªán c√≥.
  - **Inputs & Outputs**: Input: `JobItem`; Output: insert m·ªõi ho·∫∑c skip/update.
  - **Acceptance Criteria**: Kh√¥ng c√≥ b·∫£n ghi tr√πng theo `job_url` trong k·∫øt qu·∫£ m·ªõi.
  - **Definition of Done**: 
    - T·∫°o unique index (t√†i li·ªáu SQL trong README) ho·∫∑c ki·ªÉm tra t·ªìn t·∫°i b·∫±ng SELECT tr∆∞·ªõc insert.
    - Pipeline b·ªè qua ho·∫∑c c·∫≠p nh·∫≠t b·∫£n ghi ƒë√£ c√≥.
  - **Test Cases**: Crawl 2 l·∫ßn c√πng keyword; s·ªë b·∫£n ghi kh√¥ng tƒÉng tr√πng.

- [ ] Upsert (Insert-or-Update) + `updated_at` (45 minutes)
  - **Objective**: N·∫øu job ƒë√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t c√°c tr∆∞·ªùng thay ƒë·ªïi v√† set `updated_at`.
  - **Why?**: Duy tr√¨ d·ªØ li·ªáu m·ªõi nh·∫•t theo th·ªùi gian.
  - **Files to modify**: `CrawlJob/pipelines.py` (logic upsert), README (m√¥ t·∫£ c·ªôt `updated_at`).
  - **Dependencies**: Dedup/unique key.
  - **Acceptance Criteria**: C√πng `job_url` ghi ƒë√® tr∆∞·ªùng thay ƒë·ªïi, kh√¥ng t·∫°o b·∫£n ghi m·ªõi.
  - **Definition of Done**: T·ªìn t·∫°i h√†m upsert; th√™m c·ªôt `updated_at` v√†o schema.
  - **Test Cases**: S·ª≠a m√¥ t·∫£ job gi·∫£ l·∫≠p v√† crawl l·∫°i ‚Üí record ƒë∆∞·ª£c c·∫≠p nh·∫≠t.

- [ ] Structured Logging (30 minutes)
  - **Objective**: Th√™m logging chu·∫©n (level, context `spider`, `job_url`).
  - **Why?**: D·ªÖ theo d√µi l·ªói v√† ch·∫•t l∆∞·ª£ng crawl.
  - **Files to modify**: C√°c spider, `pipelines.py`.
  - **Acceptance Criteria**: Log c√≥ ƒë·ªß th√¥ng tin, ghi ra file `logs/`.
  - **Test Cases**: Ch·∫°y 1 spider v√† ki·ªÉm tra file log.

### Phase 2: Core Implementation (HIGH PRIORITY)
- [ ] API Read-Only Service (FastAPI) (2 hours)
  - **Objective**: Cung c·∫•p endpoint t√¨m ki·∫øm job theo `keyword`, `location`, `source_site`.
  - **Why?**: Cho ph√©p ·ª©ng d·ª•ng/ƒë·ªëi t√°c ti√™u th·ª• d·ªØ li·ªáu qua HTTP.
  - **Files to modify**: Th∆∞ m·ª•c `api/` m·ªõi (`main.py`), th√™m deps: `fastapi`, `uvicorn`.
  - **Acceptance Criteria**: 
    - GET `/jobs?keyword=...&site=...` tr·∫£ JSON (paging).
    - K·∫øt n·ªëi SQL Server read-only, filter c∆° b·∫£n, sort theo `created_at`.
  - **Test Cases**: G·ªçi API tr·∫£ danh s√°ch, status 200, th·ªùi gian ph·∫£n h·ªìi < 500ms n·ªôi b·ªô.

- [ ] Exporters: CSV/Parquet (45 minutes)
  - **Objective**: H·ªó tr·ª£ xu·∫•t CSV/Parquet ngo√†i JSON.
  - **Why?**: Linh ho·∫°t t√≠ch h·ª£p BI/ML.
  - **Files to modify**: `run_spider.py` (tham s·ªë `--output-format`), README.
  - **Acceptance Criteria**: T·∫°o ƒë∆∞·ª£c file `.csv`/`.parquet` v·ªõi schema ·ªïn ƒë·ªãnh.
  - **Test Cases**: So s√°nh s·ªë b·∫£n ghi gi·ªØa DB v√† file export.

- [ ] Incremental Crawling & Scheduling (1 hour)
  - **Objective**: L·∫≠p l·ªãch ch·∫°y (Windows Task Scheduler) v√† ch·ªâ crawl job m·ªõi/c·∫≠p nh·∫≠t.
  - **Why?**: Duy tr√¨ d·ªØ li·ªáu c·∫≠p nh·∫≠t theo ng√†y/gi·ªù.
  - **Files to modify**: README h∆∞·ªõng d·∫´n l·∫≠p l·ªãch; spider ch·∫•p nh·∫≠n tham s·ªë `since` (n·∫øu c·∫ßn).
  - **Acceptance Criteria**: L√™n l·ªãch ch·∫°y ƒë·ªãnh k·ª≥; crawl √≠t d·ªØ li·ªáu d∆∞ th·ª´a.
  - **Test Cases**: L√™n l·ªãch ch·∫°y th·ª≠, ki·ªÉm tra log v√† s·ªë b·∫£n ghi tƒÉng h·ª£p l√Ω.

### Phase 3: Optimization (MEDIUM PRIORITY)
- [ ] AutoThrottle & Rotating User-Agent/Proxies (1.5 hours)
  - **Objective**: Gi·∫£m rate-limit/r√†ng bu·ªôc bot.
  - **Why?**: ·ªîn ƒë·ªãnh crawl khi quy m√¥ l·ªõn.
  - **Files to modify**: `settings.py` (AutoThrottle), middleware UA/proxy.
  - **Acceptance Criteria**: Gi·∫£m l·ªói 429/ban; t·ªëc ƒë·ªô crawl ·ªïn ƒë·ªãnh.
  - **Test Cases**: So s√°nh th·ªùi gian/ l·ªói tr∆∞·ªõc-sau.

- [ ] Selector Resilience (1.5 hours)
  - **Objective**: Chu·∫©n ho√° selector theo module v√† fallback ƒëa chi·∫øn l∆∞·ª£c.
  - **Why?**: Gi·∫£m v·ª° khi HTML thay ƒë·ªïi nh·ªè.
  - **Files to modify**: `spiders/` (tr√≠ch chung h√†m extract, regex labels), `utils.py`.
  - **Acceptance Criteria**: 90% trang thay ƒë·ªïi nh·∫π v·∫´n parse ƒë∆∞·ª£c c√°c tr∆∞·ªùng ch√≠nh.
  - **Test Cases**: B·ªô trang m·∫´u (c≈©/m·ªõi) parse ·ªïn.

- [ ] Basic Tests (1 hour)
  - **Objective**: Th√™m unit test cho utils v√† pipeline; fake HTML cho parser.
  - **Why?**: B·∫£o v·ªá ch·ª©c nƒÉng c·ªët l√µi.
  - **Files to modify**: `tests/` m·ªõi; CI c√¢n nh·∫Øc sau.
  - **Acceptance Criteria**: `pytest` pass; coverage t·ªëi thi·ªÉu cho utils/pipeline.

### Phase 4: Advanced Features (LOW PRIORITY)
- [ ] Enrichment & NLP (4 hours)
  - **Objective**: Chu·∫©n ho√° tr∆∞·ªùng (m·ª©c l∆∞∆°ng, ƒë·ªãa ƒëi·ªÉm), tr√≠ch k·ªπ nƒÉng, ph√¢n lo·∫°i ng√†nh.
  - **Why?**: TƒÉng gi√° tr·ªã ph√¢n t√≠ch downstream.
  - **Files to modify**: Module `enrichment/` (chu·∫©n ho√°, mapping, NLP c∆° b·∫£n), th√™m c·ªôt m·ªõi n·∫øu c·∫ßn.
  - **Acceptance Criteria**: T·ª∑ l·ªá parse chu·∫©n ho√° >80% cho m·∫´u th·ª≠.

- [ ] Analytics Dashboard (2 hours)
  - **Objective**: Metabase/PowerBI/Streamlit dashboard nhanh.
  - **Why?**: Tr·ª±c quan ho√° s·ªë li·ªáu.
  - **Files to modify**: T√†i li·ªáu c·∫•u h√¨nh + script k·∫øt n·ªëi.
  - **Acceptance Criteria**: Xem ƒë∆∞·ª£c top c√¥ng ty, m·ª©c l∆∞∆°ng theo v·ªã tr√≠.

## üìä Workflow Visualization
```mermaid
graph TD
    subgraph "Phase 1: Quick Wins"
        A[Dedup & Unique] --> B[Upsert + updated_at]
        B --> C[Structured Logging]
    end
    subgraph "Phase 2: Core Implementation"
        C --> D[FastAPI Read API]
        D --> E[CSV/Parquet Export]
        E --> F[Incremental & Scheduling]
    end
    subgraph "Phase 3: Optimization"
        F --> G[AutoThrottle & Rotating UA/Proxies]
        G --> H[Selector Resilience]
        H --> I[Basic Tests]
    end
    subgraph "Phase 4: Advanced"
        I --> J[Enrichment & NLP]
        J --> K[Analytics Dashboard]
    end
```

## üéØ Next Actions
1. Implement Dedup + Upsert in `CrawlJob/pipelines.py` (∆∞u ti√™n cao)
2. Thi·∫øt k·∫ø API ƒë·ªçc (FastAPI) ƒë·ªÉ truy v·∫•n jobs
3. B·∫≠t AutoThrottle v√† chu·∫©n b·ªã chi·∫øn l∆∞·ª£c UA/Proxy n·∫øu m·ªü r·ªông

## üìä Progress Tracking
- **Total tasks**: 12
- **Completed**: 5
- **Remaining**: 7
- **Estimated time**: ~14 hours

## üéØ Success Criteria
- [ ] Kh√¥ng c√≤n tr√πng l·∫∑p theo `job_url` sau nhi·ªÅu l·∫ßn crawl
- [ ] C√≥ API read-only ƒë·ªÉ ti√™u th·ª• d·ªØ li·ªáu
- [ ] Crawl ·ªïn ƒë·ªãnh v·ªõi AutoThrottle v√† UA/Proxy
- [ ] C√≥ test c∆° b·∫£n b·∫£o v·ªá pipeline v√† utils
