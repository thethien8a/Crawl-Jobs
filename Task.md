# Project: CrawlJob

## ğŸ¯ Current State
- **Phase**: Data acquisition complete (MVP scrapers working)
- **Progress**: 7/12 tasks completed
- **Next Goal**: API access for consumers, reliability/ops improvements

## âœ… Completed Tasks
- [x] Implement spiders: JobsGO, JobOKO, 123job, CareerViet
- [x] SQL Server pipeline: create table if missing, insert items
- [x] CLI runner `run_spider.py` with FEEDS to export JSON
- [x] Environment-based DB configuration via `.env` (python-dotenv)
- [x] Basic throttling config and custom User-Agent
- [x] Dedup & Unique Constraint â€” unique on `(source_site, job_url)`; prevent duplicates (completed)
- [x] Upsert (Insert-or-Update) + `updated_at` â€” update existing rows and set `updated_at` (completed)

## ğŸ”„ Pending Tasks
### Phase 1: Quick Wins (HIGH PRIORITY)
- [ ] Structured Logging (30 minutes)
  - **Objective**: ThÃªm logging chuáº©n (level, context `spider`, `job_url`).
  - **Why?**: Dá»… theo dÃµi lá»—i vÃ  cháº¥t lÆ°á»£ng crawl.
  - **Files to modify**: CÃ¡c spider, `pipelines.py`.
  - **Acceptance Criteria**: Log cÃ³ Ä‘á»§ thÃ´ng tin, ghi ra file `logs/`.
  - **Test Cases**: Cháº¡y 1 spider vÃ  kiá»ƒm tra file log.

### Phase 2: Core Implementation (HIGH PRIORITY)
- [ ] API Read-Only Service (FastAPI) (2 hours)
  - **Objective**: Cung cáº¥p endpoint tÃ¬m kiáº¿m job theo `keyword`, `location`, `source_site`.
  - **Why?**: Cho phÃ©p á»©ng dá»¥ng/Ä‘á»‘i tÃ¡c tiÃªu thá»¥ dá»¯ liá»‡u qua HTTP.
  - **Files to modify**: ThÆ° má»¥c `api/` má»›i (`main.py`), thÃªm deps: `fastapi`, `uvicorn`.
  - **Acceptance Criteria**: 
    - GET `/jobs?keyword=...&site=...` tráº£ JSON (paging).
    - Káº¿t ná»‘i SQL Server read-only, filter cÆ¡ báº£n, sort theo `created_at`.
  - **Test Cases**: Gá»i API tráº£ danh sÃ¡ch, status 200, thá»i gian pháº£n há»“i < 500ms ná»™i bá»™.

- [ ] Exporters: CSV/Parquet (45 minutes)
  - **Objective**: Há»— trá»£ xuáº¥t CSV/Parquet ngoÃ i JSON.
  - **Why?**: Linh hoáº¡t tÃ­ch há»£p BI/ML.
  - **Files to modify**: `run_spider.py` (tham sá»‘ `--output-format`), README.
  - **Acceptance Criteria**: Táº¡o Ä‘Æ°á»£c file `.csv`/`.parquet` vá»›i schema á»•n Ä‘á»‹nh.
  - **Test Cases**: So sÃ¡nh sá»‘ báº£n ghi giá»¯a DB vÃ  file export.

- [ ] Incremental Crawling & Scheduling (1 hour)
  - **Objective**: Láº­p lá»‹ch cháº¡y (Windows Task Scheduler) vÃ  chá»‰ crawl job má»›i/cáº­p nháº­t.
  - **Why?**: Duy trÃ¬ dá»¯ liá»‡u cáº­p nháº­t theo ngÃ y/giá».
  - **Files to modify**: README hÆ°á»›ng dáº«n láº­p lá»‹ch; spider cháº¥p nháº­n tham sá»‘ `since` (náº¿u cáº§n).
  - **Acceptance Criteria**: LÃªn lá»‹ch cháº¡y Ä‘á»‹nh ká»³; crawl Ã­t dá»¯ liá»‡u dÆ° thá»«a.
  - **Test Cases**: LÃªn lá»‹ch cháº¡y thá»­, kiá»ƒm tra log vÃ  sá»‘ báº£n ghi tÄƒng há»£p lÃ½.

### Phase 3: Optimization (MEDIUM PRIORITY)
- [ ] AutoThrottle & Rotating User-Agent/Proxies (1.5 hours)
  - **Objective**: Giáº£m rate-limit/rÃ ng buá»™c bot.
  - **Why?**: á»”n Ä‘á»‹nh crawl khi quy mÃ´ lá»›n.
  - **Files to modify**: `settings.py` (AutoThrottle), middleware UA/proxy.
  - **Acceptance Criteria**: Giáº£m lá»—i 429/ban; tá»‘c Ä‘á»™ crawl á»•n Ä‘á»‹nh.
  - **Test Cases**: So sÃ¡nh thá»i gian/ lá»—i trÆ°á»›c-sau.

- [ ] Selector Resilience (1.5 hours)
  - **Objective**: Chuáº©n hoÃ¡ selector theo module vÃ  fallback Ä‘a chiáº¿n lÆ°á»£c.
  - **Why?**: Giáº£m vá»¡ khi HTML thay Ä‘á»•i nhá».
  - **Files to modify**: `spiders/` (trÃ­ch chung hÃ m extract, regex labels), `utils.py`.
  - **Acceptance Criteria**: 90% trang thay Ä‘á»•i nháº¹ váº«n parse Ä‘Æ°á»£c cÃ¡c trÆ°á»ng chÃ­nh.
  - **Test Cases**: Bá»™ trang máº«u (cÅ©/má»›i) parse á»•n.

- [ ] Basic Tests (1 hour)
  - **Objective**: ThÃªm unit test cho utils vÃ  pipeline; fake HTML cho parser.
  - **Why?**: Báº£o vá»‡ chá»©c nÄƒng cá»‘t lÃµi.
  - **Files to modify**: `tests/` má»›i; CI cÃ¢n nháº¯c sau.
  - **Acceptance Criteria**: `pytest` pass; coverage tá»‘i thiá»ƒu cho utils/pipeline.

### Phase 4: Advanced Features (LOW PRIORITY)
- [ ] Enrichment & NLP (4 hours)
  - **Objective**: Chuáº©n hoÃ¡ trÆ°á»ng (má»©c lÆ°Æ¡ng, Ä‘á»‹a Ä‘iá»ƒm), trÃ­ch ká»¹ nÄƒng, phÃ¢n loáº¡i ngÃ nh.
  - **Why?**: TÄƒng giÃ¡ trá»‹ phÃ¢n tÃ­ch downstream.
  - **Files to modify**: Module `enrichment/` (chuáº©n hoÃ¡, mapping, NLP cÆ¡ báº£n), thÃªm cá»™t má»›i náº¿u cáº§n.
  - **Acceptance Criteria**: Tá»· lá»‡ parse chuáº©n hoÃ¡ >80% cho máº«u thá»­.

- [ ] Analytics Dashboard (2 hours)
  - **Objective**: Metabase/PowerBI/Streamlit dashboard nhanh.
  - **Why?**: Trá»±c quan hoÃ¡ sá»‘ liá»‡u.
  - **Files to modify**: TÃ i liá»‡u cáº¥u hÃ¬nh + script káº¿t ná»‘i.
  - **Acceptance Criteria**: Xem Ä‘Æ°á»£c top cÃ´ng ty, má»©c lÆ°Æ¡ng theo vá»‹ trÃ­.

## ğŸ“Š Workflow Visualization
```mermaid
graph TD
    subgraph "Phase 1: Quick Wins"
        A[Dedup & Unique] --> B[Upsert + updated_at]
        B --> C[Structured Logging]
    end
    subgraph "Phase 2: Core Implementation"
        C --> D[FastAPI Read API]
        D --> E[CSV/Parquet Export]
        E --> F[Incremental & Scheduling]
    end
    subgraph "Phase 3: Optimization"
        F --> G[AutoThrottle & Rotating UA/Proxies]
        G --> H[Selector Resilience]
        H --> I[Basic Tests]
    end
    subgraph "Phase 4: Advanced"
        I --> J[Enrichment & NLP]
        J --> K[Analytics Dashboard]
    end
```

## ğŸ¯ Next Actions
1. Structured Logging (thÃªm logger chuáº©n vÃ  ghi file dÆ°á»›i `logs/`)
2. Thiáº¿t káº¿ API Ä‘á»c (FastAPI) Ä‘á»ƒ truy váº¥n jobs
3. Báº­t AutoThrottle vÃ  chuáº©n bá»‹ chiáº¿n lÆ°á»£c UA/Proxy náº¿u má»Ÿ rá»™ng

## ğŸ“Š Progress Tracking
- **Total tasks**: 12
- **Completed**: 7
- **Remaining**: 5
- **Estimated time**: ~11.5 hours

## ğŸ¯ Success Criteria
- [ ] KhÃ´ng cÃ²n trÃ¹ng láº·p theo `job_url` sau nhiá»u láº§n crawl
- [ ] CÃ³ API read-only Ä‘á»ƒ tiÃªu thá»¥ dá»¯ liá»‡u
- [ ] Crawl á»•n Ä‘á»‹nh vá»›i AutoThrottle vÃ  UA/Proxy
- [ ] CÃ³ test cÆ¡ báº£n báº£o vá»‡ pipeline vÃ  utils
